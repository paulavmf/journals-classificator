@ARTICLE{7752929,
author={R. F. S. Teixeira and N. J. Leite},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={A New Framework for Quality Assessment of High-Resolution Fingerprint
Images},
year={2017},
volume={39},
number={10},
pages={1905-1917},
abstract={The quality assessment of sets of features extracted from patterns of
epidermal ridges on our fingers is a biometric challenge problem with
implications on questions concerning security, privacy and identity fraud. In
this work, we introduced a new methodology to analyze the quality of high-
resolution fingerprint images containing sets of fingerprint pores. Our
approach takes into account the spatial interrelationship between the
considered features and some basic transformations involving point process and
anisotropic analysis. We proposed two new quality index algorithms following
spatial and structural classes of analysis. These algorithms have proved to be
effective as a performance predictor and as a filter excluding low-quality
features in a recognition process. The experiments using error reject curves
show that the proposed approaches outperform the state-of-the-art quality
assessment algorithm for high-resolution fingerprint recognition, besides
defining a new method for reconstructing their friction ridge phases in a very
consistent way.},
keywords={feature extraction;fingerprint identification;image
resolution;quality management;anisotropic analysis;biometrics;epidermal
ridges;error reject curves;feature extraction;fingerprint pores;friction ridge
phases;high-resolution fingerprint images;high-resolution fingerprint
recognition;point process;quality assessment;quality index algorithms;Algorithm
design and analysis;Biomedical imaging;Feature
extraction;Friction;Indexes;Measurement;Quality
assessment;Biometrics;fingerprint analysis;high-resolution images;pores;quality
assessment},
doi={10.1109/TPAMI.2016.2631529},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7731172,
author={N. J. Hughes and G. J. Goodhill},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Estimating Cortical Feature Maps with Dependent Gaussian Processes},
year={2017},
volume={39},
number={10},
pages={1918-1928},
abstract={A striking example of brain organisation is the stereotyped
arrangement of cell preferences in the visual cortex for edges of particular
orientations in the visual image. These “orientation preference maps” appear to
have remarkably consistent statistical properties across many species. However
fine scale analysis of these properties requires the accurate reconstruction of
maps from imaging data which is highly noisy. A new approach for solving this
reconstruction problem is to use Bayesian Gaussian process methods, which
produce more accurate results than classical techniques. However, so far this
work has not considered the fact that maps for several other features of visual
input coexist with the orientation preference map and that these maps have
mutually dependent spatial arrangements. Here we extend the Gaussian process
framework to the multiple output case, so that we can consider multiple maps
simultaneously. We demonstrate that this improves reconstruction of multiple
maps compared to both classical techniques and the single output approach, can
encode the empirically observed relationships, and is easily extendible. This
provides the first principled approach for studying the spatial relationships
between feature maps in visual cortex.},
keywords={Bayes methods;Gaussian processes;edge detection;image
reconstruction;self-organising feature maps;statistical analysis;Bayesian
Gaussian process methods;brain organisation;cell preference stereotyped
arrangement;cortical feature maps estimation;dependent Gaussian
processes;imaging data;maps reconstruction;orientation preference
maps;statistical properties;visual cortex;visual image edges;Covariance
matrices;Gaussian processes;Image reconstruction;Imaging;Kernel;Noise
measurement;Visualization;Gaussian processes;multitask
learning;neuroimaging;visual cortical maps},
doi={10.1109/TPAMI.2016.2624295},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7723880,
author={B. Peng and L. Zhang and X. Mou and M. H. Yang},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Evaluation of Segmentation Quality via Adaptive Composition of Reference
Segmentations},
year={2017},
volume={39},
number={10},
pages={1929-1941},
abstract={Evaluating image segmentation quality is a critical step for
generating desirable segmented output and comparing performance of algorithms,
among others. However, automatic evaluation of segmented results is inherently
challenging since image segmentation is an ill-posed problem. This paper
presents a framework to evaluate segmentation quality using multiple labeled
segmentations which are considered as references. For a segmentation to be
evaluated, we adaptively compose a reference segmentation using multiple
labeled segmentations, which locally matches the input segments while
preserving structural consistency. The quality of a given segmentation is then
measured by its distance to the composed reference. A new dataset of 200
images, where each one has 6 to 15 labeled segmentations, is developed for
performance evaluation of image segmentation. Furthermore, to quantitatively
compare the proposed segmentation evaluation algorithm with the state-of-the-
art methods, a benchmark segmentation evaluation dataset is proposed. Extensive
experiments are carried out to validate the proposed segmentation evaluation
framework.},
keywords={image segmentation;adaptive composition;benchmark segmentation
evaluation dataset;image segmentation quality;multiple labeled
segmentations;reference segmentations;segmentation quality evaluation;Benchmark
testing;Electronic mail;Image segmentation;Impedance
matching;Indexes;Observers;Performance evaluation;Image segmentation
evaluation;image segmentation dataset;segmentation quality},
doi={10.1109/TPAMI.2016.2622703},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7571172,
author={C. Spampinato and S. Palazzo and D. Giordano},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Gamifying Video Object Segmentation},
year={2017},
volume={39},
number={10},
pages={1942-1958},
abstract={Video object segmentation can be considered as one of the most
challenging computer vision problems. Indeed, so far, no existing solution is
able to effectively deal with the peculiarities of real-world videos,
especially in cases of articulated motion and object occlusions; limitations
that appear more evident when we compare the performance of automated methods
with the human one. However, manually segmenting objects in videos is largely
impractical as it requires a lot of time and concentration. To address this
problem, in this paper we propose an interactive video object segmentation
method, which exploits, on one hand, the capability of humans to identify
correctly objects in visual scenes, and on the other hand, the collective human
brainpower to solve challenging and large-scale tasks. In particular, our
method relies on a game with a purpose to collect human inputs on object
locations, followed by an accurate segmentation phase achieved by optimizing an
energy function encoding spatial and temporal constraints between object
regions as well as human-provided location priors. Performance analysis carried
out on complex video benchmarks, and exploiting data provided by over 60 users,
demonstrated that our method shows a better trade-off between annotation times
and segmentation accuracy than interactive video annotation and automated video
object segmentation approaches.},
keywords={computer games;computer vision;image segmentation;interactive
systems;video signal processing;collective human brainpower;computer
vision;gamification;interactive video annotation;video object
segmentation;visual scenes;Computer vision;Computers;Data mining;Games;Motion
segmentation;Object segmentation;Visualization;Interactive video
annotation;games with a purpose;human in the loop;spatio-temporal superpixel
segmentation},
doi={10.1109/TPAMI.2016.2610973},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7748513,
author={V. Rengarajan and A. N. Rajagopalan and R. Aravind and G. Seetharaman},

journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Image Registration and Change Detection under Rolling Shutter Motion
Blur},
year={2017},
volume={39},
number={10},
pages={1959-1972},
abstract={In this paper, we address the problem of registering a distorted
image and a reference image of the same scene by estimating the camera motion
that had caused the distortion. We simultaneously detect the regions of changes
between the two images. We attend to the coalesced effect of rolling shutter
and motion blur that occurs frequently in moving CMOS cameras. We first model a
general image formation framework for a 3D scene following a layered approach
in the presence of rolling shutter and motion blur. We then develop an
algorithm which performs layered registration to detect changes. This algorithm
includes an optimisation problem that leverages the sparsity of the camera
trajectory in the pose space and the sparsity of changes in the spatial domain.
We create a synthetic dataset for change detection in the presence of motion
blur and rolling shutter effect covering different types of camera motion for
both planar and 3D scenes. We compare our method with existing registration
methods and also show several real examples captured with CMOS cameras.},
keywords={cameras;image registration;motion estimation;object
detection;optimisation;3D scene;CMOS cameras;camera motion estimation;camera
trajectory;change detection;distorted image;image formation framework;image
registration;layered registration;motion blur;optimisation;planar scenes;pose
space;region changes detection;rolling shutter;Cameras;Distortion;Kernel;Sensor
arrays;Solid modeling;Three-dimensional displays;Rolling shutter;aerial
imaging;change detection;image registration;motion blur},
doi={10.1109/TPAMI.2016.2630687},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7707464,
author={N. Hu and G. Englebienne and Z. Lou and B. Kröse},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Learning to Recognize Human Activities Using Soft Labels},
year={2017},
volume={39},
number={10},
pages={1973-1984},
abstract={Human activity recognition system is of great importance in robot-
care scenarios. Typically, training such a system requires activity labels to
be both completely and accurately annotated. In this paper, we go beyond such
restriction and propose a learning method that allow labels to be incomplete
and uncertain. We introduce the idea of soft labels which allows annotators to
assign multiple, and weighted labels to data segments. This is very useful in
many situations, e.g., when the labels are uncertain, when part of the labels
are missing, or when multiple annotators assign inconsistent labels. We
formulate the activity recognition task as a sequential labeling problem.
Latent variables are embedded in the model in order to exploit sub-level
semantics for better estimation. We propose a max-margin framework which
incorporate soft labels for learning the model parameters. The model is
evaluated on two challenging datasets. To simulate the uncertainty in data
annotation, we randomly change the labels for transition segments. The results
show significant improvement over the state-of-the-art approach.},
keywords={image motion analysis;learning (artificial intelligence);activity
labels;data annotation;data segments;human activity recognition system;latent
variables;learning method;max-margin framework;robot-care scenarios;sequential
labeling;soft labels;sublevel semantics;weighted labels;Data
models;Labeling;Robots;Support vector machines;Training;Uncertainty;RGB-
D perception;human activity recognition;max-margin learning},
doi={10.1109/TPAMI.2016.2621761},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7748481,
author={L. Gorelick and Y. Boykov and O. Veksler and I. B. Ayed and A. Delong},

journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Local Submodularization for Binary Pairwise Energies},
year={2017},
volume={39},
number={10},
pages={1985-1999},
abstract={Many computer vision problems require optimization of binary non-
submodular energies. We propose a general optimization framework based on local
submodular approximations (LSA). Unlike standard LP relaxation methods that
linearize the whole energy globally, our approach iteratively approximates the
energy locally. On the other hand, unlike standard local optimization methods
(e.g., gradient descent or projection techniques) we use non-linear submodular
approximations and optimize them without leaving the domain of integer
solutions. We discuss two specific LSA algorithms based on trust region and
auxiliary function principles, LSA-TR and LSA-AUX. The proposed methods obtain
state-of-the-art results on a wide range of applications such as binary
deconvolution, curvature regularization, inpainting, segmentation with
repulsion and two types of shape priors. Finally, we discuss a move-making
extension to the LSA-TR approach. While our paper is focused on pairwise
energies, our ideas extend to higher-order problems. The code is available
online.},
keywords={approximation theory;computer vision;optimisation;LSA-AUX;LSA-
TR;binary nonsubmodular energies;binary pairwise energies;computer vision;local
submodular approximations;nonlinear submodular
approximations;optimization;Approximation algorithms;Computer vision;Linear
approximation;Optimization;Standards;Taylor series;Upper bound;Discrete
optimization;auxiliary functions;graph cuts;local submodularization;trust
region},
doi={10.1109/TPAMI.2016.2630686},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7755776,
author={A. Tariq and A. Karim and H. Foroosh},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={NELasso: Group-Sparse Modeling for Characterizing Relations Among Named
Entities in News Articles},
year={2017},
volume={39},
number={10},
pages={2000-2014},
abstract={Named entities such as people, locations, and organizations play a
vital role in characterizing online content. They often reflect information of
interest and are frequently used in search queries. Although named entities can
be detected reliably from textual content, extracting relations among them is
more challenging, yet useful in various applications (e.g., news recommending
systems). In this paper, we present a novel model and system for learning
semantic relations among named entities from collections of news articles. We
model each named entity occurrence with sparse structured logistic regression,
and consider the words (predictors) to be grouped based on background
semantics. This sparse group LASSO approach forces the weights of word groups
that do not influence the prediction towards zero. The resulting sparse
structure is utilized for defining the type and strength of relations. Our
unsupervised system yields a named entities' network where each relation is
typed, quantified, and characterized in context. These relations are the key to
understanding news material over time and customizing newsfeeds for readers.
Extensive evaluation of our system on articles from TIME magazine and BBC News
shows that the learned relations correlate with static semantic relatedness
measures like WLM, and capture the evolving relationships among named entities
over time.},
keywords={compressed sensing;feature extraction;information resources;query
formulation;text analysis;unsupervised learning;word processing;BBC News
shows;LASSO sparse group;NELasso;TIME magazine;group-sparse modeling;named
entities;news articles;newsfeeds;online content;relations extraction;search
queries;semantic relations learning;sparse structured logistic
regression;textual content;unsupervised system;word groups;Context;Electronic
publishing;Encyclopedias;Internet;Semantics;Vocabulary;LASSO;Sparse group
learning;named entities;news understanding;semantic network construction},
doi={10.1109/TPAMI.2016.2632117},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7736984,
author={T. Pieciak and S. Aja-Fernández and G. Vegas-Sánchez-Ferrero},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Non-Stationary Rician Noise Estimation in Parallel MRI Using a Single
Image: A #x00A0;Variance-Stabilizing Approach},
year={2017},
volume={39},
number={10},
pages={2015-2029},
abstract={Parallel magnetic resonance imaging (pMRI) techniques have gained a
great importance both in research and clinical communities recently since they
considerably accelerate the image acquisition process. However, the image
reconstruction algorithms needed to correct the subsampling artifacts affect
the nature of noise, i.e., it becomes non-stationary. Some methods have been
proposed in the literature dealing with the non-stationary noise in pMRI.
However, their performance depends on information not usually available such as
multiple acquisitions, receiver noise matrices, sensitivity coil profiles,
reconstruction coefficients, or even biophysical models of the data. Besides,
some methods show an undesirable granular pattern on the estimates as a side
effect of local estimation. Finally, some methods make strong assumptions that
just hold in the case of high signal-to-noise ratio (SNR), which limits their
usability in real scenarios. We propose a new automatic noise estimation
technique for non-stationary Rician noise that overcomes the aforementioned
drawbacks. Its effectiveness is due to the derivation of a variance-stabilizing
transformation designed to deal with any SNR. The method was compared to the
main state-of-the-art methods in synthetic and real scenarios. Numerical
results confirm the robustness of the method and its better performance for the
whole range of SNRs.},
keywords={image reconstruction;image sampling;magnetic resonance
imaging;SNR;automatic noise estimation;image acquisition;image
reconstruction;local estimation;nonstationary Rician noise
estimation;pMRI;parallel MRI;parallel magnetic resonance imaging;signal-to-
noise ratio;subsampling artifacts;variance-stabilizing approach;variance-
stabilizing transformation;Data models;Estimation;Image reconstruction;Magnetic
resonance imaging;Receivers;Rician channels;Sensitivity;MRI;Rician
distribution;noise estimation;parallel MRI;spatially variant noise;variance-
stabilizing transformation},
doi={10.1109/TPAMI.2016.2625789},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7748514,
author={R. Peharz and R. Gens and F. Pernkopf and P. Domingos},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={On the Latent Variable Interpretation in Sum-Product Networks},
year={2017},
volume={39},
number={10},
pages={2030-2044},
abstract={One of the central themes in Sum-Product networks (SPNs) is the
interpretation of sum nodes as marginalized latent variables (LVs). This
interpretation yields an increased syntactic or semantic structure, allows the
application of the EM algorithm and to efficiently perform MPE inference. In
literature, the LV interpretation was justified by explicitly introducing the
indicator variables corresponding to the LVs’ states. However, as pointed out
in this paper, this approach is in conflict with the completeness condition in
SPNs and does not fully specify the probabilistic model. We propose a remedy
for this problem by modifying the original approach for introducing the LVs,
which we call SPN augmentation. We discuss conditional independencies in
augmented SPNs, formally establish the probabilistic interpretation of the sum-
weights and give an interpretation of augmented SPNs as Bayesian networks.
Based on these results, we find a sound derivation of the EM algorithm for
SPNs. Furthermore, the Viterbi-style algorithm for MPE proposed in literature
was never proven to be correct. We show that this is indeed a correct
algorithm, when applied to selective SPNs, and in particular when applied to
augmented SPNs. Our theoretical results are confirmed in experiments on
synthetic data and 103 real-world datasets.},
keywords={Bayes methods;Computational modeling;Inference algorithms;Mixture
models;Periodic structures;Probabilistic logic;Semantics;MPE inference;Sum-
product networks;expectation-maximization;latent variables;mixture models},
doi={10.1109/TPAMI.2016.2618381},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7723916,
author={K. Al Ismaeil and D. Aouada and T. Solignac and B. Mirbach and B.
Ottersten},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Real-Time Enhancement of Dynamic Depth Videos with Non-Rigid
Deformations},
year={2017},
volume={39},
number={10},
pages={2045-2059},
abstract={We propose a novel approach for enhancing depth videos containing
non-rigidly deforming objects. Depth sensors are capable of capturing depth
maps in real-time but suffer from high noise levels and low spatial
resolutions. While solutions for reconstructing 3D details in static scenes, or
scenes with rigid global motions have been recently proposed, handling
unconstrained non-rigid deformations in relative complex scenes remains a
challenge. Our solution consists in a recursive dynamic multi-frame
superresolution algorithm where the relative local 3D motions between
consecutive frames are directly accounted for. We rely on the assumption that
these 3D motions can be decoupled into lateral motions and radial
displacements. This allows to perform a simple local per-pixel tracking where
both depth measurements and deformations are dynamically optimized. The
geometric smoothness is subsequently added using a multi-level L1 minimization
with a bilateral total variation regularization. The performance of this method
is thoroughly evaluated on both real and synthetic data. As compared to
alternative approaches, the results show a clear improvement in reconstruction
accuracy and in robustness to noise, to relative large non-rigid deformations,
and to topological changes. Moreover, the proposed approach, implemented on a
CPU, is shown to be computationally efficient and working in real-time.},
keywords={Kalman filters;image recognition;image reconstruction;stereo image
processing;video signal processing;3D details;3D motions;CPU;bilateral total
variation regularization;depth maps;depth measurements;depth sensors;dynamic
depth videos;geometric smoothness;global motions;high noise levels;lateral
motions;local per-pixel tracking;low spatial resolutions;nonrigidly deforming
objects;radial displacements;real-time enhancement;recursive dynamic multiframe
superresolution algorithm;relative complex scenes;static scenes;unconstrained
nonrigid deformations;Cameras;Heuristic algorithms;Image resolution;Real-time
systems;Three-dimensional displays;Two dimensional displays;Videos;Depth
enhancement;Kalman filtering;bilateral total variation;non-rigid
deformations;registration;super-resolution},
doi={10.1109/TPAMI.2016.2622698},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7728137,
author={Z. Hui and A. C. Sankaranarayanan},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Shape and Spatially-Varying Reflectance Estimation from Virtual
Exemplars},
year={2017},
volume={39},
number={10},
pages={2060-2073},
abstract={This paper addresses the problem of estimating the shape of objects
that exhibit spatially-varying reflectance. We assume that multiple images of
the object are obtained under a fixed view-point and varying illumination,
i.e., the setting of photometric stereo. At the core of our techniques is the
assumption that the BRDF at each pixel lies in the non-negative span of a known
BRDF dictionary. This assumption enables a per-pixel surface normal and BRDF
estimation framework that is computationally tractable and requires no
initialization in spite of the underlying problem being non-convex. Our
estimation framework first solves for the surface normal at each pixel using a
variant of example-based photometric stereo. We design an efficient multi-scale
search strategy for estimating the surface normal and subsequently, refine this
estimate using a gradient descent procedure. Given the surface normal estimate,
we solve for the spatially-varying BRDF by constraining the BRDF at each pixel
to be in the span of the BRDF dictionary; here, we use additional priors to
further regularize the solution. A hallmark of our approach is that it does not
require iterative optimization techniques nor the need for careful
initialization, both of which are endemic to most state-of-the-art techniques.
We showcase the performance of our technique on a wide range of simulated and
real scenes where we outperform competing methods.},
keywords={gradient methods;stereo image processing;BRDF dictionary;BRDF
estimation framework;example-based photometric stereo;gradient
descent;multiscale search strategy;shape estimation;spatially-varying
BRDF;spatially-varying reflectance estimation;surface normal estimation;virtual
exemplars;Computational modeling;Dictionaries;Estimation;Inverse
problems;Lighting;Optimization;Shape;BRDF estimation;Photometric
stereo;dictionaries;spatially varying BRDF},
doi={10.1109/TPAMI.2016.2623613},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7707434,
author={L. Wang and G. Hua and R. Sukthankar and J. Xue and Z. Niu and N.
Zheng},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Video Object Discovery and Co-Segmentation with Extremely Weak
Supervision},
year={2017},
volume={39},
number={10},
pages={2074-2088},
abstract={We present a spatio-temporal energy minimization formulation for
simultaneous video object discovery and co-segmentation across multiple videos
containing irrelevant frames. Our approach overcomes a limitation that most
existing video co-segmentation methods possess, i.e., they perform poorly when
dealing with practical videos in which the target objects are not present in
many frames. Our formulation incorporates a spatio-temporal auto-context model,
which is combined with appearance modeling for superpixel labeling. The
superpixel-level labels are propagated to the frame level through a multiple
instance boosting algorithm with spatial reasoning, based on which frames
containing the target object are identified. Our method only needs to be
bootstrapped with the frame-level labels for a few video frames (e.g., usually
1 to 3) to indicate if they contain the target objects or not. Extensive
experiments on four datasets validate the efficacy of our proposed method: 1)
object segmentation from a single video on the SegTrack dataset, 2) object co-
segmentation from multiple videos on a video co-segmentation dataset, and 3)
joint object discovery and co-segmentation from multiple videos containing
irrelevant frames on the MOViCS dataset and XJTU-Stevens, a new dataset that we
introduce in this paper. The proposed method compares favorably with the state-
of-the-art in all of these experiments.},
keywords={image segmentation;minimisation;spatial reasoning;video signal
processing;MOViCS dataset;SegTrack dataset;XJTU-Stevens dataset;appearance
modeling;extremely weak supervision;frame-level labels;multiple instance
boosting algorithm;simultaneous video object discovery;spatial
reasoning;spatio-temporal autocontext model;spatio-temporal energy
minimization;superpixel labeling;superpixel-level labels;video object
cosegmentation;Color;Image color analysis;Labeling;Minimization;Object
segmentation;Proposals;Shape;Spatial-MILBoost;Video object discovery;spatio-
temporal auto-context model;video object co-segmentation},
doi={10.1109/TPAMI.2016.2612187},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7740886,
author={A. Habibian and T. Mensink and C. G. M. Snoek},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Video2vec Embeddings Recognize Events When Examples Are Scarce},
year={2017},
volume={39},
number={10},
pages={2089-2103},
abstract={This paper aims for event recognition when video examples are scarce
or even completely absent. The key in such a challenging setting is a semantic
video representation. Rather than building the representation from individual
attribute detectors and their annotations, we propose to learn the entire
representation from freely available web videos and their descriptions using an
embedding between video features and term vectors. In our proposed embedding,
which we call Video2vec, the correlations between the words are utilized to
learn a more effective representation by optimizing a joint objective balancing
descriptiveness and predictability. We show how learning the Video2vec
embedding using a multimodal predictability loss, including appearance, motion
and audio features, results in a better predictable representation. We also
propose an event specific variant of Video2vec to learn a more accurate
representation for the words, which are indicative of the event, by introducing
a term sensitive descriptiveness loss. Our experiments on three challenging
collections of web videos from the NIST TRECVID Multimedia Event Detection and
Columbia Consumer Videos datasets demonstrate: i) the advantages of Video2vec
over representations using attributes or alternative embeddings, ii) the
benefit of fusing video modalities by an embedding over common strategies, iii)
the complementarity of term sensitive descriptiveness and multimodal
predictability for event recognition. By its ability to improve predictability
of present day audiovisual video features, while at the same time maximizing
their semantic descriptiveness, Video2vec leads to state-of-the-art accuracy
for both fewand zero-example recognition of events in video.},
keywords={image representation;learning (artificial intelligence);vectors;video
signal processing;Columbia Consumer Videos datasets;NIST TRECVID multimedia
event detection;Video2vec embeddings;audiovisual video features;event
recognition;joint objective balancing descriptiveness;learning;multimodal
predictability loss;semantic descriptiveness;semantic video representation;term
sensitive descriptiveness loss;term vectors;video
modalities;Correlation;Feature
extraction;NIST;Semantics;Training;Vehicles;Visualization;Event
recognition;representation learning;semantic video representation},
doi={10.1109/TPAMI.2016.2627563},
ISSN={0162-8828},
month={Oct},}
@ARTICLE{7676344,
author={L. Wang and Z. Xiong and G. Shi and F. Wu and W. Zeng},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Adaptive Nonlocal Sparse Representation for Dual-Camera Compressive
Hyperspectral Imaging},
year={2017},
volume={39},
number={10},
pages={2104-2111},
abstract={Leveraging the compressive sensing (CS) theory, coded aperture
snapshot spectral imaging (CASSI) provides an efficient solution to recover 3D
hyperspectral data from a 2D measurement. The dual-camera design of CASSI, by
adding an uncoded panchromatic measurement, enhances the reconstruction
fidelity while maintaining the snapshot advantage. In this paper, we propose an
adaptive nonlocal sparse representation (ANSR) model to boost the performance
of dual-camera compressive hyperspectral imaging (DCCHI). Specifically, the CS
reconstruction problem is formulated as a 3D cube based sparse representation
to make full use of the nonlocal similarity in both the spatial and spectral
domains. Our key observation is that, the panchromatic image, besides playing
the role of direct measurement, can be further exploited to help the nonlocal
similarity estimation. Therefore, we design a joint similarity metric by
adaptively combining the internal similarity within the reconstructed
hyperspectral image and the external similarity within the panchromatic image.
In this way, the fidelity of CS reconstruction is greatly enhanced. Both
simulation and hardware experimental results show significant improvement of
the proposed method over the state-of-the-art.},
keywords={cameras;compressed sensing;hyperspectral imaging;image
reconstruction;image representation;stereo image processing;3D cube based
sparse representation;3D hyperspectral data recovery;ANSR;CASSI;CS
reconstruction problem;CS theory;DCCHI;adaptive nonlocal sparse
representation;coded aperture snapshot spectral imaging;compressive
sensing;dual-camera compressive hyperspectral imaging;nonlocal similarity
estimation;panchromatic image;reconstruction fidelity;uncoded panchromatic
measurement;Dictionaries;Hyperspectral imaging;Image coding;Image
reconstruction;Spectral analysis;Three-dimensional displays;Two dimensional
displays;Compressive sensing;dual-camera;hyperspectral imaging;nonlocal
similarity;sparse representation},
doi={10.1109/TPAMI.2016.2621050},
ISSN={0162-8828},
month={Oct},}
