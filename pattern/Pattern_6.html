<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"></head><body>@ARTICLE{7494603,
<br>
author={M. A. Ferrer and M. Diaz and C. Carmona-Duarte and A. Morales},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={A Behavioral Handwriting Model for Static and Dynamic Signature Synthesis}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1041-1053},
<br>  abstract={The synthetic generation of static handwritten 
signatures based on motor equivalence theory has been recently proposed 
for biometric applications. Motor equivalence divides the human 
handwriting action into an effector dependent cognitive level and an 
effector independent motor level. The first level has been suggested by 
others as an engram, generated through a spatial grid, and the second 
has been emulated with kinematic filters. Our paper proposes a 
development of this methodology in which we generate dynamic information
 and provide a unified comprehensive synthesizer for both static and 
dynamic signature synthesis. The dynamics are calculated by lognormal 
sampling of the 8-connected continuous signature trajectory, which 
includes, as a novelty, the pen-ups. The forgery generation imitates a 
signature by extracting the most perceptually relevant points of the 
given genuine signature and interpolating them. The capacity to 
synthesize both static and dynamic signatures using a unique model is 
evaluated according to its ability to adapt to the static and dynamic 
signature inter-and intra-personal variability. Our highly promising 
results suggest the possibility of using the synthesizer in different 
areas beyond the generation of unlimited databases for biometric 
training.},
<br>  keywords={authorisation;biometrics (access control);feature 
extraction;handwriting recognition;optical character 
recognition;behavioral handwriting model;biometric 
applications;biometric training;dynamic signature inter-personal 
variability;dynamic signature intra-personal variability;dynamic 
signature synthesis;human handwriting action;lognormal sampling;motor 
equivalence theory;static handwritten signatures;static signature 
inter-personal variability;static signature intra-personal 
variability;static signature synthesis;Analytical 
models;Databases;Forgery;Morphology;Motor 
drives;Synthesizers;Trajectory;Biometric recognition;kinematic theory of
 human movement;motor equivalence theory;on-line and off-line synthetic 
generation;signature verification},
<br>  doi={10.1109/TPAMI.2016.2582167},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7482852,
<br>
author={Q. Nguyen and F. Tudisco and A. Gautier and M. Hein},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={An Efficient Multilinear Optimization Framework for Hypergraph Matching}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1054-1075},
<br>  abstract={Hypergraph matching has recently become a popular 
approach for solving correspondence problems in computer vision as it 
allows the use of higher-order geometric information. Hypergraph 
matching can be formulated as a third-order optimization problem subject
 to assignment constraints which turns out to be NP-hard. In recent 
work, we have proposed an algorithm for hypergraph matching which first 
lifts the third-order problem to a fourth-order problem and then solves 
the fourth-order problem via optimization of the corresponding 
multilinear form. This leads to a tensor block coordinate ascent scheme 
which has the guarantee of providing monotonic ascent in the original 
matching score function and leads to state-of-the-art performance both 
in terms of achieved matching score and accuracy. In this paper we show 
that the lifting step to a fourth-order problem can be avoided yielding a
 third-order scheme with the same guarantees and performance but being 
two times faster. Moreover, we introduce a homotopy type method which 
further improves the performance.},
<br>  keywords={computational complexity;computer vision;graph 
theory;image matching;optimisation;tensors;NP-hard problem;computer 
vision;efficient multilinear optimization framework;fourth-order 
problem;higher-order geometric information;hypergraph matching;original 
matching score function;tensor block coordinate ascent 
scheme;third-order optimization problem;Algorithm design and 
analysis;Approximation algorithms;Computer vision;Optimization;Pattern 
matching;Tensile stress;Three-dimensional displays;Hypergraph 
Matching;block coordinate ascent;multilinear form;tensor},
<br>  doi={10.1109/TPAMI.2016.2574706},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7506010,
<br>
author={H. Cevikalp},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Best Fitting Hyperplanes for Classification}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1076-1088},
<br>  abstract={In this paper, we propose novel methods that are more 
suitable than classical large-margin classifiers for open set 
recognition and object detection tasks. The proposed methods use the 
best fitting hyperplanes approach, and the main idea is to find the best
 fitting hyperplanes such that each hyperplane is close to the samples 
of one of the classes and is as far as possible from the other class 
samples. To this end, we propose two different classifiers: The first 
classifier solves a convex quadratic optimization problem, but negative 
samples can lie on one side of the best fitting hyperplane. The second 
classifier, however, allows the negative samples to lie on both sides of
 the fitting hyperplane by using concave-convex procedure. Both methods 
are extended to the nonlinear case by using the kernel trick. In 
contrast to the existing hyperplane fitting classifiers in the 
literature, our proposed methods are suitable for large-scale problems, 
and they return sparse solutions. The experiments on several databases 
show that the proposed methods typically outperform other hyperplane 
fitting classifiers, and they work as good as the SVM classifier in 
classical recognition tasks. However, the proposed methods significantly
 outperform SVM in open set recognition and object detection tasks.},
<br>  keywords={concave programming;convex programming;image 
classification;object detection;object recognition;quadratic 
programming;sparse matrices;support vector machines;visual databases;SVM
 classifier;best-fitting hyperlane classifier;concave-convex 
procedure;convex quadratic optimization problem;kernel 
methods;large-margin classifiers;large-scale problems;object detection 
task;open set recognition task;sparse solutions;Eigenvalues and 
eigenfunctions;Kernel;Object detection;Optimization;Support vector 
machines;Testing;Training;Best fitting hyperlane classifier;kernel 
methods;large margin classifier;open set recognition;support vector 
machines},
<br>  doi={10.1109/TPAMI.2016.2587647},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7469374,
<br>
author={L. Lin and G. Wang and W. Zuo and X. Feng and L. Zhang},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Cross-Domain Visual Matching via Generalized Similarity Measure and Feature Learning}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1089-1102},
<br>  abstract={Cross-domain visual data matching is one of the 
fundamental problems in many real-world vision tasks, e.g., matching 
persons across ID photos and surveillance videos. Conventional 
approaches to this problem usually involves two steps: i) projecting 
samples from different domains into a common space, and ii) computing 
(dis-)similarity in this space based on a certain distance. In this 
paper, we present a novel pairwise similarity measure that advances 
existing models by i) expanding traditional linear projections into 
affine transformations and ii) fusing affine Mahalanobis distance and 
Cosine similarity by a data-driven combination. Moreover, we unify our 
similarity measure with feature representation learning via deep 
convolutional neural networks. Specifically, we incorporate the 
similarity measure matrix into the deep architecture, enabling an 
end-to-end way of model optimization. We extensively evaluate our 
generalized similarity model in several challenging cross-domain 
matching tasks: person re-identification under different views and face 
verification over different modalities (i.e., faces from still images 
and videos, older and younger faces, and sketch and photo portraits). 
The experimental results demonstrate superior performance of our model 
over other state-of-the-art methods.},
<br>  keywords={computer vision;data handling;face recognition;image 
matching;image representation;learning (artificial intelligence);matrix 
algebra;neural nets;Mahalanobis distance;cosine similarity;cross-domain 
visual data matching;deep convolutional neural networks;face 
verification;feature representation learning;generalized similarity 
measurement;pairwise similarity measurement;person 
re-identification;real-world vision tasks;similarity measure 
matrix;Euclidean distance;Face;Neural networks;Pattern 
matching;Videos;Visualization;Similarity model;cross-domain 
matching;deep learning;person verification},
<br>  doi={10.1109/TPAMI.2016.2567386},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7487020,
<br>
author={B. Gu and V. S. Sheng and K. Y. Tay and W. Romano and S. Li},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Cross Validation Through Two-Dimensional Solution Surface for Cost-Sensitive SVM}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1103-1121},
<br>  abstract={Model selection plays an important role in 
cost-sensitive SVM (CS-SVM). It has been proven that the global minimum 
cross validation (CV) error can be efficiently computed based on the 
solution path for one parameter learning problems. However, it is a 
challenge to obtain the global minimum CV error for CS-SVM based on 
one-dimensional solution path and traditional grid search, because 
CS-SVM is with two regularization parameters. In this paper, we propose a
 solution and error surfaces based CV approach (CV-SES). More 
specifically, we first compute a two-dimensional solution surface for 
CS-SVM based on a bi-parameter space partition algorithm, which can fit 
solutions of CS-SVM for all values of both regularization parameters. 
Then, we compute a two-dimensional validation error surface for each CV 
fold, which can fit validation errors of CS-SVM for all values of both 
regularization parameters. Finally, we obtain the CV error surface by 
superposing K validation error surfaces, which can find the global 
minimum CV error of CSSVM. Experiments are conducted on seven datasets 
for cost sensitive learning and on four datasets for imbalanced 
learning. Experimental results not only show that our proposed CV-SES 
has a better generalization ability than CS-SVM with various hybrids 
between grid search and solution path methods, and than recent proposed 
cost-sensitive hinge loss SVM with three-dimensional grid search, but 
also show that CV-SES uses less running time.},
<br>  keywords={generalisation (artificial intelligence);learning 
(artificial intelligence);support vector machines;CS-SVM;CV error 
surface;CV-SES;K-validation error surfaces;biparameter space partition 
algorithm;cost-sensitive SVM;generalization ability;global minimum CV 
error;global minimum cross validation error;grid search;imbalanced 
learning;parameter learning problem;regularization parameter;solution 
path methods;solution-and-error surface-based CV 
approach;two-dimensional solution surface;two-dimensional validation 
error surface;Computational modeling;Fasteners;Kernel;Search 
methods;Space exploration;Support vector machines;Training;Solution 
surface;cost-sensitive support vector machine;cross validation;solution 
path;space partition},
<br>  doi={10.1109/TPAMI.2016.2578326},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7494641,
<br>
author={D. Wang and C. Otto and A. K. Jain},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Face Search at Scale}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1122-1136},
<br>  abstract={Given the prevalence of social media websites, one 
challenge facing computer vision researchers is to devise methods to 
search for persons of interest among the billions of shared photos on 
these websites. Despite significant progress in face recognition, 
searching a large collection of unconstrained face images remains a 
difficult problem. To address this challenge, we propose a face search 
system which combines a fast search procedure, coupled with a 
state-of-the-art commercial off the shelf (COTS) matcher, in a cascaded 
framework. Given a probe face, we first filter the large gallery of 
photos to find the top-k most similar faces using features learned by a 
convolutional neural network. The k retrieved candidates are re-ranked 
by combining similarities based on deep features and those output by the
 COTS matcher. We evaluate the proposed face search system on a gallery 
containing 80 million web-downloaded face images. Experimental results 
demonstrate that while the deep features perform worse than the COTS 
matcher on a mugshot dataset (93.7 percent versus 98.6 percent TAR@FAR 
of 0.01 percent), fusing the deep features with the COTS matcher 
improves the overall performance (99.5 percent TAR@FAR of 0.01 percent).
 This shows that the learned deep features provide complementary 
information over representations used in state-of-the-art face matchers.
 On the unconstrained face image benchmarks, the performance of the 
learned deep features is competitive with reported accuracies. LFW 
database: 98.20 percent accuracy under the standard protocol and 88.03 
percent TAR@FAR of 0.1 percent under the BLUFR protocol; IJB-A 
benchmark: 51.0 percent TAR@FAR of 0.1 percent (verification), rank 1 
retrieval of 82.2 percent (closed-set search), 61.5 percent FNIR@FAR of 1
 percent (open-set search). The proposed face search system offers an 
excellent trade-off between accuracy and scalability on galleries with 
millions of images. Additionally, in a face search experiment inv- lving
 photos of the Tsarnaev brothers, convicted of the Boston Marathon 
bombing, the proposed cascade face search system could find the younger 
brother's (Dzhokhar Tsarnaev) photo at rank 1 in 1 second on a 5 M 
gallery and at rank 8 in 7 seconds on an 80 M gallery.},
<br>  keywords={Web sites;face recognition;learning (artificial 
intelligence);neural nets;social networking (online);BLUFR protocol;COTS
 matcher;FNIR@FAR;IJB-A benchmark;LFW database;TAR@FAR;Web-downloaded 
face images;cascaded framework;commercial-off-the-shelf matcher;computer
 vision;convolutional neural network;face recognition;face search 
experiment;face search system;mugshot dataset;social media 
Websites;unconstrained face image benchmarks;Benchmark testing;Face;Face
 recognition;Media;Probes;Protocols;Search problems;Face search;cascaded
 system;deep learning;large face collections;scalability;unconstrained 
face recognition},
<br>  doi={10.1109/TPAMI.2016.2582166},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7485869,
<br>
author={S. Ren and K. He and R. Girshick and J. Sun},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1137-1149},
<br>  abstract={State-of-the-art object detection networks depend on 
region proposal algorithms to hypothesize object locations. Advances 
like SPPnet [1] and Fast R-CNN [2] have reduced the running time of 
these detection networks, exposing region proposal computation as a 
bottleneck. In this work, we introduce a Region Proposal Network(RPN) 
that shares full-image convolutional features with the detection 
network, thus enabling nearly cost-free region proposals. An RPN is a 
fully convolutional network that simultaneously predicts object bounds 
and objectness scores at each position. The RPN is trained end-to-end to
 generate high-quality region proposals, which are used by Fast R-CNN 
for detection. We further merge RPN and Fast R-CNN into a single network
 by sharing their convolutional features-using the recently popular 
terminology of neural networks with 'attention' mechanisms, the RPN 
component tells the unified network where to look. For the very deep 
VGG-16 model [3], our detection system has a frame rate of 5 fps 
(including all steps) on a GPU, while achieving state-of-the-art object 
detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with 
only 300 proposals per image. In ILSVRC and COCO 2015 competitions, 
Faster R-CNN and RPN are the foundations of the 1st-place winning 
entries in several tracks. Code has been made publicly available.},
<br>  keywords={graphics processing units;neural nets;object 
detection;COCO 2015 competitions;GPU;ILSVRC;MS COCO datasets;PASCAL VOC 
2007;PASCAL VOC 2012;RPN;attention mechanisms;deep VGG-16 
model;faster-R-CNN;full-image convolutional features;high-quality region
 proposals;object detection accuracy;real-time object detection;region 
proposal networks;Convolutional codes;Detectors;Feature 
extraction;Object detection;Proposals;Search problems;Training;Object 
detection;convolutional neural network;region proposal},
<br>  doi={10.1109/TPAMI.2016.2577031},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7469791,
<br>
author={L. Wang and M. Chen and M. Rodrigues and D. Wilcox and R. Calderbank and L. Carin},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Information-Theoretic Compressive Measurement Design}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1150-1164},
<br>  abstract={An information-theoretic projection design framework is 
proposed, of interest for feature design and compressive measurements. 
Both Gaussian and Poisson measurement models are considered. The 
gradient of a proposed information-theoretic metric (ITM) is derived, 
and a gradient-descent algorithm is applied in design; connections are 
made to the information bottleneck. The fundamental solution structure 
of such design is revealed in the case of a Gaussian measurement model 
and arbitrary input statistics. This new theoretical result reveals how 
ITM parameter settings impact the number of needed projection 
measurements, with this verified experimentally. The ITM achieves 
promising results on real data, for both signal recovery and 
classification.},
<br>  keywords={compressed sensing;signal classification;Gaussian 
measurement models;Poisson measurement models;feature 
design;information-theoretic compressive measurement 
design;information-theoretic metric;information-theoretic projection 
design framework;signal classification;signal recovery;Algorithm design 
and analysis;Analytical models;Energy measurement;Measurement 
uncertainty;Mutual information;Noise measurement;Information-theoretic 
metric;compressive sensing;gradient of mutual information;information 
bottleneck;projection design},
<br>  doi={10.1109/TPAMI.2016.2568189},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7482729,
<br>
author={P. Wei and Y. Zhao and N. Zheng and S. C. Zhu},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Modeling 4D Human-Object Interactions for Joint Event Segmentation, Recognition, and Object Localization}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1165-1179},
<br>  abstract={In this paper, we present a 4D human-object interaction 
(4DHOI) model for solving three vision tasks jointly: i) event 
segmentation from a video sequence, ii) event recognition and parsing, 
and iii) contextual object localization. The 4DHOI model represents the 
geometric, temporal, and semantic relations in daily events involving 
human-object interactions. In 3D space, the interactions of human poses 
and contextual objects are modeled by semantic co-occurrence and 
geometric compatibility. On the time axis, the interactions are 
represented as a sequence of atomic event transitions with coherent 
objects. The 4DHOI model is a hierarchical spatial-temporal graph 
representation which can be used for inferring scene functionality and 
object affordance. The graph structures and parameters are learned using
 an ordered expectation maximization algorithm which mines the 
spatial-temporal structures of events from RGB-D video samples. Given an
 input RGB-D video, the inference is performed by a dynamic programming 
beam search algorithm which simultaneously carries out event 
segmentation, recognition, and object localization. We collected a large
 multiview RGB-D event dataset which contains 3,815 video sequences and 
383,036 RGB-D frames captured by three RGB-D cameras. The experimental 
results on three challenging datasets demonstrate the strength of the 
proposed method.},
<br>  keywords={computer vision;dynamic 
programming;expectation-maximisation algorithm;graph theory;image 
capture;image colour analysis;image recognition;image segmentation;image
 sequences;learning (artificial intelligence);stereo image 
processing;video signal processing;3D space;4D-human-object 
interactions;4DHOI model;RGB-D cameras;RGB-D frames capture;RGB-D video 
samples;contextual object localization;dynamic programming beam search 
algorithm;expectation maximization algorithm;graph parameters;graph 
structures;hierarchical spatial-temporal graph representation;joint 
event recognition;joint event segmentation;multiview RGB-D event 
dataset;video sequence;vision tasks;Context modeling;Hidden Markov 
models;Robots;Semantics;Solid modeling;Three-dimensional displays;Video 
sequences;Human-object interaction;event recognition;object 
affordance;object localization;sequence segmentation},
<br>  doi={10.1109/TPAMI.2016.2574712},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7478642,
<br>
author={D. Keysers and T. Deselaers and H. A. Rowley and L. L. Wang and V. Carbune},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Multi-Language Online Handwriting Recognition}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1180-1194},
<br>  abstract={We describe Google's online handwriting recognition 
system that currently supports 22 scripts and 97 languages. The system's
 focus is on fast, high-accuracy text entry for mobile, touch-enabled 
devices. We use a combination of state-of-the-art components and combine
 them with novel additions in a flexible framework. This architecture 
allows us to easily transfer improvements between languages and scripts.
 This made it possible to build recognizers for languages that, to the 
best of our knowledge, are not handled by any other online handwriting 
recognition system. The approach also enabled us to use the same 
architecture both on very powerful machines for recognition in the cloud
 as well as on mobile devices with more limited computational power by 
changing some of the settings of the system. In this paper we give a 
general overview of the system architecture and the novel components, 
such as unified timeand position-based input interpretation, trainable 
segmentation, minimum-error rate training for feature combination, and a
 cascade of pruning strategies. We present experimental results for 
different setups. The system is currently publicly available in several 
Google products, for example in Google Translate and as an input method 
for Android devices.},
<br>  keywords={handwriting recognition;mobile computing;natural 
language processing;optical character recognition;smart phones;text 
analysis;Android devices;Google Translate;Google online handwriting 
recognition system;Google products;high-accuracy text entry;mobile 
devices;multilanguage online handwriting recognition;touch-enabled 
devices;Character recognition;Google;Handwriting recognition;Hidden 
Markov models;Ink;Training;Writing;Online handwriting 
recognition;handwriting recognition},
<br>  doi={10.1109/TPAMI.2016.2572693},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7487047,
<br>
author={D. Tward and M. Miller and A. Trouvé and L. Younes},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Parametric Surface Diffeomorphometry for Low Dimensional Embeddings of Dense Segmentations and Imagery}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1195-1208},
<br>  abstract={In the field of Computational Anatomy, biological form 
(including our focus, neuroanatomy) is studied quantitatively through 
the action of the diffeomorphism group on example anatomies - a 
technique called diffeomorphometry. Here we design an algorithm within 
this framework to pass from dense objects common in neuromaging studies 
(binary segmentations, structural images) to a sparse representation 
defined on the surface boundaries of anatomical structures, and embedded
 into the low dimensional coordinates of a parametric model. Our main 
new contribution is to introduce an expanded group action to 
simultaneously deform surfaces through direct mapping of points, as well
 as images through functional composition with the inverse. This allows 
us to index the diffeomorphisms with respect to two-dimensional surface 
geometries like subcortical gray matter structures, but explicitly map 
onto cost functions determined by noisy 3-dimensional measurements. We 
consider models generated from empirical covariance of training data, as
 well as bandlimited (Laplace-Beltrami eigenfunction) models when no 
such data is available. We show applications to noisy or anomalous 
segmentations, and other typical problems in neuroimaging studies. We 
reproduce statistical results detecting changes in Alzheimer's disease, 
despite dimensionality reduction. Lastly we apply our algorithm to the 
common problem of segmenting subcortical structures from T1 MR images.},
<br>  keywords={biomedical MRI;brain;eigenvalues and 
eigenfunctions;image representation;image segmentation;medical image 
processing;neurophysiology;Alzheimer's disease;Laplace-Beltrami 
eigenfunction;T1 MR images;binary segmentations;computational 
anatomy;dense segmentations;diffeomorphism;low dimensional 
embeddings;neuroanatomy;neuromaging;noisy 3-dimensional 
measurements;parametric model;parametric surface 
diffeomorphometry;sparse representation;structural images;subcortical 
gray matter;two-dimensional surface geometries;Biological system 
modeling;Diseases;Hippocampus;Image segmentation;Magnetic resonance 
imaging;Measurement;Shape;Computational 
anatomy;diffeomorphometry;medical imaging;neuroimaging;shape analysis},
<br>  doi={10.1109/TPAMI.2016.2578317},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7494617,
<br>
author={C. Xu and L. Zhang and L. Cheng and R. Koch},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Pose Estimation from Line Correspondences: A Complete Analysis and a Series of Solutions}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1209-1222},
<br>  abstract={In this paper we deal with the camera pose estimation 
problem from a set of 2D/3D line correspondences, which is also known as
 PnL (Perspective-n-Line) problem. We carry out our study by comparing 
PnL with the well-studied PnP (Perspective-n-Point) problem, and our 
contributions are three-fold: (1) We provide a complete 3D configuration
 analysis for P3L, which includes the wellknown P3P problem as well as 
several existing analyses as special cases. (2) By exploring the 
similarity between PnL and PnP, we propose a new subset-based PnL 
approach as well as a series of linear-formulation-based PnL approaches 
inspired by their PnP counterparts. (3) The proposed 
linear-formulation-based methods can be easily extended to deal with the
 line and point features simultaneously.},
<br>  keywords={cameras;pose estimation;2D line correspondences;3D 
configuration analysis;3D line correspondences;P3L problem;P3P 
problem;PnP problem;camera pose estimation problem;line 
features;linear-formulation-based PnL approach;perspective-n-line 
problem;perspective-n-point problem;point features;subset-based PnL 
approach;Cameras;Computational complexity;Iterative methods;Mathematical
 model;Pose estimation;Three-dimensional 
displays;Perspective-3-Line;camera pose estimation;configuration 
analysis;perspective-n-line},
<br>  doi={10.1109/TPAMI.2016.2582162},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7486981,
<br>
author={W. Zhang and L. Zhang and Z. Jin and R. Jin and D. Cai and X. Li and R. Liang and X. He},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Sparse Learning with Stochastic Composite Optimization}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1223-1236},
<br>  abstract={In this paper, we study Stochastic Composite 
Optimization (SCO) for sparse learning that aims to learn a sparse 
solution from a composite function. Most of the recent SCO algorithms 
have already reached the optimal expected convergence rate O(1/λT), but 
they often fail to deliver sparse solutions at the end either due to the
 limited sparsity regularization during stochastic optimization (SO) or 
due to the limitation in online-to-batch conversion. Even when the 
objective function is strongly convex, their high probability bounds can
 only attain O(√(log (1/δ)/T)) with d is the failure probability, which 
is much worse than the expected convergence rate. To address these 
limitations, we propose a simple yet effective two-phase Stochastic 
Composite Optimization scheme by adding a novel powerful sparse 
online-to-batch conversion to the general Stochastic Optimization 
algorithms. We further develop three concrete algorithms, OptimalSL, 
LastSL and AverageSL, directly under our scheme to prove the 
effectiveness of the proposed scheme. Both the theoretical analysis and 
the experiment results show that our methods can really outperform the 
existing methods at the ability of sparse learning and at the meantime 
we can improve the high probability bound to approximately O(log (log 
(T)/δ)/λT).},
<br>  keywords={approximation theory;computational 
complexity;convergence;convex programming;learning (artificial 
intelligence);probability;stochastic 
programming;AverageSL;LastSL;OptimalSL;SCO algorithms;SO;composite 
function;concrete algorithms;failure probability;optimal expected 
convergence rate;sparse learning;sparse online-to-batch 
conversion;stochastic composite optimization;Algorithm design and 
analysis;Convergence;Linear 
programming;Optimization;Standards;Stochastic processes;Training;Sparse 
learning;stochastic composite optimization;stochastic optimization},
<br>  doi={10.1109/TPAMI.2016.2578323},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7487003,
<br>
author={T. Collins and A. Bartoli},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Planar Structure-from-Motion with Affine Camera Models: Closed-Form Solutions, Ambiguities and Degeneracy Analysis}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1237-1255},
<br>  abstract={Planar Structure-from-Motion (SfM) is the problem of 
reconstructing a planar object or surface from a set of 2D images using 
motion information. The problem is well-understood with the perspective 
camera model and can be solved with Homography Decomposition (HD). 
However when the structure is small and/or viewed far from the camera 
the perspective effects diminish, and in the limit the projections 
become affine. In these situations HD fails because the problem itself 
becomes ill-posed. We propose a stable alternative using affine camera 
models. These have been used extensively to reconstruct non-planar 
structures, however a general, accurate and closed-form method for 
planar structures has been missing. The problem is fundamentally 
different with planar structures because the types of affine camera 
models one can use are more restricted and it is inherently more 
ambiguous and non-linear. We provide a closed-form method for the 
orthographic camera model that solves the general problem (three or more
 views with three or more correspondences and missing correspondences) 
and returns all metric structure solutions and corresponding camera 
poses. The method does not require initialisation, and optimises an 
objective function that is very similar to the reprojection error. In 
fact there is no clear benefit in refining its solutions with bundle 
adjustment, which is a remarkable result. We also present a new 
theoretical analysis that deepens our understanding of the problem. The 
main result is the necessary and sufficient geometric conditions for the
 problem to be degenerate with the orthographic camera. We also show 
there can exist up to two solutions for metric structure with four or 
more views (previously it was assumed to be unique), and we give the 
necessary and sufficient geometric conditions for disambiguation. Other 
theoretical results include showing that in the case of three images the
 optimal reconstruction (with respect to reprojection error) can usually
 be foun- in closed-form, and additional prior knowledge needed to solve
 with non-orthographic affine cameras.},
<br>  keywords={cameras;image motion analysis;image 
reconstruction;optimisation;pose estimation;2D images;HD;SfM;camera 
poses;closed-form solutions;degeneracy analysis;homography 
decomposition;metric structure solutions;necessary-and-sufficient 
geometric conditions;nonorthographic affine cameras;nonplanar structure 
reconstruction;orthographic camera model;planar object reconstruction 
problem;planar structure-from-motion;reprojection 
error;Cameras;Closed-form solutions;Computational modeling;Image 
reconstruction;Measurement;Transforms;Transmission line matrix 
methods;Structure-from-Motion;ambiguity;critical 
motion;degeneracy;factorization;orthographic;para-perspective;plane;stratification;weak-perspective},
<br>  doi={10.1109/TPAMI.2016.2578333},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7527621,
<br>
author={Y. Chen and T. Pock},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Trainable Nonlinear Reaction Diffusion: A Flexible Framework for Fast and Effective Image Restoration}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1256-1272},
<br>  abstract={Image restoration is a long-standing problem in 
low-level computer vision with many interesting applications. We 
describe a flexible learning framework based on the concept of nonlinear
 reaction diffusion models for various image restoration problems. By 
embodying recent improvements in nonlinear diffusion models, we propose a
 dynamic nonlinear reaction diffusion model with time-dependent 
parameters (i.e., linear filters and influence functions). In contrast 
to previous nonlinear diffusion models, all the parameters, including 
the filters and the influence functions, are simultaneously learned from
 training data through a loss based approach. We call this approach 
TNRD-Trainable Nonlinear Reaction Diffusion. The TNRD approach is 
applicable for a variety of image restoration tasks by incorporating 
appropriate reaction force. We demonstrate its capabilities with three 
representative applications, Gaussian image denoising, single image 
super resolution and JPEG deblocking. Experiments show that our trained 
nonlinear diffusion models largely benefit from the training of the 
parameters and finally lead to the best reported performance on common 
test datasets for the tested applications. Our trained models preserve 
the structural simplicity of diffusion models and take only a small 
number of diffusion steps, thus are highly efficient. Moreover, they are
 also well-suited for parallel computation on GPUs, which makes the 
inference procedure extremely fast.},
<br>  keywords={Gaussian processes;graphics processing units;image 
denoising;image resolution;image restoration;learning (artificial 
intelligence);GPU;Gaussian image denoising;JPEG deblocking;TNRD;dynamic 
nonlinear reaction diffusion model;flexible learning framework;image 
restoration problems;low-level computer vision;single-image super 
resolution;time-dependent parameters;trainable nonlinear reaction 
diffusion;Analytical models;Computational modeling;Diffusion 
processes;Image denoising;Image restoration;Mathematical model;JPEG 
deblocking;Nonlinear reaction diffusion;image denoising;image super 
resolution;loss specific training},
<br>  doi={10.1109/TPAMI.2016.2596743},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>@ARTICLE{7470420,
<br>
author={A. Majumdar and R. Singh and M. Vatsa},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Face Verification via Class Sparsity Based Supervised Encoding}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={6},
<br>  pages={1273-1280},
<br>  abstract={Autoencoders are deep learning architectures that learn 
feature representation by minimizing the reconstruction error. Using an 
autoencoder as baseline, this paper presents a novel formulation for a 
class sparsity based supervised encoder, termed as CSSE. We postulate 
that features from the same class will have a common sparsity 
pattern/support in the latent space. Therefore, in the formulation of 
the autoencoder, a supervision penalty is introduced as a jointsparsity 
promoting l2;1-norm. The formulation of CSSE is derived for a single 
hidden layer and it is applied for multiple hidden layers using a greedy
 layer-bylayer learning approach. The proposed CSSE approach is applied 
for learning face representation and verification experiments are 
performed on the LFW and PaSC face databases. The experiments show that 
the proposed approach yields improved results compared to autoencoders 
and comparable results with state-ofthe-art face recognition 
algorithms.},
<br>  keywords={encoding;error analysis;face recognition;image 
reconstruction;learning (artificial intelligence);minimisation;pose 
estimation;CSSE;LFW face databases;PaSC face 
databases;autoencoders;class sparsity based supervised encoding;deep 
learning architectures;face verification;greedy layer-by-layer learning 
approach;reconstruction error minimization;single-hidden layer;Algorithm
 design and analysis;Convolution;Encoding;Face;Face recognition;Machine 
learning;Training;Face verification;autoencoders;deep 
learning;supervised feature learning},
<br>  doi={10.1109/TPAMI.2016.2569436},
<br>  ISSN={0162-8828},
<br>  month={June},}<br>

</body></html>