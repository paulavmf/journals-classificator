@ARTICLE{7453156,
author={G. Trigeorgis and K. Bousmalis and S. Zafeiriou and B. W. Schuller},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={A Deep Matrix Factorization Method for Learning Attribute
Representations},
year={2017},
volume={39},
number={3},
pages={417-429},
abstract={Semi-Non-negative Matrix Factorization is a technique that learns a
low-dimensional representation of a dataset that lends itself to a clustering
interpretation. It is possible that the mapping between this new representation
and our original data matrix contains rather complex hierarchical information
with implicit lower-level hidden attributes, that classical one level
clustering methodologies cannot interpret. In this work we propose a novel
model, Deep Semi-NMF, that is able to learn such hidden representations that
allow themselves to an interpretation of clustering according to different,
unknown attributes of a given dataset. We also present a semi-supervised
version of the algorithm, named Deep WSF, that allows the use of (partial)
prior information for each of the known attributes of a dataset, that allows
the model to be used on datasets with mixed attribute knowledge. Finally, we
show that our models are able to learn low-dimensional representations that are
better suited for clustering, but also classification, outperforming Semi-Non-
negative Matrix Factorization, but also other state-of-the-art methodologies
variants.},
keywords={learning (artificial intelligence);matrix decomposition;pattern
clustering;classical one level clustering methodology;clustering
interpretation;complex hierarchical information;data matrix;deep WSF;deep
matrix factorization method;deep semiNMF model;implicit lower-level hidden
attributes;learning attribute representations;low-dimensional dataset
representation;mixed attribute knowledge;seminonnegative matrix
factorization;semisupervised learning algorithm;Algorithm design and
analysis;Clustering algorithms;Data models;Face;Face recognition;Feature
extraction;Matrix decomposition;Deep WSF;Semi-NMF;WSF;deep semi-NMF;face
classification;face clustering;matrix factorization;semi-supervised
learning;unsupervised feature learning},
doi={10.1109/TPAMI.2016.2554555},
ISSN={0162-8828},
month={March},}
@ARTICLE{7457669,
author={A. Mitra and S. Biswas and C. Bhattacharyya},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Bayesian Modeling of Temporal Coherence in Videos for Entity Discovery
and Summarization},
year={2017},
volume={39},
number={3},
pages={430-443},
abstract={A video is understood by users in terms of entities present in it.
Entity Discovery is the task of building appearance model for each entity
(e.g., a person), and finding all its occurrences in the video. We represent a
video as a sequence of tracklets, each spanning 10-20 frames, and associated
with one entity. We pose Entity Discovery as tracklet clustering, and approach
it by leveraging Temporal Coherence (TC): the property that temporally
neighboring tracklets are likely to be associated with the same entity. Our
major contributions are the first Bayesian nonparametric models for TC at
tracklet-level. We extend Chinese Restaurant Process (CRP) to TC-CRP, and
further to Temporally Coherent Chinese Restaurant Franchise (TC-CRF) to jointly
model entities and temporal segments using mixture components and sparse
distributions. For discovering persons in TV serial videos without meta-data
like scripts, these methods show considerable improvement over state-of-the-art
approaches to tracklet clustering in terms of clustering accuracy, cluster
purity and entity coverage. The proposed methods can perform online tracklet
clustering on streaming videos unlike existing approaches, and can
automatically reject false tracklets. Finally we discuss entity-driven video
summarization- where temporal segments of the video are selected based on the
discovered entities, to create a semantically meaningful summary.},
keywords={Bayes methods;nonparametric statistics;pattern clustering;video
signal processing;video streaming;Bayesian nonparametric models;Chinese
Restaurant Process;TC-CRF;TC-CRP;TV serial videos;appearance model;entity
discovery;entity summarization;entity-driven video summarization;mixture
components;pose entity discovery;sparse distributions;temporal
coherence;temporal segments;temporally coherent Chinese restaurant
franchise;tracklet clustering;tracklet sequence;tracklet-level TC;video
streaming;Bayes methods;Coherence;Computational modeling;Feature
extraction;TV;Videos;YouTube;Bayesian nonparametrics;Chinese restaurant
process;entity discovery;entity-driven video summarization;temporal
coherence;temporal segmentation;tracklet clustering},
doi={10.1109/TPAMI.2016.2557785},
ISSN={0162-8828},
month={March},}
@ARTICLE{7452654,
author={K. Luu and M. Savvides and T. D. Bui and C. Y. Suen},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Compressed Submanifold Multifactor Analysis},
year={2017},
volume={39},
number={3},
pages={444-456},
abstract={Although widely used, Multilinear PCA (MPCA), one of the leading
multilinear analysis methods, still suffers from four major drawbacks. First,
it is very sensitive to outliers and noise. Second, it is unable to cope with
missing values. Third, it is computationally expensive since MPCA deals with
large multi-dimensional datasets. Finally, it is unable to maintain the local
geometrical structures due to the averaging process. This paper proposes a
novel approach named Compressed Submanifold Multifactor Analysis (CSMA) to
solve the four problems mentioned above. Our approach can deal with the problem
of missing values and outliers via SVD-L1. The Random Projection method is used
to obtain the fast low-rank approximation of a given multifactor dataset. In
addition, it is able to preserve the geometry of the original data. Our CSMA
method can be used efficiently for multiple purposes, e.g., noise and outlier
removal, estimation of missing values, biometric applications. We show that
CSMA method can achieve good results and is very efficient in the inpainting
problem. Our method also achieves higher face recognition rates compared to
LRTC, SPMA, MPCA and some other methods, i.e., PCA, LDA and LPP, on three
challenging face databases, i.e., CMU-MPIE, CMU-PIE and Extended YALE-B.},
keywords={face recognition;principal component analysis;CMU-MPIE;CMU-PIE;CSMA
method;LDA;LPP;LRTC;MPCA;SPMA;SVD-L1;compressed submanifold multifactor
analysis;extended YALE-B;face databases;face recognition rates;multidimensional
datasets;multifactor dataset;multilinear PCA;multilinear analysis
methods;random projection method;Approximation algorithms;Face;Face
recognition;Lighting;Multiaccess communication;Principal component
analysis;Tensile stress;ℓ-norm optimization;Tensor analysis;compressed
sensing;multifactor analysis;random projection},
doi={10.1109/TPAMI.2016.2554107},
ISSN={0162-8828},
month={March},}
@ARTICLE{7452658,
author={Y. Xu and E. Carlinet and T. Géraud and L. Najman},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Hierarchical Segmentation Using Tree-Based Shape Spaces},
year={2017},
volume={39},
number={3},
pages={457-469},
abstract={Current trends in image segmentation are to compute a hierarchy of
image segmentations from fine to coarse. A classical approach to obtain a
single meaningful image partition from a given hierarchy is to cut it in an
optimal way, following the seminal approach of the scale-set theory. While
interesting in many cases, the resulting segmentation, being a non-horizontal
cut, is limited by the structure of the hierarchy. In this paper, we propose a
novel approach that acts by transforming an input hierarchy into a new saliency
map. It relies on the notion of shape space: a graph representation of a set of
regions extracted from the image. Each region is characterized with an
attribute describing it. We weigh the boundaries of a subset of meaningful
regions (local minima) in the shape space by extinction values based on the
attribute. This extinction-based saliency map represents a new hierarchy of
segmentations highlighting regions having some specific characteristics. Each
threshold of this map represents a segmentation which is generally different
from any cut of the original hierarchy. This new approach thus enlarges the set
of possible partition results that can be extracted from a given hierarchy.
Qualitative and quantitative illustrations demonstrate the usefulness of the
proposed method.},
keywords={graph theory;image segmentation;set theory;extinction-based saliency
map;graph representation;hierarchical image segmentation;image partition;region
extraction set;scale-set theory;tree-based shape spaces;Image edge
detection;Image segmentation;Indexes;Market research;Partitioning
algorithms;Shape;α-tree;Graph;binary partition tree;hierarchical
segmentation;hierarchy;image segmentation;minimum spanning tree;object
spotting;saliency map;shape space;tree of shapes},
doi={10.1109/TPAMI.2016.2554550},
ISSN={0162-8828},
month={March},}
@ARTICLE{7431988,
author={P. Wang and C. Shen and A. v. d. Hengel and P. H. S. Torr},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Large-Scale Binary Quadratic Optimization Using Semidefinite Relaxation
and Applications},
year={2017},
volume={39},
number={3},
pages={470-485},
abstract={In computer vision, many problems can be formulated as binary
quadratic programs (BQPs), which are in general NP hard. Finding a solution
when the problem is of large size to be of practical interest typically
requires relaxation. Semidefinite relaxation usually yields tight bounds, but
its computational complexity is high. In this work, we present a semidefinite
programming (SDP) formulation for BQPs, with two desirable properties. First,
it produces similar bounds to the standard SDP formulation. Second, compared
with the conventional SDP formulation, the proposed SDP formulation leads to a
considerably more efficient and scalable dual optimization approach. We then
propose two solvers, namely, quasi-Newton and smoothing Newton methods, for the
simplified dual problem. Both of them are significantly more efficient than
standard interior-point methods. Empirically the smoothing Newton solver is
faster than the quasi-Newton solver for dense or medium-sized problems, while
the quasi-Newton solver is preferable for large sparse/structured problems.},
keywords={Newton method;computational complexity;quadratic
programming;relaxation;NP hard;SDP;binary quadratic optimization;binary
quadratic programs;dual optimization;dual problem;quasiNewton
solver;semidefinite programming;semidefinite relaxation;smoothing Newton
methods;Computer vision;Image segmentation;Linear
programming;Optimization;Smoothing methods;Standards;Symmetric matrices;Binary
quadratic optimization;Markov random fields;semidefinite programming},
doi={10.1109/TPAMI.2016.2541146},
ISSN={0162-8828},
month={March},}
@ARTICLE{7450177,
author={Z. Lu and Z. Fu and T. Xiang and P. Han and L. Wang and X. Gao},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Learning from Weak and Noisy Labels for Semantic Segmentation},
year={2017},
volume={39},
number={3},
pages={486-500},
abstract={A weakly supervised semantic segmentation (WSSS) method aims to learn
a segmentation model from weak (image-level) as opposed to strong (pixel-level)
labels. By avoiding the tedious pixel-level annotation process, it can exploit
the unlimited supply of user-tagged images from media-sharing sites such as
Flickr for large scale applications. However, these `free' tags/labels are
often noisy and few existing works address the problem of learning with both
weak and noisy labels. In this work, we cast the WSSS problem into a label
noise reduction problem. Specifically, after segmenting each image into a set
of superpixels, the weak and potentially noisy image-level labels are
propagated to the superpixel level resulting in highly noisy labels; the key to
semantic segmentation is thus to identify and correct the superpixel noisy
labels. To this end, a novel L1-optimisation based sparse learning model is
formulated to directly and explicitly detect noisy labels. To solve the L1-
optimisation problem, we further develop an efficient learning algorithm by
introducing an intermediate labelling variable. Extensive experiments on three
benchmark datasets show that our method yields state-of-the-art results given
noise-free labels, whilst significantly outperforming the existing methods when
the weak labels are also noisy.},
keywords={image denoising;image segmentation;learning (artificial
intelligence);optimisation;set theory;smart pixels;L1-optimisation problem;WSSS
problem;free tag-labels;image-level labels;label noise reduction
problem;learning problem;media-sharing sites;pixel-level annotation
process;sparse learning model;superpixel level;superpixel noisy labels;user-
tagged images;weak labels;weakly supervised semantic segmentation;Computational
modeling;Image segmentation;Labeling;Noise measurement;Noise
reduction;Semantics;Training;Semantic segmentation;label noise reduction;sparse
learning;weakly supervised learning},
doi={10.1109/TPAMI.2016.2552172},
ISSN={0162-8828},
month={March},}
@ARTICLE{7457717,
author={A. Elhayek and E. de Aguiar and A. Jain and J. Thompson and L.
Pishchulin and M. Andriluka and C. Bregler and B. Schiele and C. Theobalt},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={MARCOnI #x2014;ConvNet-Based MARker-Less Motion Capture in Outdoor and
Indoor Scenes},
year={2017},
volume={39},
number={3},
pages={501-514},
abstract={Marker-less motion capture has seen great progress, but most state-
of-the-art approaches fail to reliably track articulated human body motion with
a very low number of cameras, let alone when applied in outdoor scenes with
general background. In this paper, we propose a method for accurate marker-less
capture of articulated skeleton motion of several subjects in general scenes,
indoors and outdoors, even from input filmed with as few as two cameras. The
new algorithm combines the strengths of a discriminative image-based joint
detection method with a model-based generative motion tracking algorithm
through an unified pose optimization energy. The discriminative part-based pose
detection method is implemented using Convolutional Networks (ConvNet) and
estimates unary potentials for each joint of a kinematic skeleton model. These
unary potentials serve as the basis of a probabilistic extraction of pose
constraints for tracking by using weighted sampling from a pose posterior that
is guided by the model. In the final energy, we combine these constraints with
an appearance-based model-to-image similarity term. Poses can be computed very
efficiently using iterative local optimization, since joint detection with a
trained ConvNet is fast, and since our formulation yields a combined pose
estimation energy with analytic derivatives. In combination, this enables to
track full articulated joint angles at state-of-the-art accuracy and temporal
stability with a very low number of cameras. Our method is efficient and lends
itself to implementation on parallel computing hardware, such as GPUs. We test
our method extensively and show its advantages over related work on many indoor
and outdoor data sets captured by ourselves, as well as data sets made
available to the community by other research labs. The availability of good
evaluation data sets is paramount for scientific progress, and many existing
test data sets focus on controlled indoor settings, do not feature much variety
- n the scenes, and often lack a large corpus of data with ground truth
annotation. We therefore further contribute with a new extensive test data set
called MPI-MARCOnI for indoor and outdoor marker-less motion capture that
features 12 scenes of varying complexity and varying camera count, and that
features ground truth reference data from different modalities, ranging from
manual joint annotations to marker-based motion capture results. Our new method
is tested on these data, and the data set will be made available to the
community.},
keywords={convolution;image motion analysis;indoor environment;pose
estimation;ConvNet-based marker-less motion capture;MPI-MARCOnI;appearance-
based model-to-image similarity term;articulated human body motion;articulated
skeleton motion;cameras;convolutional networks;discriminative image-based joint
detection method;discriminative part-based pose detection;ground truth
annotation;ground truth reference data;indoor data sets;indoor scenes;indoor
settings;iterative local optimization;kinematic skeleton model;model-based
generative motion tracking algorithm;outdoor data sets;outdoor marker-less
motion capture;outdoor scenes;parallel computing hardware;pose constraints;pose
estimation energy;probabilistic extraction;temporal stability;unary
potentials;unified pose optimization energy;varying camera
count;Cameras;Computational modeling;Optimization;Skeleton;Three-dimensional
displays;Tracking;Motion capture;convolutional neural networks;marker-less
motion capture;multi-model dataset},
doi={10.1109/TPAMI.2016.2557779},
ISSN={0162-8828},
month={March},}
@ARTICLE{7497466,
author={R. Martín-Clemente and V. Zarzoso},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={On the Link Between L1-PCA and ICA},
year={2017},
volume={39},
number={3},
pages={515-528},
abstract={Principal component analysis (PCA) based on L1-norm maximization is
an emerging technique that has drawn growing interest in the signal processing
and machine learning research communities, especially due to its robustness to
outliers. The present work proves that L1-norm PCA can perform independent
component analysis (ICA) under the whitening assumption. However, when the
source probability distributions fulfil certain conditions, the L1-norm
criterion needs to be minimized rather than maximized, which can be
accomplished by simple modifications on existing optimal algorithms for L1-PCA.
If the sources have symmetric distributions, we show in addition that L1-PCA is
linked to kurtosis optimization. A number of numerical experiments illustrate
the theoretical results and analyze the comparative performance of different
algorithms for ICA via L1-PCA. Although our analysis is asymptotic in the
sample size, this equivalence opens interesting new perspectives for performing
ICA using optimal algorithms for L1-PCA with guaranteed global convergence
while inheriting the increased robustness to outliers of the L1-norm
criterion.},
keywords={independent component analysis;optimisation;principal component
analysis;statistical distributions;ICA;L1-PCA;L1-norm criterion;L1-norm
maximization;guaranteed global convergence;independent component
analysis;kurtosis optimization;machine learning;principal component
analysis;signal processing;source probability distributions;symmetric
distributions;whitening assumption;Algorithm design and analysis;Approximation
algorithms;Convergence;Covariance matrices;Independent component
analysis;Principal component analysis;Robustness;Feature extraction or
construction;L1-norm;feature evaluation and selection;feature
representation;independent component analysis;interactive data exploration and
discovery;multivariate statistics;principal component analysis},
doi={10.1109/TPAMI.2016.2557797},
ISSN={0162-8828},
month={March},}
@ARTICLE{7442563,
author={W. S. Chu and F. D. l. Torre and J. F. Cohn},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Selective Transfer Machine for Personalized Facial Expression Analysis},

year={2017},
volume={39},
number={3},
pages={529-545},
abstract={Automatic facial action unit (AU) and expression detection from
videos is a long-standing problem. The problem is challenging in part because
classifiers must generalize to previously unknown subjects that differ markedly
in behavior and facial morphology (e.g., heavy versus delicate brows, smooth
versus deeply etched wrinkles) from those on which the classifiers are trained.
While some progress has been achieved through improvements in choices of
features and classifiers, the challenge occasioned by individual differences
among people remains. Person-specific classifiers would be a possible solution
but for a paucity of training data. Sufficient training data for person-
specific classifiers typically is unavailable. This paper addresses the problem
of how to personalize a generic classifier without additional labels from the
test subject. We propose a transductive learning method, which we refer to as a
Selective Transfer Machine (STM), to personalize a generic classifier by
attenuating person-specific mismatches. STM achieves this effect by
simultaneously learning a classifier and re-weighting the training samples that
are most relevant to the test subject. We compared STM to both generic
classifiers and cross-domain learning methods on four benchmarks: CK+ [44],
GEMEP-FERA [67], RUFACS [4] and GFT [57]. STM outperformed generic classifiers
in all.},
keywords={face recognition;feature extraction;image classification;learning
(artificial intelligence);support vector machines;CK+ benchmarks;GEMEP-FERA
benchmarks;GFT benchmarks;RU-FACS benchmarks;STM;automatic facial action
unit;behavior morphology;cross-domain learning methods;expression
detection;facial morphology;generic classifiers;person-specific
classifiers;person-specific mismatches;personalized facial expression
analysis;selective transfer machine;sufficient training data;training
data;transductive learning method;Face;Feature extraction;Gold;Hidden Markov
models;Shape;Training;Training data;Facial expression analysis;domain
adaptation;personalization;support vector machine (SVM);transfer learning},
doi={10.1109/TPAMI.2016.2547397},
ISSN={0162-8828},
month={March},}
@ARTICLE{7452621,
author={M. W. Tao and P. P. Srinivasan and S. Hadap and S. Rusinkiewicz and J.
Malik and R. Ramamoorthi},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Shape Estimation from Shading, Defocus, and Correspondence Using Light-
Field Angular Coherence},
year={2017},
volume={39},
number={3},
pages={546-560},
abstract={Light-field cameras are quickly becoming commodity items, with
consumer and industrial applications. They capture many nearby views
simultaneously using a single image with a micro-lens array, thereby providing
a wealth of cues for depth recovery: defocus, correspondence, and shading. In
particular, apart from conventional image shading, one can refocus images after
acquisition, and shift one's viewpoint within the sub-apertures of the main
lens, effectively obtaining multiple views. We present a principled algorithm
for dense depth estimation that combines defocus and correspondence metrics. We
then extend our analysis to the additional cue of shading, using it to refine
fine details in the shape. By exploiting an all-in-focus image, in which pixels
are expected to exhibit angular coherence, we define an optimization framework
that integrates photo consistency, depth consistency, and shading consistency.
We show that combining all three sources of information: defocus,
correspondence, and shading, outperforms state-of-the-art light-field depth
estimation algorithms in multiple scenarios.},
keywords={light coherence;optical focusing;shape recognition;all-in-focus
image;correspondence metrics;depth consistency;depth estimation;image
shading;light-field angular coherence;photo consistency;shading
consistency;shape
estimation;Cameras;Coherence;Estimation;Geometry;Lenses;Lighting;Shape;3D
reconstruction;Light fields;depth cues;reflection components separation;shape
from shading;specular-free image},
doi={10.1109/TPAMI.2016.2554121},
ISSN={0162-8828},
month={March},}
@ARTICLE{7458874,
author={B. Biggio and G. Fumera and G. L. Marcialis and F. Roli},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Statistical Meta-Analysis of Presentation Attacks for Secure
Multibiometric Systems},
year={2017},
volume={39},
number={3},
pages={561-575},
abstract={Prior work has shown that multibiometric systems are vulnerable to
presentation attacks, assuming that their matching score distribution is
identical to that of genuine users, without fabricating any fake trait. We have
recently shown that this assumption is not representative of current
fingerprint and face presentation attacks, leading one to overestimate the
vulnerability of multibiometric systems, and to design less effective fusion
rules. In this paper, we overcome these limitations by proposing a statistical
meta-model of face and fingerprint presentation attacks that characterizes a
wider family of fake score distributions, including distributions of known and,
potentially, unknown attacks. This allows us to perform a thorough security
evaluation of multibiometric systems against presentation attacks, quantifying
how their vulnerability may vary also under attacks that are different from
those considered during design, through an uncertainty analysis. We empirically
show that our approach can reliably predict the performance of multibiometric
systems even under never-before-seen face and fingerprint presentation attacks,
and that the secure fusion rules designed using our approach can exhibit an
improved trade-off between the performance in the absence and in the presence
of attack. We finally argue that our method can be extended to other biometrics
besides faces and fingerprints.},
keywords={biometrics (access control);face recognition;fingerprint
identification;image fusion;image matching;security of data;statistical
analysis;face presentation attacks;fake score distributions;fingerprint
presentation attacks;matching score distribution;multibiometric systems;secure
fusion rules;secure multibiometric systems;security evaluation;statistical
metaanalysis;statistical metamodel;uncertainty analysis;Biometrics (access
control);Fabrication;Face;Facsimile;ISO
Standards;Measurement;Security;Statistical meta-analysis;presentation
attacks;secure multibiometric fusion;security evaluation;uncertainty analysis},

doi={10.1109/TPAMI.2016.2558154},
ISSN={0162-8828},
month={March},}
@ARTICLE{7442536,
author={J. Yang and M. H. Yang},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Top-Down Visual Saliency via Joint CRF and Dictionary Learning},
year={2017},
volume={39},
number={3},
pages={576-588},
abstract={Top-down visual saliency is an important module of visual attention.
In this work, we propose a novel top-down saliency model that jointly learns a
Conditional Random Field (CRF) and a visual dictionary. The proposed model
incorporates a layered structure from top to bottom: CRF, sparse coding and
image patches. With sparse coding as an intermediate layer, CRF is learned in a
feature-adaptive manner; meanwhile with CRF as the output layer, the dictionary
is learned under structured supervision. For efficient and effective joint
learning, we develop a max-margin approach via a stochastic gradient descent
algorithm. Experimental results on the Graz-02 and PASCAL VOC datasets show
that our model performs favorably against state-of-the-art top-down saliency
methods for target object localization. In addition, the dictionary update
significantly improves the performance of our model. We demonstrate the merits
of the proposed top-down saliency model by applying it to prioritizing object
proposals for detection and predicting human fixations.},
keywords={computer vision;gradient methods;stochastic processes;conditional
random field;dictionary learning;image patches;joint CRF;max-margin
approach;sparse coding;stochastic gradient descent algorithm;target object
localization;top down visual saliency;visual attention;visual
dictionary;Computational modeling;Context;Context
modeling;Dictionaries;Prediction algorithms;Predictive
models;Visualization;Visual saliency;dictionary learning and conditional random
fields;fixation prediction;top-down visual saliency},
doi={10.1109/TPAMI.2016.2547384},
ISSN={0162-8828},
month={March},}
@ARTICLE{7448472,
author={B. Wang and G. Wang and K. L. Chan and L. Wang},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Tracklet Association by Online Target-Specific Metric Learning and
Coherent Dynamics Estimation},
year={2017},
volume={39},
number={3},
pages={589-602},
abstract={In this paper, we present a novel method based on online target-
specific metric learning and coherent dynamics estimation for tracklet (track
fragment) association by network flow optimization in long-term multi-person
tracking. Our proposed framework aims to exploit appearance and motion cues to
prevent identity switches during tracking and to recover missed detections.
Furthermore, target-specific metrics (appearance cue) and motion dynamics
(motion cue) are proposed to be learned and estimated online, i.e., during the
tracking process. Our approach is effective even when such cues fail to
identify or follow the target due to occlusions or object-to-object
interactions. We also propose to learn the weights of these two tracking cues
to handle the difficult situations, such as severe occlusions and object-to-
object interactions effectively. Our method has been validated on several
public datasets and the experimental results show that it outperforms several
state-of-the-art tracking methods.},
keywords={learning (artificial intelligence);motion estimation;target
tracking;coherent dynamics estimation;long-term multiperson tracking;motion
cues;motion dynamics;network flow optimization;object-to-object
interactions;online target-specific metric learning;public datasets;target-
specific metrics;track fragment;tracking cues;tracking process;tracklet
association;Dynamics;Optimization;Reliability;Target tracking;Trajectory;Multi-
object tracking;motion dynamics;network flow optimization;target-specific
metric learning;tracklet association},
doi={10.1109/TPAMI.2016.2551245},
ISSN={0162-8828},
month={March},}
@ARTICLE{7448905,
author={M. Alterman and Y. Y. Schechner and Y. Swirski},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Triangulation in Random Refractive Distortions},
year={2017},
volume={39},
number={3},
pages={603-616},
abstract={Random refraction occurs in turbulence and through a wavy water-air
interface. It creates distortion that changes in space, time and with
viewpoint. Localizing objects in three dimensions (3D) despite this random
distortion is important to some predators and also to submariners avoiding the
salient use of periscopes. We take a multiview approach to this task. Refracted
distortion statistics induce a probabilistic relation between any pixel
location and a line of sight in space. Measurements of an object's random
projection from multiple views and times lead to a likelihood function of the
object's 3D location. The likelihood leads to estimates of the 3D location and
its uncertainty. Furthermore, multiview images acquired simultaneously in a
wide stereo baseline have uncorrelated distortions. This helps reduce the
acquisition time needed for localization. The method is demonstrated in
stereoscopic video sequences, both in a lab and a swimming pool.},
keywords={distortion;estimation theory;image sequences;object
recognition;statistics;stereo image processing;turbulence;video signal
processing;3D location estimation;object 3D location;object localization;object
random projection;random refractive distortions;refracted distortion
statistics;stereo baseline;stereoscopic video
sequences;triangulation;turbulence;water air
interface;Cameras;Distortion;Distortion measurement;Maximum likelihood
estimation;Optical imaging;Optical refraction;Three-dimensional
displays;Underwater;likelihood;probability;stereo;triangulation},
doi={10.1109/TPAMI.2016.2551740},
ISSN={0162-8828},
month={March},}
@ARTICLE{7469410,
author={I. Takigawa and H. Mamitsuka},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Generalized Sparse Learning of Linear Models Over the Complete Subgraph
Feature Set},
year={2017},
volume={39},
number={3},
pages={617-624},
abstract={Supervised learning over graphs is an intrinsically difficult
problem: simultaneous learning of relevant features from the complete subgraph
feature set, in which enumerating all subgraph features occurring in given
graphs is practically intractable due to combinatorial explosion. We show that
1) existing graph supervised learning studies, such as Adaboost, LPBoost, and
LARS/LASSO, can be viewed as variations of a branch-and-bound algorithm with
simple bounds, which we call Morishita-Kudo bounds; 2) We present a direct
sparse optimization algorithm for generalized problems with arbitrary twice-
differentiable loss functions, to which Morishita-Kudo bounds cannot be
directly applied; 3) We experimentally showed that i) our direct optimization
method improves the convergence rate and stability, and ii) L1-penalized
logistic regression (L1LogReg) by our method identifies a smaller subgraph set,
keeping the competitive performance, iii) the learned subgraphs by L1-LogReg
are more size-balanced than competing methods, which are biased to small-sized
subgraphs.},
keywords={convergence;data mining;graph theory;learning (artificial
intelligence);set theory;stability;tree searching;Adaboost;L1-LogReg;L1-
penalized logistic regression;LARS/LASSO;LPBoost;Morishita-Kudo bounds;branch-
and-bound algorithm;convergence rate;direct sparse optimization
algorithm;generalized sparse learning;graph supervised learning;linear
models;stability;subgraph feature set;subgraph
mining;Convergence;Explosions;Kernel;Logistics;Optimization;Stability
criteria;Supervised learning for graphs;block coordinate gradient descent;graph
mining;simultaneous feature learning;sparsity-inducing regularization},
doi={10.1109/TPAMI.2016.2567399},
ISSN={0162-8828},
month={March},}
