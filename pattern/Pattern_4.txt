@ARTICLE{7870813,
author={K. Grauman and E. Learned-Miller and A. Torralba and A. Zisserman},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Guest Editorial: Best of CVPR 2015},
year={2017},
volume={39},
number={4},
pages={625-626},
abstract={The papers in this special section were presented at the CVPR 2015
conference that was held in Boston, MA.},
keywords={Meetings;Pattern recognition;Special issues and sections},
doi={10.1109/TPAMI.2017.2663859},
ISSN={0162-8828},
month={April},}
@ARTICLE{7486965,
author={B. Hariharan and P. Arbeláez and R. Girshick and J. Malik},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Object Instance Segmentation and Fine-Grained Localization Using
Hypercolumns},
year={2017},
volume={39},
number={4},
pages={627-639},
abstract={Recognition algorithms based on convolutional networks (CNNs)
typically use the output of the last layer as a feature representation.
However, the information in this layer may be too coarse spatially to allow
precise localization. On the contrary, earlier layers may be precise in
localization but will not capture semantics. To get the best of both worlds, we
define the hypercolumn at a pixel as the vector of activations of all CNN units
above that pixel. Using hypercolumns as pixel descriptors, we show results on
three fine-grained localization tasks: simultaneous detection and segmentation,
where we improve state-of-the-art from 49.7 mean APr to 62.4, keypoint
localization, where we get a 3.3 point boost over a strong regression baseline
using CNN features, and part labeling, where we show a 6.6 point gain over a
strong baseline.},
keywords={feature extraction;image segmentation;neural nets;object
recognition;regression analysis;CNN units activation vector;convolutional
network;feature representation;fine-grained localization;hypercolumns;keypoint
localization;object instance segmentation;part labeling;pixel
descriptors;recognition algorithm;regression baseline;simultaneous detection
and segmentation;Image segmentation;Labeling;Nonlinear optics;Object
detection;Optical imaging;Proposals;Semantics;Segmentation;convolutional
networks;detection;part labeling},
doi={10.1109/TPAMI.2016.2578328},
ISSN={0162-8828},
month={April},}
@ARTICLE{7478072,
author={E. Shelhamer and J. Long and T. Darrell},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Fully Convolutional Networks for Semantic Segmentation},
year={2017},
volume={39},
number={4},
pages={640-651},
abstract={Convolutional networks are powerful visual models that yield
hierarchies of features. We show that convolutional networks by themselves,
trained end-to-end, pixels-to-pixels, improve on the previous best result in
semantic segmentation. Our key insight is to build “fully convolutional”
networks that take input of arbitrary size and produce correspondingly-sized
output with efficient inference and learning. We define and detail the space of
fully convolutional networks, explain their application to spatially dense
prediction tasks, and draw connections to prior models. We adapt contemporary
classification networks (AlexNet, the VGG net, and GoogLeNet) into fully
convolutional networks and transfer their learned representations by fine-
tuning to the segmentation task. We then define a skip architecture that
combines semantic information from a deep, coarse layer with appearance
information from a shallow, fine layer to produce accurate and detailed
segmentations. Our fully convolutional networks achieve improved segmentation
of PASCAL VOC (30% relative improvement to 67.2% mean IU on 2012), NYUDv2, SIFT
Flow, and PASCAL-Context, while inference takes one tenth of a second for a
typical image.},
keywords={feedforward neural nets;image classification;image
representation;image resolution;image segmentation;learning (artificial
intelligence);transforms;NYUDv2;PASCAL VOC;PASCAL-Context;SIFT Flow;coarse
layer;contemporary classification networks;correspondingly-sized output;fine
layer;fully convolutional networks;learned representations;semantic
segmentation;spatially dense prediction tasks;visual models;Computer
architecture;Convolution;Fuses;Image
segmentation;Proposals;Semantics;Training;Convolutional Networks;Deep
Learning;Semantic Segmentation;Transfer Learning},
doi={10.1109/TPAMI.2016.2572683},
ISSN={0162-8828},
month={April},}
@ARTICLE{7505636,
author={O. Vinyals and A. Toshev and S. Bengio and D. Erhan},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Show and Tell: Lessons Learned from the 2015 MSCOCO Image Captioning
Challenge},
year={2017},
volume={39},
number={4},
pages={652-663},
abstract={Automatically describing the content of an image is a fundamental
problem in artificial intelligence that connects computer vision and natural
language processing. In this paper, we present a generative model based on a
deep recurrent architecture that combines recent advances in computer vision
and machine translation and that can be used to generate natural sentences
describing an image. The model is trained to maximize the likelihood of the
target description sentence given the training image. Experiments on several
datasets show the accuracy of the model and the fluency of the language it
learns solely from image descriptions. Our model is often quite accurate, which
we verify both qualitatively and quantitatively. Finally, given the recent
surge of interest in this task, a competition was organized in 2015 using the
newly released COCO dataset. We describe and analyze the various improvements
we applied to our own baseline and show the resulting performance in the
competition, which we won ex-aequo with a team from Microsoft Research.},
keywords={artificial intelligence;computer vision;language translation;natural
language processing;optimisation;recurrent neural nets;artificial
intelligence;computer vision;deep recurrent architecture;image
captioning;likelihood maximization;machine translation;natural language
processing;natural sentence generation;target description
sentence;Computational modeling;Computer vision;Logic gates;Natural
languages;Recurrent neural networks;Training;Visualization;Image
captioning;language model;recurrent neural network;sequence-to-sequence},
doi={10.1109/TPAMI.2016.2587640},
ISSN={0162-8828},
month={April},}
@ARTICLE{7534740,
author={A. Karpathy and L. Fei-Fei},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Deep Visual-Semantic Alignments for Generating Image Descriptions},
year={2017},
volume={39},
number={4},
pages={664-676},
abstract={We present a model that generates natural language descriptions of
images and their regions. Our approach leverages datasets of images and their
sentence descriptions to learn about the inter-modal correspondences between
language and visual data. Our alignment model is based on a novel combination
of Convolutional Neural Networks over image regions, bidirectional Recurrent
Neural Networks (RNN) over sentences, and a structured objective that aligns
the two modalities through a multimodal embedding. We then describe a
Multimodal Recurrent Neural Network architecture that uses the inferred
alignments to learn to generate novel descriptions of image regions. We
demonstrate that our alignment model produces state of the art results in
retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show
that the generated descriptions outperform retrieval baselines on both full
images and on a new dataset of region-level annotations. Finally, we conduct
large-scale analysis of our RNN language model on the Visual Genome dataset of
4.1 million captions and highlight the differences between image and region-
level caption statistics.},
keywords={feedforward neural nets;image processing;inference mechanisms;natural
language processing;recurrent neural nets;statistical analysis;Flickr30K
dataset;Flickr8K dataset;MSCOCO dataset;RNN language model;Visual Genome
dataset;bidirectional RNN;bidirectional recurrent neural networks;convolutional
neural networks;deep visual-semantic alignments;image regions;inferred
alignments;inter modal correspondences;language data;multimodal
embedding;multimodal recurrent neural network architecture;natural language
image description generation;region-level annotations;region-level caption
statistics;visual data;Analytical models;Context;Image segmentation;Natural
languages;Recurrent neural networks;Visualization;Image captioning;deep neural
networks;language model;recurrent neural network;visual-semantic embeddings},
doi={10.1109/TPAMI.2016.2598339},
ISSN={0162-8828},
month={April},}
@ARTICLE{7558228,
author={J. Donahue and L. A. Hendricks and M. Rohrbach and S. Venugopalan and
S. Guadarrama and K. Saenko and T. Darrell},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Long-Term Recurrent Convolutional Networks for Visual Recognition and
Description},
year={2017},
volume={39},
number={4},
pages={677-691},
abstract={Models based on deep convolutional networks have dominated recent
image interpretation tasks; we investigate whether models which are also
recurrent are effective for tasks involving sequences, visual and otherwise. We
describe a class of recurrent convolutional architectures which is end-to-end
trainable and suitable for large-scale visual understanding tasks, and
demonstrate the value of these models for activity recognition, image
captioning, and video description. In contrast to previous models which assume
a fixed visual representation or perform simple temporal averaging for
sequential processing, recurrent convolutional models are “doubly deep” in that
they learn compositional representations in space and time. Learning long-term
dependencies is possible when nonlinearities are incorporated into the network
state updates. Differentiable recurrent models are appealing in that they can
directly map variable-length inputs (e.g., videos) to variable-length outputs
(e.g., natural language text) and can model complex temporal dynamics; yet they
can be optimized with backpropagation. Our recurrent sequence models are
directly connected to modern visual convolutional network models and can be
jointly trained to learn temporal dynamics and convolutional perceptual
representations. Our results show that such models have distinct advantages
over state-of-the-art models for recognition or generation which are separately
defined or optimized.},
keywords={backpropagation;computer vision;image sequences;neural net
architecture;object recognition;recurrent neural nets;activity
recognition;backpropagation;complex temporal dynamics;compositional
representation learning;convolutional perceptual representations;differentiable
recurrent models;image captioning;large-scale visual understanding tasks;long-
term dependency Learning;long-term recurrent convolutional networks;network
state updates;recurrent convolutional architectures;recurrent sequence
models;temporal dynamic learning;variable-length input mapping;variable-length
output mapping;video description;visual convolutional network models;visual
description;visual recognition;Computational modeling;Computer
architecture;Data models;Logic gates;Predictive models;Recurrent neural
networks;Visualization;Computer vision;convolutional nets;deep
learning;transfer learning},
doi={10.1109/TPAMI.2016.2599174},
ISSN={0162-8828},
month={April},}
@ARTICLE{7469347,
author={A. Dosovitskiy and J. T. Springenberg and M. Tatarchenko and T. Brox},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Learning to Generate Chairs, Tables and Cars with Convolutional
Networks},
year={2017},
volume={39},
number={4},
pages={692-705},
abstract={We train generative `up-convolutional' neural networks which are able
to generate images of objects given object style, viewpoint, and color. We
train the networks on rendered 3D models of chairs, tables, and cars. Our
experiments show that the networks do not merely learn all images by heart, but
rather find a meaningful representation of 3D models allowing them to assess
the similarity of different models, interpolate between given views to generate
the missing ones, extrapolate views, and invent new objects not present in the
training set by recombining training instances, or even two different object
classes. Moreover, we show that such generative networks can be used to find
correspondences between different objects from the dataset, outperforming
existing approaches on this task.},
keywords={convolution;image representation;learning (artificial
intelligence);rendering (computer graphics);solid modelling;3D model
rendering;3D model representation;car model;chair model;object color;object
images;object style;object viewpoint;table model;up-convolutional neural
network training;Automobiles;Image color analysis;Image segmentation;Neural
networks;Solid modeling;Three-dimensional displays;Training;Convolutional
networks;generative models;image generation;up-convolutional networks},
doi={10.1109/TPAMI.2016.2567384},
ISSN={0162-8828},
month={April},}
@ARTICLE{7740877,
author={A. Punjani and M. A. Brubaker and D. J. Fleet},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Building Proteins in a Day: Efficient 3D Molecular Structure Estimation
with Electron Cryomicroscopy},
year={2017},
volume={39},
number={4},
pages={706-718},
abstract={Discovering the 3D atomic-resolution structure of molecules such as
proteins and viruses is one of the foremost research problems in biology and
medicine. Electron Cryomicroscopy (cryo-EM) is a promising vision-based
technique for structure estimation which attempts to reconstruct 3D atomic
structures from a large set of 2D transmission electron microscope images. This
paper presents a new Bayesian framework for cryo-EM structure estimation that
builds on modern stochastic optimization techniques to allow one to scale to
very large datasets. We also introduce a novel Monte-Carlo technique that
reduces the cost of evaluating the objective function during optimization by
over five orders of magnitude. The net result is an approach capable of
estimating 3D molecular structure from large-scale datasets in about a day on a
single CPU workstation.},
keywords={Bayes methods;Monte Carlo methods;biology computing;molecular
biophysics;proteins;stereo image processing;stochastic programming;transmission
electron microscopy;2D transmission electron microscope image;3D atomic
structure reconstruction;3D atomic-resolution structure discovery;3D molecular
structure estimation;Bayesian framework;Monte-Carlo technique;biology;cryo-EM
structure estimation;electron cryomicroscopy;medicine;proteins;stochastic
optimization technique;viruses;vision-based technique;Computational
modeling;Estimation;Image reconstruction;Optimization;Proteins;Three-
dimensional displays;Two dimensional displays;3D reconstruction;electron
cryomicroscopy;importance sampling;molecular structure;single
particle;stochastic optimization},
doi={10.1109/TPAMI.2016.2627573},
ISSN={0162-8828},
month={April},}
@ARTICLE{7482798,
author={S. Tulsiani and A. Kar and J. Carreira and J. Malik},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Learning Category-Specific Deformable 3D Models for Object
Reconstruction},
year={2017},
volume={39},
number={4},
pages={719-731},
abstract={We address the problem of fully automatic object localization and
reconstruction from a single image. This is both a very challenging and very
important problem which has, until recently, received limited attention due to
difficulties in segmenting objects and predicting their poses. Here we leverage
recent advances in learning convolutional networks for object detection and
segmentation and introduce a complementary network for the task of camera
viewpoint prediction. These predictors are very powerful, but still not perfect
given the stringent requirements of shape reconstruction. Our main contribution
is a new class of deformable 3D models that can be robustly fitted to images
based on noisy pose and silhouette estimates computed upstream and that can be
learned directly from 2D annotations available in object detection datasets.
Our models capture top-down information about the main global modes of shape
variation within a class providing a “low-frequency” shape. In order to capture
fine instance-specific shape details, we fuse it with a high-frequency
component recovered from shading cues. A comprehensive quantitative analysis
and ablation study on the PASCAL 3D+ dataset validates the approach as we show
fully automatic reconstructions on PASCAL VOC as well as large improvements on
the task of viewpoint prediction.},
keywords={convolution;estimation theory;image denoising;image
reconstruction;image segmentation;learning (artificial intelligence);neural
nets;object detection;object recognition;solid modelling;stereo image
processing;camera viewpoint prediction;convolutional network
learning;deformable 3D models;image noisy pose;object detection;object
localization;object reconstruction;object segmentation;shape
reconstruction;viewpoint estimation;Cameras;Deformable models;Image
reconstruction;Shape;Solid modeling;Three-dimensional displays;Training;3D
shape modeling;Object reconstruction;scene understanding;viewpoint estimation},

doi={10.1109/TPAMI.2016.2574713},
ISSN={0162-8828},
month={April},}
@ARTICLE{7728146,
author={A. Davis* and K. L. Bouman* and J. G. Chen and M. Rubinstein and O.
Büyüköztürk and F. Durand and W. T. Freeman},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Visual Vibrometry: Estimating Material Properties from Small Motions in
Video},
year={2017},
volume={39},
number={4},
pages={732-745},
abstract={The estimation of material properties is important for scene
understanding, with many applications in vision, robotics, and structural
engineering. This paper connects fundamentals of vibration mechanics with
computer vision techniques in order to infer material properties from small,
often imperceptible motions in video. Objects tend to vibrate in a set of
preferred modes. The frequencies of these modes depend on the structure and
material properties of an object. We show that by extracting these frequencies
from video of a vibrating object, we can often make inferences about that
object's material properties. We demonstrate our approach by estimating
material properties for a variety of objects by observing their motion in high-
speed and regular frame rate video.},
keywords={computer vision;image motion analysis;materials science
computing;video signal processing;computer vision;object material property
estimation;regular frame rate video;structural engineering;structure
properties;vibrating object video;vibration mechanics;visual
vibrometry;Damping;Estimation;Fabrics;Geometry;Material properties;Measurement
by laser beam;Vibrations;Material properties;computational
imaging;computational photography;small motion;vibration},
doi={10.1109/TPAMI.2016.2622271},
ISSN={0162-8828},
month={April},}
@ARTICLE{7755748,
author={K. Tanaka and Y. Mukaigawa and H. Kubo and Y. Matsushita and Y. Yagi},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Recovering Inner Slices of Layered Translucent Objects by Multi-
Frequency Illumination},
year={2017},
volume={39},
number={4},
pages={746-757},
abstract={This paper describes a method for recovering appearance of inner
slices of translucent objects. The appearance of a layered translucent object
is the summed appearance of all layers, where each layer is blurred by a depth-
dependent point spread function (PSF). By exploiting the difference of low-pass
characteristics of depth-dependent PSFs, we develop a multi-frequency
illumination method for obtaining the appearance of individual inner slices.
Specifically, by observing the target object with varying the spatial frequency
of checker-pattern illumination, our method recovers the appearance of inner
slices via computation. We study the effect of non-uniform transmission due to
inhomogeneity of translucent objects and develop a method for recovering clear
inner slices based on the pixel-wise PSF estimates under the assumption of
spatial smoothness of inner slice appearances. We quantitatively evaluate the
accuracy of the proposed method by simulations and qualitatively show faithful
recovery using real-world scenes.},
keywords={image restoration;image texture;lighting;PSF;checker-pattern
illumination;image restoration;inner slice recovering;layered translucent
objects;multifrequency illumination;point spread function;target
object;Cameras;Computational modeling;Lighting;Nonhomogeneous
media;Scattering;Descattering;image restoration;layer separation;projector-
camera system},
doi={10.1109/TPAMI.2016.2631625},
ISSN={0162-8828},
month={April},}
@ARTICLE{7755788,
author={T. J. Chin and P. Purkait and A. Eriksson and D. Suter},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Efficient Globally Optimal Consensus Maximisation with Tree Search},
year={2017},
volume={39},
number={4},
pages={758-772},
abstract={Maximum consensus is one of the most popular criteria for robust
estimation in computer vision. Despite its widespread use, optimising the
criterion is still customarily done by randomised sample-and-test techniques,
which do not guarantee optimality of the result. Several globally optimal
algorithms exist, but they are too slow to challenge the dominance of
randomised methods. Our work aims to change this state of affairs by proposing
an efficient algorithm for global maximisation of consensus. Under the
framework of LP-type methods, we show how consensus maximisation for a wide
variety of vision tasks can be posed as a tree search problem. This insight
leads to a novel algorithm based on A* search. We propose efficient heuristic
and support set updating routines that enable A* search to efficiently find
globally optimal results. On common estimation problems, our algorithm is much
faster than previous exact methods. Our work identifies a promising direction
for globally optimal consensus maximisation.},
keywords={computer vision;optimisation;tree searching;A* search;LP-type
methods;computer vision;global consensus maximisation;globally optimal
algorithms;globally optimal consensus maximisation;randomised
methods;randomised sample-and-test techniques;robust estimation;set updating
routines;tree search problem;vision tasks;Computer
vision;Context;Estimation;Optimization;Robustness;Runtime;Search
problems;Robust regression;global optimisation;graph and tree search
strategies},
doi={10.1109/TPAMI.2016.2631531},
ISSN={0162-8828},
month={April},}
@ARTICLE{7458903,
author={B. Fernando and E. Gavves and J. Oramas M. and A. Ghodrati and T.
Tuytelaars},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Rank Pooling for Action Recognition},
year={2017},
volume={39},
number={4},
pages={773-787},
abstract={We propose a function-based temporal pooling method that captures the
latent structure of the video sequence data - e.g., how frame-level features
evolve over time in a video. We show how the parameters of a function that has
been fit to the video data can serve as a robust new video representation. As a
specific example, we learn a pooling function via ranking machines. By learning
to rank the frame-level features of a video in chronological order, we obtain a
new representation that captures the video-wide temporal dynamics of a video,
suitable for action recognition. Other than ranking functions, we explore
different parametric models that could also explain the temporal changes in
videos. The proposed functional pooling methods, and rank pooling in
particular, is easy to interpret and implement, fast to compute and effective
in recognizing a wide variety of actions. We evaluate our method on various
benchmarks for generic action, fine-grained action and gesture recognition.
Results show that rank pooling brings an absolute improvement of 7-10 average
pooling baseline. At the same time, rank pooling is compatible with and
complementary to several appearance and local motion based methods and
features, such as improved trajectories and deep learning features.},
keywords={gesture recognition;image representation;image sequences;video signal
processing;action recognition;frame-level features;function-based temporal
pooling method;gesture recognition;local motion based methods;rank pooling
function;ranking machines;video representation;video sequence data latent
structure;video-wide temporal dynamics;Data models;Dynamics;Feature
extraction;Hidden Markov models;Recurrent neural
networks;Training;Visualization;Action recognition;rank pooling;temporal
encoding;temporal pooling;video dynamics},
doi={10.1109/TPAMI.2016.2558148},
ISSN={0162-8828},
month={April},}
@ARTICLE{7458912,
author={V. Pătrăucean and P. Gurdjos and R. Grompone von Gioi},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Joint A Contrario Ellipse and Line Detection},
year={2017},
volume={39},
number={4},
pages={788-802},
abstract={We propose a line segment and elliptical arc detector that produces a
reduced number of false detections on various types of images without any
parameter tuning. For a given region of pixels in a grey-scale image, the
detector decides whether a line segment or an elliptical arc is present (model
validation). If both interpretations are possible for the same region, the
detector chooses the one that best explains the data (model selection). We
describe a statistical criterion based on the a contrariotheory, which serves
for both validation and model selection. The experimental results highlight the
performance of the proposed approach compared to state-of-the-art detectors,
when applied on synthetic and real images.},
keywords={image segmentation;object detection;statistical analysis;contrario
ellipse;elliptical arc detector;grey-scale image;line detection;line
segment;model validation;real images;statistical criterion;synthetic
images;Adaptation models;Data models;Detectors;Image edge detection;Image
segmentation;Noise measurement;Tuning;a contrario theory;Ellipse detection;line
segment detection;model selection},
doi={10.1109/TPAMI.2016.2558150},
ISSN={0162-8828},
month={April},}
@ARTICLE{7463054,
author={P. Li and Q. Wang and H. Zeng and L. Zhang},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Local Log-Euclidean Multivariate Gaussian Descriptor and Its Application
to Image Classification},
year={2017},
volume={39},
number={4},
pages={803-817},
abstract={This paper presents a novel image descriptor to effectively
characterize the local, high-order image statistics. Our work is inspired by
the Diffusion Tensor Imaging and the structure tensor method (or covariance
descriptor), and motivated by popular distribution-based descriptors such as
SIFT and HoG. Our idea is to associate one pixel with a multivariate Gaussian
distribution estimated in the neighborhood. The challenge lies in that the
space of Gaussians is not a linear space but a Riemannian manifold. We show,
for the first time to our knowledge, that the space of Gaussians can be
equipped with a Lie group structure by defining a multiplication operation on
this manifold, and that it is isomorphic to a subgroup of the upper triangular
matrix group. Furthermore, we propose methods to embed this matrix group in the
linear space, which enables us to handle Gaussians with Euclidean operations
rather than complicated Riemannian operations. The resulting descriptor, called
Local Log-Euclidean Multivariate Gaussian (L2EMG) descriptor, works well with
low-dimensional and high-dimensional raw features. Moreover, our descriptor is
a continuous function of features without quantization, which can model the
first- and second-order statistics. Extensive experiments were conducted to
evaluate thoroughly L2EMG, and the results showed that L2EMG is very
competitive with state-of-the-art descriptors in image classification.},
keywords={Gaussian processes;Lie groups;biodiffusion;higher order
statistics;image classification;tensors;L2EMG descriptor;Lie group;Riemannian
manifold;covariance descriptor;diffusion tensor imaging;high-order image
statistics;image classification;image descriptor;local log Euclidean
multivariate Gaussian descriptor;structure tensor;Covariance matrices;Diffusion
tensor imaging;Feature extraction;Histograms;Manifolds;Measurement;Symmetric
matrices;Image descriptors;Lie group;image classification;space of Gaussians},
doi={10.1109/TPAMI.2016.2560816},
ISSN={0162-8828},
month={April},}
@ARTICLE{7464858,
author={H. Peng and B. Li and H. Ling and W. Hu and W. Xiong and S. J.
Maybank},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Salient Object Detection via Structured Matrix Decomposition},
year={2017},
volume={39},
number={4},
pages={818-832},
abstract={Low-rank recovery models have shown potential for salient object
detection, where a matrix is decomposed into a low-rank matrix representing
image background and a sparse matrix identifying salient objects. Two
deficiencies, however, still exist. First, previous work typically assumes the
elements in the sparse matrix are mutually independent, ignoring the spatial
and pattern relations of image regions. Second, when the low-rank and sparse
matrices are relatively coherent, e.g., when there are similarities between the
salient objects and background or when the background is complicated, it is
difficult for previous models to disentangle them. To address these problems,
we propose a novel structured matrix decomposition model with two structural
regularizations: (1) a tree-structured sparsity-inducing regularization that
captures the image structure and enforces patches from the same object to have
similar saliency values, and (2) a Laplacian regularization that enlarges the
gaps between salient objects and the background in feature space. Furthermore,
high-level priors are integrated to guide the matrix decomposition and boost
the detection. We evaluate our model for salient object detection on five
challenging datasets including single object, multiple objects and complex
scene images, and show competitive results as compared with 24 state-of-the-art
methods in terms of seven performance metrics.},
keywords={feature extraction;image representation;matrix decomposition;object
detection;sparse matrices;trees (mathematics);Laplacian regularization;complex
scene image;feature space;image background representation;image structure;low-
rank matrix;low-rank recovery model;saliency value;salient object
detection;salient object identification;salient object similarity;sparse
matrix;structural regularization;structured matrix decomposition model;tree-
structured sparsity-inducing regularization;Computational modeling;Image color
analysis;Image segmentation;Laplace equations;Matrix decomposition;Object
detection;Sparse matrices;Salient object detection;low rank;matrix
decomposition;structured sparsity;subspace learning},
doi={10.1109/TPAMI.2016.2562626},
ISSN={0162-8828},
month={April},}
