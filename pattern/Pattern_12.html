<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"></head><body>@ARTICLE{7792748,
<br>
author={K. Fu and J. Jin and R. Cui and F. Sha and C. Zhang},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Aligning Where to See and What to Tell: Image Captioning with Region-Based Attention and Scene-Specific Contexts}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2321-2334},
<br>  abstract={Recent progress on automatic generation of image 
captions has shown that it is possible to describe the most salient 
information conveyed by images with accurate and meaningful sentences. 
In this paper, we propose an image captioning system that exploits the 
parallel structures between images and sentences. In our model, the 
process of generating the next word, given the previously generated 
ones, is aligned with the visual perception experience where the 
attention shifts among the visual regions-such transitions impose a 
thread of ordering in visual perception. This alignment characterizes 
the flow of latent meaning, which encodes what is semantically shared by
 both the visual scene and the text description. Our system also makes 
another novel modeling contribution by introducing scene-specific 
contexts that capture higher-level semantic information encoded in an 
image. The contexts adapt language models for word generation to 
specific scene types. We benchmark our system and contrast to published 
results on several popular datasets, using both automatic evaluation 
metrics and human evaluation. We show that either region-based attention
 or scene-specific contexts improves systems without those components. 
Furthermore, combining these two modeling ingredients attains the 
state-of-the-art performance.},
<br>  keywords={image capture;text analysis;visual perception;automatic 
generation;capture higher-level semantic information;image captioning 
system;language models;salient information;scene-specific contexts;text 
description;visual perception experience;visual regions;visual 
scene;word generation;Adaptation models;Computational modeling;Context 
modeling;Data mining;Feature extraction;Image 
classification;Visualization;Image captioning;LSTM;scene-specific 
context;visual attention},
<br>  doi={10.1109/TPAMI.2016.2642953},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7812753,
<br>
author={L. Liu and P. Wang and C. Shen and L. Wang and A. v. d. Hengel and C. Wang and H. T. Shen},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Compositional Model Based Fisher Vector Coding for Image Classification}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2335-2348},
<br>  abstract={Deriving from the gradient vector of a generative model 
of local features, Fisher vector coding (FVC) has been identified as an 
effective coding method for image classification. Most, if not all, FVC 
implementations employ the Gaussian mixture model (GMM) as the 
generative model for local features. However, the representative power 
of a GMM can be limited because it essentially assumes that local 
features can be characterized by a fixed number of feature prototypes, 
and the number of prototypes is usually small in FVC. To alleviate this 
limitation, in this work, we break the convention which assumes that a 
local feature is drawn from one of a few Gaussian distributions. 
Instead, we adopt a compositional mechanism which assumes that a local 
feature is drawn from a Gaussian distribution whose mean vector is 
composed as a linear combination of multiple key components, and the 
combination weight is a latent random variable. In doing so we greatly 
enhance the representative power of the generative model underlying FVC.
 To implement our idea, we design two particular generative models 
following this compositional approach. In our first model, the mean 
vector is sampled from the subspace spanned by a set of bases and the 
combination weight is drawn from a Laplace distribution. In our second 
model, we further assume that a local feature is composed of a 
discriminative part and a residual part. As a result, a local feature is
 generated by the linear combination of discriminative part bases and 
residual part bases. The decomposition of the discriminative and 
residual parts is achieved via the guidance of a pre-trained supervised 
coding method. By calculating the gradient vector of the proposed 
models, we derive two new Fisher vector coding strategies. The first is 
termed Sparse Coding-based Fisher Vector Coding (SCFVC) and can be used 
as the substitute of traditional GMM based FVC. The second is termed 
Hybrid Sparse Coding-based Fisher vector coding (HSCFVC) since it 
combine- the merits of both pre-trained supervised coding methods and 
FVC. Using pre-trained Convolutional Neural Network (CNN) activations as
 local features, we experimentally demonstrate that the proposed methods
 are superior to traditional GMM based FVC and achieve state-of-the-art 
performance in various image classification tasks.},
<br>  keywords={Gaussian distribution;feature extraction;feedforward 
neural nets;image classification;image coding;image 
representation;learning (artificial intelligence);vectors;GMM;Gaussian 
distributions;Gaussian mixture model;HSCFVC;Hybrid Sparse Coding-based 
Fisher vector coding;Laplace distribution;compositional model based 
fisher vector coding;generative model;gradient vector;image 
classification;local feature;mean vector;particular generative 
models;pretrained Convolutional Neural Network activations;supervised 
coding method;Convolutional codes;Encoding;Feature extraction;Gaussian 
distribution;Image classification;Image coding;Image 
representation;Vectors;Fisher vector coding;convolutional 
networks;generic image classification;hybrid sparse coding;sparse 
coding},
<br>  doi={10.1109/TPAMI.2017.2651061},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7807268,
<br>
author={D. Lefloch and M. Kluge and H. Sarbolandi and T. Weyrich and A. Kolb},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Comprehensive Use of Curvature for Robust and Accurate Online Surface Reconstruction}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2349-2365},
<br>  abstract={Interactive real-time scene acquisition from hand-held 
depth cameras has recently developed much momentum, enabling 
applications in ad-hoc object acquisition, augmented reality and other 
fields. A key challenge to online reconstruction remains error 
accumulation in the reconstructed camera trajectory, due to 
drift-inducing instabilities in the range scan alignments of the 
underlying iterative-closest-point (ICP) algorithm. Various strategies 
have been proposed to mitigate that drift, including SIFT-based 
pre-alignment, color-based weighting of ICP pairs, stronger weighting of
 edge features, and so on. In our work, we focus on surface curvature as
 a feature that is detectable on range scans alone and hence does not 
depend on accurate multi-sensor alignment. In contrast to previous work 
that took curvature into consideration, however, we treat curvature as 
an independent quantity that we consistently incorporate into every 
stage of the real-time reconstruction pipeline, including densely 
curvature-weighted ICP, range image fusion, local surface 
reconstruction, and rendering. Using multiple benchmark sequences, and 
in direct comparison to other state-of-the-art online acquisition 
systems, we show that our approach significantly reduces drift, both 
when analyzing individual pipeline stages in isolation, as well as seen 
across the online reconstruction pipeline as a whole.},
<br>  keywords={cameras;computational geometry;image fusion;image 
reconstruction;iterative methods;object detection;rendering (computer 
graphics);ICP pairs;accurate online surface reconstruction;ad-hoc object
 acquisition;augmented reality;densely curvature-weighted 
ICP;drift-inducing instabilities;edge features;error 
accumulation;hand-held depth cameras;iterative-closest-point 
algorithm;local surface reconstruction;online acquisition systems;online
 reconstruction pipeline;range image fusion;range scan 
alignments;real-time reconstruction pipeline;real-time scene 
acquisition;reconstructed camera trajectory;rendering;surface 
curvature;Cameras;Geometry;Image reconstruction;Iterative closest point 
algorithm;Real-time systems;Surface reconstruction;Three-dimensional 
displays;Tracking;3D reconstruction;camera tracking;curvature;depth 
fusion;differential geometry},
<br>  doi={10.1109/TPAMI.2017.2648803},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7797452,
<br>
author={M. Cl√©ment and A. Poulenard and C. Kurtz and L. Wendling},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Directional Enlacement Histograms for the Description of Complex Spatial Configurations between Objects}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2366-2380},
<br>  abstract={The analysis of spatial relations between objects in 
digital images plays a crucial role in various application domains 
related to pattern recognition and computer vision. Classical models for
 the evaluation of such relations are usually sufficient for the 
handling of simple objects, but can lead to ambiguous results in more 
complex situations. In this article, we investigate the modeling of 
spatial configurations where the objects can be imbricated in each 
other. We formalize this notion with the term enlacement, from which we 
also derive the term interlacement, denoting a mutual enlacement of two 
objects. Our main contribution is the proposition of new relative 
position descriptors designed to capture the enlacement and 
interlacement between two-dimensional objects. These descriptors take 
the form of circular histograms allowing to characterize spatial 
configurations with directional granularity, and they highlight useful 
invariance properties for typical image understanding applications. We 
also show how these descriptors can be used to evaluate different 
complex spatial relations, such as the surrounding of objects. 
Experimental results obtained in the different application domains of 
medical imaging, document image analysis and remote sensing, confirm the
 genericity of this approach.},
<br>  keywords={computer vision;document image processing;feature 
extraction;image classification;image recognition;image 
segmentation;medical image processing;circular histograms;complex 
spatial configurations;computer vision;digital images;directional 
enlacement histograms;directional granularity;document image 
analysis;image understanding applications;medical imaging;mutual 
enlacement;pattern recognition;relative position descriptors;term 
interlacement;two-dimensional objects;Computational modeling;Computer 
vision;Histograms;Image representation;Pattern recognition;Remote 
sensing;Enlacement histograms;image understanding;interlacement 
histograms;relative position descriptors;spatial relations},
<br>  doi={10.1109/TPAMI.2016.2645151},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7807308,
<br>
author={A. T. Pham and R. Raich and X. Z. Fern},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Dynamic Programming for Instance Annotation in Multi-Instance Multi-Label Learning}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2381-2394},
<br>  abstract={Labeling data for classification requires significant 
human effort. To reduce labeling cost, instead of labeling every 
instance, a group of instances (bag) is labeled by a single bag label. 
Computer algorithms are then used to infer the label for each instance 
in a bag, a process referred to as instance annotation. This task is 
challenging due to the ambiguity regarding the instance labels. We 
propose a discriminative probabilistic model for the instance annotation
 problem and introduce an expectation maximization framework for 
inference, based on the maximum likelihood approach. For many 
probabilistic approaches, brute-force computation of the instance label 
posterior probability given its bag label is exponential in the number 
of instances in the bag. Our contribution is a dynamic programming 
method for computing the posterior that is linear in the number of 
instances. We evaluate our method using both benchmark and real world 
data sets, in the domain of bird song, image annotation, and activity 
recognition. In many cases, the proposed framework outperforms, 
sometimes significantly, the current state-of-the-art MIML learning 
methods, both in instance label prediction and bag label prediction.},
<br>  keywords={dynamic programming;expectation-maximisation 
algorithm;inference mechanisms;learning (artificial 
intelligence);pattern classification;probability;bag label 
prediction;brute-force computation;classification;discriminative 
probabilistic model;dynamic programming method;expectation maximization 
framework;image annotation;inference;instance annotation 
problem;instance label posterior probability;instance label 
prediction;instance labels;maximum likelihood approach;multiinstance 
multilabel learning;single bag label;Computational modeling;Data 
models;Dynamic programming;Graphical models;Labeling;Probabilistic 
logic;Multi-instance multi-label learning;dynamic 
programming;expectation maximization;graphical model;instance 
annotation},
<br>  doi={10.1109/TPAMI.2017.2647944},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7807217,
<br>
author={L. Puggini and S. McLoone},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Forward Selection Component Analysis: Algorithms and Applications}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2395-2408},
<br>  abstract={Principal Component Analysis (PCA) is a powerful and 
widely used tool for dimensionality reduction. However, the principal 
components generated are linear combinations of all the original 
variables and this often makes interpreting results and root-cause 
analysis difficult. Forward Selection Component Analysis (FSCA) is a 
recent technique that overcomes this difficulty by performing variable 
selection and dimensionality reduction at the same time. This paper 
provides, for the first time, a detailed presentation of the FSCA 
algorithm, and introduces a number of new variants of FSCA that 
incorporate a refinement step to improve performance. We then show 
different applications of FSCA and compare the performance of the 
different variants with PCA and Sparse PCA. The results demonstrate the 
efficacy of FSCA as a low information loss dimensionality reduction and 
variable selection technique and the improved performance achievable 
through the inclusion of a refinement step.},
<br>  keywords={principal component analysis;FSCA;FSCA 
algorithm;PCA;forward selection component analysis;principal component 
analysis;principal components;root-cause analysis;variable selection 
technique;Algorithm design and analysis;Feature extraction;Input 
variables;Matching pursuit algorithms;Power capacitors;Principal 
component analysis;Signal processing algorithms;Unsupervised 
dimensionality reduction;feature selection;subset selection},
<br>  doi={10.1109/TPAMI.2017.2648792},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7807338,
<br>
author={F. Rodrigues and M. Louren√ßo and B. Ribeiro and F. C. Pereira},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Learning Supervised Topic Models for Classification and Regression from Crowds}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2409-2422},
<br>  abstract={The growing need to analyze large collections of 
documents has led to great developments in topic modeling. Since 
documents are frequently associated with other related variables, such 
as labels or ratings, much interest has been placed on supervised topic 
models. However, the nature of most annotation tasks, prone to ambiguity
 and noise, often with high volumes of documents, deem learning under a 
single-annotator assumption unrealistic or unpractical for most 
real-world applications. In this article, we propose two supervised 
topic models, one for classification and another for regression 
problems, which account for the heterogeneity and biases among different
 annotators that are encountered in practice when learning from crowds. 
We develop an efficient stochastic variational inference algorithm that 
is able to scale to very large datasets, and we empirically demonstrate 
the advantages of the proposed model over state-of-the-art approaches.},
<br>  keywords={document handling;inference mechanisms;learning 
(artificial intelligence);regression analysis;stochastic 
processes;annotation tasks;classification;crowds;documents;real-world 
applications;regression problems;single-annotator assumption;stochastic 
variational inference algorithm;supervised topic models learning;topic 
modeling;Analytical models;Data models;Inference 
algorithms;Labeling;Predictive models;Stochastic processes;Supervised 
learning;Topic models;crowdsoucing;multiple annotators;supervised 
learning},
<br>  doi={10.1109/TPAMI.2017.2648786},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7814315,
<br>
author={S. Li and S. Purushotham and C. Chen and Y. Ren and C. C. J. Kuo},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Measuring and Predicting Tag Importance for Image Retrieval}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2423-2436},
<br>  abstract={Textual data such as tags, sentence descriptions are 
combined with visual cues to reduce the semantic gap for image retrieval
 applications in today's Multimodal Image Retrieval (MIR) systems. 
However, all tags are treated as equally important in these systems, 
which may result in misalignment between visual and textual modalities 
during MIR training. This will further lead to degenerated retrieval 
performance at query time. To address this issue, we investigate the 
problem of tag importance prediction, where the goal is to automatically
 predict the tag importance and use it in image retrieval. To achieve 
this, we first propose a method to measure the relative importance of 
object and scene tags from image sentence descriptions. Using this as 
the ground truth, we present a tag importance prediction model to 
jointly exploit visual, semantic and context cues. The Structural 
Support Vector Machine (SSVM) formulation is adopted to ensure efficient
 training of the prediction model. Then, the Canonical Correlation 
Analysis (CCA) is employed to learn the relation between the image 
visual feature and tag importance to obtain robust retrieval 
performance. Experimental results on three real-world datasets show a 
significant performance improvement of the proposed MIR with Tag 
Importance Prediction (MIR/TIP) system over other MIR systems.},
<br>  keywords={image retrieval;learning (artificial 
intelligence);support vector machines;CCA;MIR systems;MIR 
training;canonical correlation analysis;degenerated retrieval 
performance;image retrieval applications;image sentence 
descriptions;image visual feature;multimodal image retrieval 
systems;object tags;robust retrieval performance;scene tags;semantic 
context cues;structural support vector machine formulation;tag 
importance prediction model;textual data;textual modalities;visual 
context cues;visual cues;visual modalities;Image retrieval;Learning 
systems;Predictive models;Semantics;Training;Visual 
databases;Visualization;Multimodal image retrieval (MIR);cross-domain 
learning;image retrieval;importance measure;importance 
prediction;semantic gap;tag importance},
<br>  doi={10.1109/TPAMI.2017.2651818},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7814339,
<br>
author={X. T. Yuan and Q. Liu},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Newton-Type Greedy Selection Methods for $ell _0$ -Constrained Minimization}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2437-2450},
<br>  abstract={We introduce a family of Newton-type greedy selection methods for <inline-formula><tex-math notation="LaTeX">$ell _0$</tex-math><alternatives><inline-graphic xlink:href="yuan-ieq2-2651813.gif"></inline-graphic></alternatives></inline-formula>
 -constrained minimization problems. The basic idea is to construct a 
quadratic function to approximate the original objective function around
 the current iterate and solve the constructed quadratic program over 
the cardinality constraint. The next iterate is then estimated via a 
line search operation between the current iterate and the solution of 
the sparse quadratic program. This iterative procedure can be 
interpreted as an extension of the constrained Newton methods from 
convex minimization to non-convex <inline-formula><tex-math notation="LaTeX">$ell _0$ </tex-math><alternatives><inline-graphic xlink:href="yuan-ieq3-2651813.gif"></inline-graphic></alternatives></inline-formula>
 -constrained minimization. We show that the proposed algorithms 
converge asymptotically and the rate of local convergence is superlinear
 up to certain estimation error. Our methods compare favorably against 
several state-of-the-art greedy selection methods when applied to sparse
 logistic regression and sparse support vector machines.},
<br>  keywords={Convergence;Iterative methods;Linear 
programming;Minimization;Optimization;Signal processing 
algorithms;Sparse matrices;M-estimation;Newton methods;Sparsity;greedy 
selection;optimization},
<br>  doi={10.1109/TPAMI.2017.2651813},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7807327,
<br>
author={H. Laga and Q. Xie and I. H. Jermyn and A. Srivastava},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Numerical Inversion of SRNF Maps for Elastic Shape Analysis of Genus-Zero Surfaces}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2451-2464},
<br>  abstract={Recent developments in elastic shape analysis (ESA) are 
motivated by the fact that it provides a comprehensive framework for 
simultaneous registration, deformation, and comparison of shapes. These 
methods achieve computational efficiency using certain square-root 
representations that transform invariant elastic metrics into euclidean 
metrics, allowing for the application of standard algorithms and 
statistical tools. For analyzing shapes of embeddings of <inline-formula><tex-math notation="LaTeX">$mathbf {S}^2$</tex-math><alternatives> <inline-graphic xlink:href="laga-ieq1-2647596.gif"></inline-graphic></alternatives></inline-formula> in <inline-formula> <tex-math notation="LaTeX">$mathbb {R}^3$</tex-math><alternatives><inline-graphic xlink:href="laga-ieq2-2647596.gif"> </inline-graphic></alternatives></inline-formula>, Jermyn et&nbsp;al.&nbsp;<xref ref-type="bibr" rid="ref1">[1]</xref> introduced square-root normal fields (SRNFs), which transform an elastic metric, with desirable invariant properties, into the <inline-formula><tex-math notation="LaTeX">$mathbb {L}^2$</tex-math><alternatives> <inline-graphic xlink:href="laga-ieq3-2647596.gif"></inline-graphic></alternatives></inline-formula>
 metric. These SRNFs are essentially surface normals scaled by 
square-roots of infinitesimal area elements. A critical need in shape 
analysis is a method for inverting solutions (deformations, averages, 
modes of variations, etc.) computed in SRNF space, back to the original 
surface space for visualizations and inferences. Due to the lack of 
theory for understanding SRNF maps and their inverses, we take a 
numerical approach, and derive an efficient multiresolution algorithm, 
based on solving an optimization problem in the surface space, that 
estimates surfaces corresponding to given SRNFs. This solution is found 
to be effective even for complex shapes that undergo significant 
deformations including bending and stretching, e.g., human bodies and 
animals. We use this inversion for computing elastic - hape 
deformations, transferring deformations, summarizing shapes, and for 
finding modes of variability in a given collection, while simultaneously
 registering the surfaces. We demonstrate the proposed algorithms using a
 statistical analysis of human body shapes, classification of generic 
surfaces, and analysis of brain structures.},
<br>  keywords={Extraterrestrial measurements;Optimization;Shape 
analysis;Space vehicles;Statistical analysis;Surface treatment;Elastic 
shape analysis;Riemannian metrics;elastic registration;shape 
modeling;shape statistics;square-root representations},
<br>  doi={10.1109/TPAMI.2016.2647596},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7797239,
<br>
author={T. Wu and Y. Lu and S. C. Zhu},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Online Object Tracking, Learning and Parsing with And-Or Graphs}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2465-2480},
<br>  abstract={This paper presents a method, called AOGTracker, for 
simultaneously tracking, learning and parsing (TLP) of unknown objects 
in video sequences with a hierarchical and compositional And-Or graph 
(AOG) representation. The TLP method is formulated in the Bayesian 
framework with a spatial and a temporal dynamic programming (DP) 
algorithms inferring object bounding boxes on-the-fly. During online 
learning, the AOG is discriminatively learned using latent SVM [1] to 
account for appearance (e.g., lighting and partial occlusion) and 
structural (e.g., different poses and viewpoints) variations of a 
tracked object, as well as distractors (e.g., similar objects) in 
background. Three key issues in online inference and learning are 
addressed: (i) maintaining purity of positive and negative examples 
collected online, (ii) controling model complexity in latent structure 
learning, and (iii) identifying critical moments to re-learn the 
structure of AOG based on its intrackability. The intrackability 
measures uncertainty of an AOG based on its score maps in a frame. In 
experiments, our AOGTracker is tested on two popular tracking benchmarks
 with the same parameter setting: the TB-100/50/CVPR2013 benchmarks [2] ,
 [3] , and the VOT benchmarks [4] -VOT 2013, 2014, 2015 and TIR2015 
(thermal imagery tracking). In the former, our AOGTracker outperforms 
state-of-the-art tracking algorithms including two trackers based on 
deep convolutional network [5] , [6] . In the latter, our AOGTracker 
outperforms all other trackers in VOT2013 and is comparable to the 
state-of-the-art methods in VOT2014, 2015 and TIR2015.},
<br>  keywords={Bayes methods;dynamic programming;image 
representation;image sequences;learning (artificial intelligence);object
 detection;object tracking;support vector machines;AOG 
representation;AOGTracker;And-Or graphs;Bayesian framework;SVM;TLP 
method;boxes on-the-fly;latent structure learning;online 
inference;online learning;online object tracking;parsing;temporal 
dynamic programming;video sequences;Benchmark testing;Computational 
modeling;Dynamic programming;Hidden Markov models;Object 
tracking;Trajectory;Visual tracking;and-or graph;dynamic 
programming;intrackability;latent SVM},
<br>  doi={10.1109/TPAMI.2016.2644963},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7803544,
<br>
author={V. Badrinarayanan and A. Kendall and R. Cipolla},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2481-2495},
<br>  abstract={We present a novel and practical deep fully 
convolutional neural network architecture for semantic pixel-wise 
segmentation termed SegNet. This core trainable segmentation engine 
consists of an encoder network, a corresponding decoder network followed
 by a pixel-wise classification layer. The architecture of the encoder 
network is topologically identical to the 13 convolutional layers in the
 VGG16 network [1] . The role of the decoder network is to map the low 
resolution encoder feature maps to full input resolution feature maps 
for pixel-wise classification. The novelty of SegNet lies is in the 
manner in which the decoder upsamples its lower resolution input feature
 map(s). Specifically, the decoder uses pooling indices computed in the 
max-pooling step of the corresponding encoder to perform non-linear 
upsampling. This eliminates the need for learning to upsample. The 
upsampled maps are sparse and are then convolved with trainable filters 
to produce dense feature maps. We compare our proposed architecture with
 the widely adopted FCN [2] and also with the well known 
DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison 
reveals the memory versus accuracy trade-off involved in achieving good 
segmentation performance. SegNet was primarily motivated by scene 
understanding applications. Hence, it is designed to be efficient both 
in terms of memory and computational time during inference. It is also 
significantly smaller in the number of trainable parameters than other 
competing architectures and can be trained end-to-end using stochastic 
gradient descent. We also performed a controlled benchmark of SegNet and
 other architectures on both road scenes and SUN RGB-D indoor scene 
segmentation tasks. These quantitative assessments show that SegNet 
provides good performance with competitive inference time and most 
efficient inference memory-wise as compared to other architectures. We 
also provide a Caffe implementation of SegNet and a web demo at 
http://mi.eng.cam- ac.uk/projects/segnet/.},
<br>  keywords={feature extraction;gradient methods;image 
classification;image colour analysis;image representation;image 
resolution;image segmentation;inference mechanisms;learning (artificial 
intelligence);self-organising feature maps;topology;Caffe 
implementation;DeconvNet architectures;DeepLab-LargeFOV;FCN;SUN RGB-D 
indoor scene segmentation;SUN RGB-D indoor scene segmentation 
tasks;SegNet;VGG16 network;competitive inference time;convolutional 
layers;core trainable segmentation engine;decoder network;deep 
convolutional encoder-decoder architecture;dense feature maps;encoder 
network;image segmentation;low resolution encoder feature maps;lower 
resolution input feature map;max-pooling step;nonlinear 
upsampling;pixel-wise classification layer;pixel-wise 
segmentation;practical deep fully convolutional neural network 
architecture;stochastic gradient descent;Computer 
architecture;Convolutional codes;Decoding;Image segmentation;Neural 
networks;Semantics;Training;Deep convolutional neural 
networks;decoder;encoder;indoor scenes;pooling;road scenes;semantic 
pixel-wise segmentation;upsampling},
<br>  doi={10.1109/TPAMI.2016.2644615},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7814343,
<br>
author={O. Freifeld and S. Hauberg and K. Batmanghelich and J. W. Fisher},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Transformations Based on Continuous Piecewise-Affine Velocity Fields}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2496-2509},
<br>  abstract={We propose novel finite-dimensional spaces of well-behaved <inline-formula><tex-math notation="LaTeX">$mathbb {R}^nrightarrow mathbb {R}^n$</tex-math><alternatives><inline-graphic xlink:href="freifeld-ieq1-2646685.gif"> </inline-graphic></alternatives></inline-formula>
 transformations. The latter are obtained by (fast and highly-accurate) 
integration of continuous piecewise-affine velocity fields. The proposed
 method is simple yet highly expressive, effortlessly handles optional 
constraints (e.g., volume preservation and/or boundary conditions), and 
supports convenient modeling choices such as smoothing priors and 
coarse-to-fine analysis. Importantly, the proposed approach, partly due 
to its rapid likelihood evaluations and partly due to its other 
properties, facilitates tractable inference over rich transformation 
spaces, including using Markov-Chain Monte-Carlo methods. Its 
applications include, but are not limited to: monotonic regression (more
 generally, optimization over monotonic functions); modeling cumulative 
distribution functions or histograms; time-warping; image warping; image
 registration; real-time diffeomorphic image editing; data augmentation 
for image classifiers. Our GPU-based code is publicly available.},
<br>  keywords={Biomedical imaging;Complexity theory;Computational 
modeling;Computer vision;Distribution 
functions;Histograms;Trajectory;Spatial transformations, continuous 
piecewise-affine velocity fields, diffeomorphisms, tessellations, 
priors, MCMC},
<br>  doi={10.1109/TPAMI.2016.2646685},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7784804,
<br>
author={C. Lu and D. Lin and J. Jia and C. K. Tang},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Two-Class Weather Classification}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2510-2524},
<br>  abstract={Given a single outdoor image, we propose a collaborative
 learning approach using novel weather features to label the image as 
either sunny or cloudy. Though limited, this two-class classification 
problem is by no means trivial given the great variety of outdoor images
 captured by different cameras where the images may have been edited 
after capture. Our overall weather feature combines the data-driven 
convolutional neural network (CNN) feature and well-chosen 
weather-specific features. They work collaboratively within a unified 
optimization framework that is aware of the presence (or absence) of a 
given weather cue during learning and classification. In this paper we 
propose a new data augmentation scheme to substantially enrich the 
training data, which is used to train a latent SVM framework to make our
 solution insensitive to global intensity transfer. Extensive 
experiments are performed to verify our method. Compared with our 
previous work and the sole use of a CNN classifier, this paper improves 
the accuracy up to 7-8 percent. Our weather image dataset is available 
together with the executable of our classifier.},
<br>  keywords={atmospheric techniques;climatology;feature 
extraction;feedforward neural nets;geophysical image processing;image 
classification;learning (artificial intelligence);pattern 
classification;support vector machines;CNN;cameras;collaborative 
learning approach;convolutional neural network feature;data augmentation
 scheme;latent SVM framework;learning classification;outdoor 
images;single outdoor image;training data;two-class classification 
problem;two-class weather classification;unified optimization 
framework;weather cue;weather image dataset;weather-specific 
features;Atmospheric measurements;Cameras;Clouds;Meteorology;Neural 
networks;Support vector machines;Training;Weather understanding;image 
classification;structure SVM},
<br>  doi={10.1109/TPAMI.2016.2640295},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7797474,
<br>
author={Z. Shi and Y. Yang and T. M. Hospedales and T. Xiang},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Weakly-Supervised Image Annotation and Segmentation with Objects and Attributes}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2525-2538},
<br>  abstract={We propose to model complex visual scenes using a 
non-parametric Bayesian model learned from weakly labelled images 
abundant on media sharing sites such as Flickr. Given weak image-level 
annotations of objects and attributes without locations or associations 
between them, our model aims to learn the appearance of object and 
attribute classes as well as their association on each object instance. 
Once learned, given an image, our model can be deployed to tackle a 
number of vision problems in a joint and coherent manner, including 
recognising objects in the scene (automatic object annotation), 
describing objects using their attributes (attribute prediction and 
association), and localising and delineating the objects (object 
detection and semantic segmentation). This is achieved by developing a 
novel Weakly Supervised Markov Random Field Stacked Indian Buffet 
Process (WS-MRF-SIBP) that models objects and attributes as latent 
factors and explicitly captures their correlations within and across 
superpixels. Extensive experiments on benchmark datasets demonstrate 
that our weakly supervised model significantly outperforms weakly 
supervised alternatives and is often comparable with existing strongly 
supervised models on a variety of tasks including semantic segmentation,
 automatic image annotation and retrieval based on object-attribute 
associations.},
<br>  keywords={Bayes methods;Markov processes;image annotation;image 
classification;image segmentation;object detection;object 
recognition;WS-MRF-SIBP;attribute classes;attribute prediction;automatic
 image annotation;image retrieval;image segmentation;model complex 
visual scenes;nonparametric Bayesian model;object 
detection;object-attribute associations;semantic segmentation;strongly 
supervised models;weak image-level annotations;weakly labelled 
images;weakly supervised Markov random field stacked indian buffet 
process;weakly supervised alternatives;weakly-supervised image 
annotation;Computer vision;Correlation;Data models;Detectors;Image 
segmentation;Semantics;Training;Indian buffet process;Weakly supervised 
learning;non-parametric Bayesian model;object-attribute 
association;semantic segmentation},
<br>  doi={10.1109/TPAMI.2016.2645157},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7801884,
<br>
author={M. Elhoseiny and A. Elgammal and B. Saleh},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Write a Classifier: Predicting Visual Classifiers from Unstructured Text}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2539-2553},
<br>  abstract={People typically learn through exposure to visual 
concepts associated with linguistic descriptions. For instance, teaching
 visual object categories to children is often accompanied by 
descriptions in text or speech. In a machine learning context, these 
observations motivates us to ask whether this learning process could be 
computationally modeled to learn visual classifiers. More specifically, 
the main question of this work is how to utilize purely textual 
description of visual classes with no training images, to learn explicit
 visual classifiers for them. We propose and investigate two baseline 
formulations, based on regression and domain transfer, that predict a 
linear classifier. Then, we propose a new constrained optimization 
formulation that combines a regression function and a knowledge transfer
 function with additional constraints to predict the parameters of a 
linear classifier. We also propose a generic kernelized models where a 
kernel classifier is predicted in the form defined by the representer 
theorem. The kernelized models allow defining and utilizing any two 
Reproducing Kernel Hilbert Space (RKHS) kernel functions in the visual 
space and text space, respectively. We finally propose a kernel function
 between unstructured text descriptions that builds on distributional 
semantics, which shows an advantage in our setting and could be useful 
for other applications. We applied all the studied models to predict 
visual classifiers on two fine-grained and challenging categorization 
datasets (CU Birds and Flower Datasets), and the results indicate 
successful predictions of our final model over several baselines that we
 designed.},
<br>  keywords={Hilbert spaces;image classification;learning (artificial
 intelligence);optimisation;text analysis;constrained optimization 
formulation;distributional semantics;domain transfer;explicit visual 
classifiers;generic kernelized models;kernel classifier;kernel 
function;knowledge transfer function;learning process;linear 
classifier;linguistic descriptions;machine learning context;purely 
textual description;regression function;reproducing kernel Hilbert space
 kernel functions;text space;unstructured text descriptions;visual 
classes;visual concepts;visual object categories;visual space;Knowledge 
transfer;Noise measurement;Pragmatics;Semantics;Text 
processing;Training;Visualization;Language and vision;noisy 
text;unstructured text;zero shot learning},
<br>  doi={10.1109/TPAMI.2016.2643667},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>@ARTICLE{7855789,
<br>
author={B. Li and C. Yuan and W. Xiong and W. Hu and H. Peng and X. Ding and S. Maybank},
<br>  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
<br> title={Multi-View Multi-Instance Learning Based on Joint Sparse Representation and Multi-View Dictionary Learning}, 
<br>  year={2017},
<br>  volume={39},
<br>  number={12},
<br>  pages={2554-2560},
<br>  abstract={In multi-instance learning (MIL), the relations among 
instances in a bag convey important contextual information in many 
applications. Previous studies on MIL either ignore such relations or 
simply model them with a fixed graph structure so that the overall 
performance inevitably degrades in complex environments. To address this
 problem, this paper proposes a novel multi-view multi-instance learning
 algorithm (M<inline-formula><tex-math notation="LaTeX">$^2$ </tex-math><alternatives><inline-graphic xlink:href="li-ieq1-2669303.gif"></inline-graphic></alternatives></inline-formula>IL) that combines multiple context structures in a bag into a unified framework. The novel aspects are: (i) we propose a sparse <inline-formula><tex-math notation="LaTeX">$varepsilon$</tex-math><alternatives> <inline-graphic xlink:href="li-ieq2-2669303.gif"></inline-graphic></alternatives></inline-formula>-graph
 model that can generate different graphs with different parameters to 
represent various context relations in a bag, (ii) we propose a 
multi-view joint sparse representation that integrates these graphs into
 a unified framework for bag classification, and (iii) we propose a 
multi-view dictionary learning algorithm to obtain a multi-view graph 
dictionary that considers cues from all views simultaneously to improve 
the discrimination of the M<inline-formula><tex-math notation="LaTeX"> $^2$</tex-math><alternatives><inline-graphic xlink:href="li-ieq3-2669303.gif"></inline-graphic></alternatives></inline-formula>IL. Experiments and analyses in many practical applications prove the effectiveness of the M<inline-formula> <tex-math notation="LaTeX">$^2$</tex-math><alternatives><inline-graphic xlink:href="li-ieq4-2669303.gif"> </inline-graphic></alternatives></inline-formula>IL.},
<br>  keywords={Adaptation models;Context 
modeling;Dictionaries;Euclidean distance;Learning systems;Sparse 
matrices;Support vector machines;Multi-instance learning;dictionary 
learning;multi-view;sparse representation},
<br>  doi={10.1109/TPAMI.2017.2669303},
<br>  ISSN={0162-8828},
<br>  month={Dec},}<br>

</body></html>