@ARTICLE{7555337,
author={W. Lian and L. Zhang and M. H. Yang},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={An Efficient Globally Optimal Algorithm for Asymmetric Point Matching},
year={2017},
volume={39},
number={7},
pages={1281-1293},
abstract={Although the robust point matching algorithm has been demonstrated to
be effective for non-rigid registration, there are several issues with the
adopted deterministic annealing optimization technique. First, it is not
globally optimal and regularization on the spatial transformation is needed for
good matching results. Second, it tends to align the mass centers of two point
sets. To address these issues, we propose a globally optimal algorithm for the
robust point matching problem in the case that each model point has a
counterpart in scene set. By eliminating the transformation variables, we show
that the original matching problem is reduced to a concave quadratic assignment
problem where the objective function has a low rank Hessian matrix. This
facilitates the use of large scale global optimization techniques. We propose a
modified normal rectangular branch-and-bound algorithm to solve the resulting
problem where multiple rectangles are simultaneously subdivided to increase the
chance of shrinking the rectangle containing the global optimal solution. In
addition, we present an efficient lower bounding scheme which has a linear
assignment formulation and can be efficiently solved. Extensive experiments on
synthetic and real datasets demonstrate the proposed algorithm performs
favorably against the state-of-the-art methods in terms of robustness to
outliers, matching accuracy, and run-time.},
keywords={Hessian matrices;image matching;image registration;quadratic
programming;simulated annealing;tree searching;asymmetric point
matching;branch-and-bound algorithm;concave quadratic assignment
problem;deterministic annealing optimization technique;efficient globally
optimal algorithm;low rank Hessian matrix;non-rigid registration;robust point
matching algorithm;transformation variables;Algorithm design and
analysis;Annealing;Context;Electronic mail;Linear
programming;Optimization;Robustness;Branch and bound;concave
optimization;linear assignment;point correspondence;robust point matching},
doi={10.1109/TPAMI.2016.2603988},
ISSN={0162-8828},
month={July},}
@ARTICLE{7534828,
author={R. Agarwal and Z. Chen and S. V. Sarma},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={A Novel Nonparametric Maximum Likelihood Estimator for Probability
Density Functions},
year={2017},
volume={39},
number={7},
pages={1294-1308},
abstract={Parametric maximum likelihood (ML) estimators of probability density
functions (pdfs) are widely used today because they are efficient to compute
and have several nice properties such as consistency, fast convergence rates,
and asymptotic normality. However, data is often complex making parametrization
of the pdf difficult, and nonparametric estimation is required. Popular
nonparametric methods, such as kernel density estimation (KDE), produce
consistent estimators but are not ML and have slower convergence rates than
parametric ML estimators. Further, these nonparametric methods do not share the
other desirable properties of parametric ML estimators. This paper introduces a
nonparametric ML estimator that assumes that the square-root of the underlying
pdf is band-limited (BL) and hence “smooth”. The BLML estimator is computed and
shown to be consistent. Although convergence rates are not theoretically
derived, the BLML estimator exhibits faster convergence rates than state-of-
the-art nonparametric methods in simulations. Further, algorithms to compute
the BLML estimator with lesser computational complexity than that of KDE
methods are presented. The efficacy of the BLML estimator is shown by applying
it to (i) density tail estimation and (ii) density estimation of complex
neuronal receptive fields where it outperforms state-of-the-art methods used in
neuroscience.},
keywords={maximum likelihood estimation;neural nets;probability;BLML
estimator;KDE methods;band-limited ML estimator;complex neuronal receptive
fields estimation;density tail estimation;nonparametric ML
estimator;nonparametric maximum likelihood estimator;probability density
functions;Computational modeling;Convergence;Kernel;Maximum likelihood
estimation;Probability density function;Random variables;Maximum
likelihood;density;estimation;neuronal receptive fields;nonparametric;pdf;tail
estimation},
doi={10.1109/TPAMI.2016.2598333},
ISSN={0162-8828},
month={July},}
@ARTICLE{7516654,
author={S. Ramalingam and P. Sturm},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={A Unifying Model for Camera Calibration},
year={2017},
volume={39},
number={7},
pages={1309-1319},
abstract={This paper proposes a unified theory for calibrating a wide variety
of camera models such as pinhole, fisheye, cata-dioptric, and multi-camera
networks. We model any camera as a set of image pixels and their associated
camera rays in space. Every pixel measures the light traveling along a (half-
) ray in 3-space, associated with that pixel. By this definition, calibration
simply refers to the computation of the mapping between pixels and the
associated 3D rays. Such a mapping can be computed using images of calibration
grids, which are objects with known 3D geometry, taken from unknown positions.
This general camera model allows to represent non-central cameras; we also
consider two special subclasses, namely central and axial cameras. In a central
camera, all rays intersect in a single point, whereas the rays are completely
arbitrary in a non-central one. Axial cameras are an intermediate case: the
camera rays intersect a single line. In this work, we show the theory for
calibrating central, axial and non-central models using calibration grids,
which can be either three-dimensional or planar.},
keywords={calibration;cameras;3D geometry;3D rays;axial cameras;calibration
grids;camera calibration;camera rays;central camera;image pixels;unifying
model;Calibration;Cameras;Computational modeling;Mirrors;Solid modeling;Three-
dimensional displays;Camera calibration;cata-dioptric;generic imaging
model;non-central;omni-directional},
doi={10.1109/TPAMI.2016.2592904},
ISSN={0162-8828},
month={July},}
@ARTICLE{7506134,
author={W. Ouyang and X. Zeng and X. Wang and S. Qiu and P. Luo and Y. Tian and
H. Li and S. Yang and Z. Wang and H. Li and K. Wang and J. Yan and C. C. Loy
and X. Tang},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={DeepID-Net: Object Detection with Deformable Part Based Convolutional
Neural Networks},
year={2017},
volume={39},
number={7},
pages={1320-1334},
abstract={In this paper, we propose deformable deep convolutional neural
networks for generic object detection. This new deep learning object detection
framework has innovations in multiple aspects. In the proposed new deep
architecture, a new deformation constrained pooling (def-pooling) layer models
the deformation of object parts with geometric constraint and penalty. A new
pre-training strategy is proposed to learn feature representations more
suitable for the object detection task and with good generalization capability.
By changing the net structures, training strategies, adding and removing some
key components in the detection pipeline, a set of models with large diversity
are obtained, which significantly improves the effectiveness of model
averaging. The proposed approach improves the mean averaged precision obtained
by RCNN [1] , which was the state-of-the-art, from $31$  to  $50.3$  percent on
the ILSVRC2014 detection test set. It also outperforms the winner of
ILSVRC2014, GoogLeNet, by 6.1 percent. Detailed component-wise analysis is also
provided through extensive experimental evaluation, which provides a global
view for people to understand the deep learning object detection pipeline.},
keywords={Context modeling;Deformable models;Machine learning;Neural
networks;Object detection;Training;Visualization;CNN;convolutional neural
networks;deep learning;deep model;object detection},
doi={10.1109/TPAMI.2016.2587642},
ISSN={0162-8828},
month={July},}
@ARTICLE{7526450,
author={J. Xie and G. Dai and F. Zhu and E. K. Wong and Y. Fang},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={DeepShape: Deep-Learned Shape Descriptor for 3D Shape Retrieval},
year={2017},
volume={39},
number={7},
pages={1335-1345},
abstract={Complex geometric variations of 3D models usually pose great
challenges in 3D shape matching and retrieval. In this paper, we propose a
novel 3D shape feature learning method to extract high-level shape features
that are insensitive to geometric deformations of shapes. Our method uses a
discriminative deep auto-encoder to learn deformation-invariant shape features.
First, a multiscale shape distribution is computed and used as input to the
auto-encoder. We then impose the Fisher discrimination criterion on the neurons
in the hidden layer to develop a deep discriminative auto-encoder. Finally, the
outputs from the hidden layers of the discriminative auto-encoders at different
scales are concatenated to form the shape descriptor. The proposed method is
evaluated on four benchmark datasets that contain 3D models with large
geometric variations: McGill, SHREC'10 ShapeGoogle, SHREC'14 Human and SHREC'14
Large Scale Comprehensive Retrieval Track Benchmark datasets. Experimental
results on the benchmark datasets demonstrate the effectiveness of the proposed
method for 3D shape retrieval.},
keywords={computer graphics;feature extraction;image retrieval;learning
(artificial intelligence);neural nets;shape recognition;3D shape feature
learning method;3D shape retrieval;Fisher discrimination criterion;McGill
datasets;SHREC'10 ShapeGoogle datasets;SHREC'14 Human datasets;SHREC'14 Large
Scale Comprehensive Retrieval Track Benchmark datasets;deep-learned shape
descriptor;deformation-invariant shape features;discriminative deep auto-
encoder;high-level shape feature extraction;multiscale shape
distribution;neurons;Feature extraction;Heating;Kernel;Neurons;Shape;Solid
modeling;Three-dimensional displays;3D shape retrieval;Fisher discrimination
criterion;auto-encoder;heat diffusion;heat kernel signature},
doi={10.1109/TPAMI.2016.2596722},
ISSN={0162-8828},
month={July},}
@ARTICLE{7508476,
author={X. Lagorce and G. Orchard and F. Galluppi and B. E. Shi and R. B.
Benosman},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={HOTS: A Hierarchy of Event-Based Time-Surfaces for Pattern Recognition},

year={2017},
volume={39},
number={7},
pages={1346-1359},
abstract={This paper describes novel event-based spatio-temporal features
called time-surfaces and how they can be used to create a hierarchical event-
based pattern recognition architecture. Unlike existing hierarchical
architectures for pattern recognition, the presented model relies on a time
oriented approach to extract spatio-temporal features from the asynchronously
acquired dynamics of a visual scene. These dynamics are acquired using
biologically inspired frameless asynchronous event-driven vision sensors.
Similarly to cortical structures, subsequent layers in our hierarchy extract
increasingly abstract features using increasingly large spatio-temporal
windows. The central concept is to use the rich temporal information provided
by events to create contexts in the form of time-surfaces which represent the
recent temporal activity within a local spatial neighborhood. We demonstrate
that this concept can robustly be used at all stages of an event-based
hierarchical model. First layer feature units operate on groups of pixels,
while subsequent layer feature units operate on the output of lower level
feature units. We report results on a previously published 36 class character
recognition task and a four class canonical dynamic card pip task, achieving
near 100 percent accuracy on each. We introduce a new seven class moving face
recognition task, achieving 79 percent accuracy.},
keywords={computer vision;face recognition;feature extraction;image
sensors;HOTS;asynchronously acquired dynamics;biologically inspired vision
sensors;canonical dynamic card pip task;cortical structures;event-based spatio-
temporal features;frameless asynchronous event-driven vision
sensors;hierarchical event-based pattern recognition architecture;hierarchy of
event-based time-surfaces;seven class moving face recognition task;time-
surfaces;visual scene;Biosensors;Cameras;Character recognition;Feature
extraction;Object recognition;Visualization;Neuromorphic sensing;event-based
vision;feature extraction},
doi={10.1109/TPAMI.2016.2574707},
ISSN={0162-8828},
month={July},}
@ARTICLE{7505654,
author={G. J. Qi and W. Liu and C. Aggarwal and T. Huang},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Joint Intermodal and Intramodal Label Transfers for Extremely Rare or
Unseen Classes},
year={2017},
volume={39},
number={7},
pages={1360-1373},
abstract={In this paper, we present a label transfer model from texts to images
for image classification tasks. The problem of image classification is often
much more challenging than text classification. On one hand, labeled text data
is more widely available than the labeled images for classification tasks. On
the other hand, text data tends to have natural semantic interpretability, and
they are often more directly related to class labels. On the contrary, the
image features are not directly related to concepts inherent in class labels.
One of our goals in this paper is to develop a model for revealing the
functional relationships between text and image features as to directly
transfer intermodal and intramodal labels to annotate the images. This is
implemented by learning a transfer function as a bridge to propagate the labels
between two multimodal spaces. However, the intermodal label transfers could be
undermined by blindly transferring the labels of noisy texts to annotate
images. To mitigate this problem, we present an intramodal label transfer
process, which complements the intermodal label transfer by transferring the
image labels instead when relevant text is absent from the source corpus. In
addition, we generalize the inter-modal label transfer to zero-shot learning
scenario where there are only text examples available to label unseen classes
of images without any positive image examples. We evaluate our algorithm on an
image classification task and show the effectiveness with respect to the other
compared algorithms.},
keywords={image classification;image denoising;image texture;learning
(artificial intelligence);annotate images;image classification problem;image
classification tasks;image features;intermodal label transfers;intramodal label
transfer process;intramodal label transfers;labeled text data;noisy text
labels;unseen classes;Bridges;Noise measurement;Semantics;Training;Transfer
functions;Videos;Visualization;Multimodal analysis;image
classification;intermodal and intramodal label transfers (I2LT);zero-shot
learning},
doi={10.1109/TPAMI.2016.2587643},
ISSN={0162-8828},
month={July},}
@ARTICLE{7539555,
author={D. Tang and H. J. Chang and A. Tejani and T. K. Kim},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Latent Regression Forest: Structured Estimation of 3D Hand Poses},
year={2017},
volume={39},
number={7},
pages={1374-1387},
abstract={In this paper we present the latent regression forest (LRF), a novel
framework for real-time, 3D hand pose estimation from a single depth image.
Prior discriminative methods often fall into two categories: holistic and
patch-based. Holistic methods are efficient but less flexible due to their
nearest neighbour nature. Patch-based methods can generalise to unseen samples
by consider local appearance only. However, they are complex because each pixel
need to be classified or regressed during testing. In contrast to these two
baselines, our method can be considered as a structured coarse-to-fine search,
starting from the centre of mass of a point cloud until locating all the
skeletal joints. The searching process is guided by a learnt latent tree model
which reflects the hierarchical topology of the hand. Our main contributions
can be summarised as follows: (i) Learning the topology of the hand in an
unsupervised, data-driven manner. (ii) A new forest-based, discriminative
framework for structured search in images, as well as an error regression step
to avoid error accumulation. (iii) A new multi-view hand pose dataset
containing 180 K annotated images from 10 different subjects. Our experiments
on two datasets show that the LRF outperforms baselines and prior arts in both
accuracy and efficiency.},
keywords={computer graphics;error analysis;image classification;learning
(artificial intelligence);pose estimation;regression analysis;topology;3D hand
pose structured estimation;LRF;annotated images;error regression
step;hierarchical topology;latent regression forest;multiview hand pose
dataset;patch-based methods;skeletal joints;structured coarse-to-fine
search;Pose estimation;Real-time systems;Regression tree analysis;Three-
dimensional displays;Topology;Training;Vegetation;3D;Random forest;depth;hand
pose estimation;latent tree model;regression forest},
doi={10.1109/TPAMI.2016.2599170},
ISSN={0162-8828},
month={July},}
@ARTICLE{7527684,
author={M. Lee and J. Cho and S. Oh},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Procrustean Normal Distribution for Non-Rigid Structure from Motion},
year={2017},
volume={39},
number={7},
pages={1388-1400},
abstract={A well-defined deformation model can be vital for non-rigid structure
from motion (NRSfM). Most existing methods restrict the deformation space by
assuming a fixed rank or smooth deformation, which are not exactly true in the
real world, and they require the degree of deformation to be predetermined,
which is impractical. Meanwhile, the errors in rotation estimation can have
severe effects on the performance, i.e., these errors can make a rigid motion
be misinterpreted as a deformation. In this paper, we propose an alternative to
resolve these issues, motivated by an observation that non-rigid deformations,
excluding rigid changes, can be concisely represented in a linear subspace
without imposing any strong constraints, such as smoothness or low-rank. This
observation is embedded in our new prior distribution, the Procrustean normal
distribution (PND), which is a shape distribution exclusively for non-rigid
deformations. Because of this unique characteristic of the PND, rigid and non-
rigid changes can be strictly separated, which leads to better performance. The
proposed algorithm, EM-PND, fits a PND to given 2D observations to solve NRSfM
without any user-determined parameters. The experimental results show that EM-
PND gives the state-of-the-art performance for the benchmark data sets,
confirming the adequacy of the new deformation model.},
keywords={computer vision;motion estimation;shape recognition;NRSfM;Procrustean
normal distribution;computer vision;deformation space;fixed rank;linear
subspace;non-rigid structure from motion;shape distribution;smooth
deformation;well defined deformation model;Deformable models;Discrete cosine
transforms;Gaussian distribution;Optimization;Shape;Three-dimensional
displays;Procrustean normal distribution;non-rigid structure from
motion;statistical shape model;structure from motion},
doi={10.1109/TPAMI.2016.2596720},
ISSN={0162-8828},
month={July},}
@ARTICLE{7533440,
author={W. Sun and W. J. Niessen and S. Klein},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Randomly Perturbed B-Splines for Nonrigid Image Registration},
year={2017},
volume={39},
number={7},
pages={1401-1413},
abstract={B-splines are commonly utilized to construct the transformation model
in free-form deformation (FFD) based registration. B-splines become smoother
with increasing spline order. However, a higher-order B-spline requires a
larger support region involving more control points, which means higher
computational cost. In general, the third-order B-spline is considered as a
good compromise between spline smoothness and computational cost. A lower-order
function is seldom used to construct the transformation model for registration
since it is less smooth. In this research, we investigated whether lower-order
B-spline functions can be utilized for more efficient registration, while
preserving smoothness of the deformation by using a novel random perturbation
technique. With the proposed perturbation technique, the expected value of the
cost function given probability density function (PDF) of the perturbation is
minimized by a stochastic gradient descent optimization. Extensive experiments
on 2D synthetically deformed brain images, and real 3D lung and brain scans
demonstrated that the novel randomly perturbed free-form deformation (RPFFD)
approach improves the registration accuracy and transformation smoothness.
Meanwhile, lower-order RPFFD methods reduce the computational cost
substantially.},
keywords={biomedical MRI;computerised tomography;gradient methods;image
registration;medical image processing;optimisation;probability;splines
(mathematics);FFD based registration;PDF;brain images;computational
cost;expected cost function value;free-form deformation based
registration;lower-order B-spline functions;lower-order RPFFD methods;nonrigid
image registration;perturbation technique;probability density function;randomly
perturbed B-splines;randomly perturbed free-form deformation;real 3D brain
scans;real 3D lung scans;spline order;spline smoothness;stochastic gradient
descent optimization;transformation model;Computational
efficiency;Computational modeling;Cost function;Image registration;Splines
(mathematics);Stochastic processes;B-splines;Nonrigid registration;free-form
deformation;perturbation;stochastic optimization;transformation},
doi={10.1109/TPAMI.2016.2598344},
ISSN={0162-8828},
month={July},}
@ARTICLE{7542175,
author={M. Ghifary and D. Balduzzi and W. B. Kleijn and M. Zhang},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Scatter Component Analysis: A Unified Framework for Domain Adaptation
and Domain Generalization},
year={2017},
volume={39},
number={7},
pages={1414-1430},
abstract={This paper addresses classification tasks on a particular target
domain in which labeled training data are only available from source domains
different from (but related to) the target. Two closely related frameworks,
domain adaptation and domain generalization, are concerned with such tasks,
where the only difference between those frameworks is the availability of the
unlabeled target data: domain adaptation can leverage unlabeled target
information, while domain generalization cannot. We propose Scatter Component
Analyis (SCA), a fast representation learning algorithm that can be applied to
both domain adaptation and domain generalization. SCA is based on a simple
geometrical measure, i.e., scatter, which operates on reproducing kernel
Hilbert space. SCA finds a representation that trades between maximizing the
separability of classes, minimizing the mismatch between domains, and
maximizing the separability of data; each of which is quantified through
scatter. The optimization problem of SCA can be reduced to a generalized
eigenvalue problem, which results in a fast and exact solution. Comprehensive
experiments on benchmark cross-domain object recognition datasets verify that
SCA performs much faster than several state-of-the-art algorithms and also
provides state-of-the-art classification accuracy in both domain adaptation and
domain generalization. We also show that scatter can be used to establish a
theoretical generalization bound in the case of domain adaptation.},
keywords={Hilbert spaces;eigenvalues and eigenfunctions;learning (artificial
intelligence);pattern classification;SCA;classification accuracy;domain
adaptation;domain generalization;generalized eigenvalue problem;kernel Hilbert
space;object recognition;representation learning algorithm;scatter component
analyis;Algorithm design and analysis;Kernel;Object
recognition;Optimization;Standards;Training;Visualization;Domain
adaptation;domain generalization;feature learning;kernel methods;object
recognition;scatter},
doi={10.1109/TPAMI.2016.2599532},
ISSN={0162-8828},
month={July},}
@ARTICLE{7516703,
author={T. Hassner and S. Filosof and V. Mayzels and L. Zelnik-Manor},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={SIFTing Through Scales},
year={2017},
volume={39},
number={7},
pages={1431-1443},
abstract={Scale invariant feature detectors often find stable scales in only a
few image pixels. Consequently, methods for feature matching typically choose
one of two extreme options: matching a sparse set of scale invariant features,
or dense matching using arbitrary scales. In this paper, we turn our attention
to the overwhelming majority of pixels, those where stable scales are not found
by standard techniques. We ask, is scale-selection necessary for these pixels,
when dense, scale-invariant matching is required and if so, how can it be
achieved? We make the following contributions: (i) We show that features
computed over different scales, even in low-contrast areas, can be different
and selecting a single scale, arbitrarily or otherwise, may lead to poor
matches when the images have different scales. (ii) We show that representing
each pixel as a set of SIFTs, extracted at multiple scales, allows for far
better matches than single-scale descriptors, but at a computational price.
Finally, (iii) we demonstrate that each such set may be accurately represented
by a low-dimensional, linear subspace. A subspace-to-point mapping may further
be used to produce a novel descriptor representation, the Scale-Less SIFT
(SLS), as an alternative to single-scale descriptors. These claims are verified
by quantitative and qualitative tests, demonstrating significant improvements
over existing methods. A preliminary version of this work appeared in [1] .},
keywords={feature extraction;image matching;transforms;SIFTing through
scales;computational price;dense matching;feature matching;image pixels;novel
descriptor representation;scale invariant feature detectors;scale-invariant
matching;scale-selection;single scale descriptors;standard
techniques;Detectors;Estimation;Feature extraction;Laplace equations;Optical
imaging;Optimization;Robustness;Vision and scene understanding;and
transforms;data structures;representations},
doi={10.1109/TPAMI.2016.2592916},
ISSN={0162-8828},
month={July},}
@ARTICLE{7516658,
author={M. Cordts and T. Rehfeld and M. Enzweiler and U. Franke and S. Roth},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Tree-Structured Models for Efficient Multi-Cue Scene Labeling},
year={2017},
volume={39},
number={7},
pages={1444-1454},
abstract={We propose a novel approach to semantic scene labeling in urban
scenarios, which aims to combine excellent recognition performance with highest
levels of computational efficiency. To that end, we exploit efficient tree-
structured models on two levels: pixels and superpixels. At the pixel level, we
propose to unify pixel labeling and the extraction of semantic texton features
within a single architecture, so-called encode-and-classify trees. At the
superpixel level, we put forward a multi-cue segmentation tree that groups
superpixels at multiple granularities. Through learning, the segmentation tree
effectively exploits and aggregates a wide range of complementary information
present in the data. A tree-structured CRF is then used to jointly infer the
labels of all regions across the tree. Finally, we introduce a novel object-
centric evaluation method that specifically addresses the urban setting with
its strongly varying object scales. Our experiments demonstrate competitive
labeling performance compared to the state of the art, while achieving near
real-time frame rates of up to 20 fps.},
keywords={feature extraction;image classification;image segmentation;learning
(artificial intelligence);trees (mathematics);conditional random field;encode-
and-classify trees;learning;multicue scene labeling;multicue segmentation
tree;pixel labeling;semantic scene labeling;semantic texton feature
extraction;superpixel level;tree-structured CRF;tree-structured
models;Detectors;Feature
extraction;Histograms;Labeling;Proposals;Semantics;Vegetation;Scene
labeling;automotive;decision forests;depth cues;segmentation
tree;stixels;superpixels},
doi={10.1109/TPAMI.2016.2592911},
ISSN={0162-8828},
month={July},}
@ARTICLE{7534854,
author={L. Svärm and O. Enqvist and F. Kahl and M. Oskarsson},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={City-Scale Localization for Cameras with Known Vertical Direction},
year={2017},
volume={39},
number={7},
pages={1455-1461},
abstract={We consider the problem of localizing a novel image in a large 3D
model, given that the gravitational vector is known. In principle, this is just
an instance of camera pose estimation, but the scale of the problem introduces
some interesting challenges. Most importantly, it makes the correspondence
problem very difficult so there will often be a significant number of outliers
to handle. To tackle this problem, we use recent theoretical as well as
technical advances. Many modern cameras and phones have gravitational sensors
that allow us to reduce the search space. Further, there are new techniques to
efficiently and reliably deal with extreme rates of outliers. We extend these
methods to camera pose estimation by using accurate approximations and fast
polynomial solvers. Experimental results are given demonstrating that it is
possible to reliably estimate the camera pose despite cases with more than 99
percent outlier correspondences in city-scale models with several millions of
3D points.},
keywords={image sensors;pose estimation;search problems;camera pose
estimation;cameras;city scale localization;gravitational sensors;gravitational
vector;known vertical direction;polynomial solvers;search
space;Cameras;Computational modeling;Pose estimation;Robustness;Solid
modeling;Three-dimensional displays;Localization;camera pose;position
retrieval},
doi={10.1109/TPAMI.2016.2598331},
ISSN={0162-8828},
month={July},}
@ARTICLE{7534869,
author={X. Liang and Y. Wei and L. Lin and Y. Chen and X. Shen and J. Yang and
S. Yan},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Learning to Segment Human by Watching YouTube},
year={2017},
volume={39},
number={7},
pages={1462-1468},
abstract={An intuition on human segmentation is that when a human is moving in
a video, the video-context (e.g., appearance and motion clues) may potentially
infer reasonable mask information for the whole human body. Inspired by this,
based on popular deep convolutional neural networks (CNN), we explore a very-
weakly supervised learning framework for human segmentation task, where only an
imperfect human detector is available along with massive weakly-labeled YouTube
videos. In our solution, the video-context guided human mask inference and CNN
based segmentation network learning iterate to mutually enhance each other
until no further improvement gains. In the first step, each video is decomposed
into supervoxels by the unsupervised video segmentation. The superpixels within
the supervoxels are then classified as human or non-human by graph optimization
with unary energies from the imperfect human detection results and the
predicted confidence maps by the CNN trained in the previous iteration. In the
second step, the video-context derived human masks are used as direct labels to
train CNN. Extensive experiments on the challenging PASCAL VOC 2012 semantic
segmentation benchmark demonstrate that the proposed framework has already
achieved superior results than all previous weakly-supervised methods with
object class or bounding box annotations. In addition, by augmenting with the
annotated masks from PASCAL VOC 2012, our method reaches a new state-of-the-art
performance on the human segmentation task.},
keywords={graph theory;image segmentation;neural nets;optimisation;social
networking (online);CNN;Watching YouTube;YouTube videos;deep convolutional
neural networks;graph optimization;human mask inference;human
segmentation;reasonable mask information;segment human;supervised learning
framework;video context;video-context;Detectors;Image segmentation;Motion
segmentation;Optimization;Semantics;Training;YouTube;Human
segmentation;convolutional neural network;incremental learning;weakly-
supervised learning},
doi={10.1109/TPAMI.2016.2598340},
ISSN={0162-8828},
month={July},}
@ARTICLE{7494684,
author={D. Průša and T. Werner},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={LP Relaxation of the Potts Labeling Problem Is as Hard as Any Linear
Program},
year={2017},
volume={39},
number={7},
pages={1469-1475},
abstract={In our recent work, we showed that solving the LP relaxation of the
pairwise min-sum labeling problem (also known as MAP inference in graphical
models or discrete energy minimization) is not much easier than solving any
linear program. Precisely, the general linear program reduces in linear time
(assuming the Turing model of computation) to the LP relaxation of the min-sum
labeling problem. The reduction is possible, though in quadratic time, even to
the min-sum labeling problem with planar structure. Here we prove similar
results for the pairwise min-sum labeling problem with attractive Potts
interactions (also known as the uniform metric labeling problem).},
keywords={graph theory;linear programming;LP relaxation;MAP inference;Turing
model;attractive Potts interactions;discrete energy minimization;graphical
models;linear program;min-sum labeling problem;pairwise min-sum labeling
problem;planar structure;potts labeling problem;uniform metric labeling
problem;Approximation algorithms;Computational modeling;Cost function;Graphical
models;Labeling;Measurement;Minimization;MAP inference;Markov random
field;Potts model;discrete energy minimization;graphical model;linear
programming relaxation;uniform metric labeling problem;valued constraint
satisfaction},
doi={10.1109/TPAMI.2016.2582165},
ISSN={0162-8828},
month={July},}
@ARTICLE{7546875,
author={S. Ren and K. He and R. Girshick and X. Zhang and J. Sun},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Object Detection Networks on Convolutional Feature Maps},
year={2017},
volume={39},
number={7},
pages={1476-1481},
abstract={Most object detectors contain two important components: a feature
extractor and an object classifier. The feature extractor has rapidly evolved
with significant research efforts leading to better deep convolutional
architectures. The object classifier, however, has not received much attention
and many recent systems (like SPPnet and Fast/Faster R-CNN) use simple multi-
layer perceptrons. This paper demonstrates that carefully designing deep
networks for object classification is just as important. We experiment with
region-wise classifier networks that use shared, region-independent
convolutional features. We call them “Networks on Convolutional feature maps”
(NoCs). We discover that aside from deep feature maps, a deep and convolutional
per-region classifier is of particular importance for object detection, whereas
latest superior image classification models (such as ResNets and GoogLeNets) do
not directly lead to good detection accuracy without using such a per-region
classifier. We show by experiments that despite the effective ResNets and
Faster R-CNN systems, the design of NoCs is an essential element for the 1st-
place winning entries in ImageNet and MS COCO challenges 2015.},
keywords={feature extraction;image classification;multilayer perceptrons;object
detection;Networks on Convolutional feature maps;NoCs;convolutional feature
maps;deep convolutional architectures;feature extractor;image classification
models;object classification;object classifier;object detection
networks;region-wise classifier networks;simple multilayer
perceptrons;Detectors;Electronic mail;Feature extraction;Object
detection;Proposals;Support vector machines;Training;CNN;Object
detection;convolutional feature map},
doi={10.1109/TPAMI.2016.2601099},
ISSN={0162-8828},
month={July},}
@ARTICLE{7494650,
author={G. D. Finlayson and R. Zakizadeh and A. Gijsenij},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={The Reproduction Angular Error for Evaluating the Performance of
Illuminant Estimation Algorithms},
year={2017},
volume={39},
number={7},
pages={1482-1488},
abstract={The angle between the RGBs of the measured illuminant and estimated
illuminant colors-the recovery angular error-has been used to evaluate the
performance of the illuminant estimation algorithms. However we noticed that
this metric is not in line with how the illuminant estimates are used.
Normally, the illuminant estimates are `divided out' from the image to,
hopefully, provide image colors that are not confounded by the color of the
light. However, even though the same reproduction results the same scene might
have a large range of recovery errors. In this work the scale of the problem
with the recovery error is quantified. Next we propose a new metric for
evaluating illuminant estimation algorithms, called the reproduction angular
error, which is defined as the angle between the RGB of a white surface when
the actual and estimated illuminations are `divided out'. Our new metric ties
algorithm performance to how the illuminant estimates are used. For a given
algorithm, adopting the new reproduction angular error leads to different
optimal parameters. Further the ranked list of best to worst algorithms changes
when the reproduction angular is used. The importance of using an appropriate
performance metric is established.},
keywords={image colour analysis;lighting;illuminant estimation algorithms;image
colors;reproduction angular error;white surface RGB;Algorithm design and
analysis;Estimation error;Image color analysis;Lighting;Measurement
uncertainty;Illuminant estimation;color constancy;error metric;performance
evaluation},
doi={10.1109/TPAMI.2016.2582171},
ISSN={0162-8828},
month={July},}
