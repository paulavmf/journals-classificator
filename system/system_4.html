<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"></head><body>@ARTICLE{7883944,
<br>
author={K. Huang and T. Tan and S. Maybank and R. Chellappa and J. Aggarval},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={Guest Editorial Introduction to the Special Issue on 
Large-Scale Video Analytics for Enhanced Security: Algorithms and 
Systems}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={589-592},
<br>  abstract={Due to the rapid increase of the number of cameras used 
in the video surveillance and the huge needs of the smart city and 
public security, video surveillance by human beings is no longer 
suitable. Hence, since the end of the last century, video analytics for 
security or visual surveillance has become one of the hottest research 
topics. Wide-area video surveillance systems can have extremely high 
data rates and high data volumes. Therefore, the challenge of video 
analytics is to extract meaningful information efficiently from the huge
 flow of video data in order to produce high-level semantic descriptions
 of the activities occurring in the area under surveillance.},
<br>  keywords={Algorithm design and analysis;Security;Special issues and sections;Surveillance;Visual analytics},
<br>  doi={10.1109/TSMC.2017.2679694},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>@ARTICLE{7428948,
<br>
author={C. T. Fan and Y. K. Wang and C. R. Huang},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={Heterogeneous Information Fusion and Visualization for a Large-Scale Intelligent Video Surveillance System}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={593-604},
<br>  abstract={Wide-area monitoring for a smart community can be 
challenging in systems engineering because of its large scale and 
heterogeneity at the sensor, algorithm, and visualization levels. A 
smart interface to visualize high-level information fused from a 
diversity of low-level surveillance data, and to facilitate rapid 
response of events, is critical for the design of the system. This paper
 presents an event-driven visualization mechanism fusing multimodal 
information for a large-scale intelligent video surveillance system. The
 mechanism proactively helps security personnel intuitively be aware of 
events through close cooperation among visualization, data fusion, and 
sensor tasking. The visualization not only displays 2-D, 3-D, and 
geographical information within a condensed form of interface but also 
automatically shows the only important video streams corresponding to 
spontaneous alerts and events by a decision process called display 
switching arbitration. The display switching arbitration decides the 
importance of cameras by score ranking that considers event urgency and 
semantic object features. This system has been successfully deployed in a
 campus to demonstrate its usability and efficiency for an installation 
with two camera clusters that include dozens of cameras, and with a lot 
of video analytics to detect alerts and events. A further simulation 
comparing the display switching arbitration with similar camera 
selection methods shows that our method improves the visualization by 
selecting better representative camera views and reducing redundant 
switchover among multiview videos.},
<br>  keywords={data visualisation;sensor fusion;video cameras;video 
streaming;video surveillance;camera selection;data fusion;decision 
process;display switching arbitration;event urgency;event-driven 
visualization;heterogeneous information fusion;high-level information 
visualization;intelligent video surveillance system;multimodal 
information;multiview videos;score ranking;semantic object 
features;sensor tasking;smart community;smart interface;systems 
engineering;video analytics;video streams;Artificial 
intelligence;Cameras;Data visualization;Streaming 
media;Surveillance;Switches;Visualization;Display switching 
arbitration;information fusion;third-generation surveillance system 
(3GSS);visual surveillance;visualizability;visualization},
<br>  doi={10.1109/TSMC.2016.2531671},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>@ARTICLE{7707385,
<br>
author={R. Ding and M. Yu and H. Oh and W. H. Chen},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={New Multiple-Target Tracking Strategy Using Domain Knowledge and Optimization}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={605-616},
<br>  abstract={This paper proposes an environment-dependent vehicle 
dynamic modeling approach considering interactions between the noisy 
control input of a dynamic model and the environment in order to make 
best use of domain knowledge. Based on this modeling, a new domain 
knowledge-aided moving horizon estimation (DMHE) method is proposed for 
ground moving target tracking. The proposed method incorporates 
different types of domain knowledge in the estimation process 
considering both environmental physical constraints and interaction 
behaviors between targets and the environment. Furthermore, in order to 
deal with a data association ambiguity problem of multiple-target 
tracking in a cluttered environment, the DMHE is combined with a 
multiple-hypothesis tracking structure. Numerical simulation results 
show that the proposed DMHE-based method and its extension could achieve
 better performance than traditional tracking methods which utilize no 
domain knowledge or simple physical constraint information only.},
<br>  keywords={sensor fusion;target tracking;DMHE method;constraint 
information;data association ambiguity problem;domain knowledge-aided 
moving horizon estimation;environment-dependent vehicle dynamic modeling
 approach;multiple-hypothesis tracking structure;multiple-target 
tracking strategy;numerical 
simulation;Estimation;Force;Optimization;Roads;Target tracking;Vehicle 
dynamics;Vehicles;Domain knowledge;force-based model;moving horizon 
estimation (MHE);multiple-hypothesis tracking (MHT);multiple-target 
tracking (MTT)},
<br>  doi={10.1109/TSMC.2016.2615188},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>@ARTICLE{7707436,
<br>
author={Y. Guo and D. Tao and W. Liu and J. Cheng},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={Multiview Cauchy Estimator Feature Embedding for Depth and Inertial Sensor-Based Human Action Recognition}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={617-627},
<br>  abstract={The ever-growing popularity of Kinect and inertial 
sensors has prompted intensive research efforts on human action 
recognition. Since human actions were extracted from Kinect and inertial
 sensors, they can be characterized by multiple feature representations.
 By encoding the multiview features into a unified space, it could be 
optimal for human action recognition. In this paper, we propose a new 
unsupervised feature fusion method termed multiview Cauchy estimator 
feature embedding (MCEFE) for human action recognition. By minimizing 
empirical risk, MCEFE integrates the encoded complementary information 
in multiple views to find the unified data representation and the 
projection matrices. To enhance robustness to outliers, the Cauchy 
estimator is imposed on the reconstruction error. Furthermore, ensemble 
manifold regularization is enforced on the projection matrices to encode
 the correlations between different views and avoid overfitting. 
Experiments are conducted on the new Chinese Academy of Sciences-Yunnan 
University-multimodal human action database to demonstrate the 
effectiveness and robustness of MCEFE for human action recognition.},
<br>  keywords={data structures;database management systems;image 
fusion;image recognition;matrix algebra;minimisation;Chinese Academy of 
Sciences;Kinect sensors;MCEFE;Yunnan University;depth-based human action
 recognition;empirical risk minimization;inertial sensor-based human 
action recognition;multimodal human action database;multiview Cauchy 
estimator feature embedding;projection matrices;unified data 
representation;unsupervised feature fusion 
method;Cameras;Databases;Feature extraction;Robustness;Sensor 
fusion;Sensor phenomena and characterization;Computer 
interfaces;parameter estimation;pattern recognition},
<br>  doi={10.1109/TSMC.2016.2617465},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>@ARTICLE{7736115,
<br>
author={X. Cheng and Y. Zhang and J. Cui and L. Zhou},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={Object Tracking via Temporal Consistency Dictionary Learning}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={628-638},
<br>  abstract={Sparse representation-based methods have been 
successfully applied to visual tracking. However, complex and 
inefficient optimization limits their deployment in practical tracking 
scenarios. In this paper, we propose a temporal consistency dictionary 
learning tracking algorithm to enable efficient dictionary learning and 
tracking executive. First, we present an objective function which 
introduces the fixed dictionary and variance dictionary to reconstruct 
the object's appearance. In particular, the proposed method takes the 
temporal consistency into account by adding a regularization term into 
the objective function to constrain the difference of object appearance 
at adjacent frames. Then the optimization problem is solved in an 
iteration way. Moreover, the proposed method can encode the object's 
local structural information, and the local patches from the same 
candidate altogether for a global appearance representation. Second, we 
develop an effective observation likelihood function based on the 
proposed model. It takes the influence of patches with large 
reconstruction errors into consideration, thereby, alleviating the 
drifting of the object. Finally, we present an appearance updating 
strategy to adapt to the object's appearance variations by the online 
dictionary learning. Experimental evaluations on the TB50 and TB100 
datasets show that the proposed tracking method outperforms sparse 
representation related visual tracking as well as other state-of-the-art
 tracking methods.},
<br>  keywords={image representation;iterative methods;object 
tracking;optimisation;TB100 datasets;TB50 datasets;fixed 
dictionary;global appearance representation;iteration method;object 
tracking;observation likelihood function;optimization 
problem;regularization term;sparse representation-based methods;temporal
 consistency dictionary learning tracking algorithm;variance 
dictionary;visual tracking;Dictionaries;Linear programming;Object 
tracking;Optimization;Radar tracking;Visualization;Dictionary 
learning;object tracking;sparse representation;surveillance;temporal 
consistency},
<br>  doi={10.1109/TSMC.2016.2618749},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>@ARTICLE{7728082,
<br>
author={K. Chen and Z. Zhang},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={Pedestrian Counting With Back-Propagated Information and Target Drift Remedy}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={639-647},
<br>  abstract={Pedestrian density is one of the important factors in 
designing visual surveillance and intelligent transportation systems, 
but it is challenging to obtain accurate and robust estimates because of
 both inconsistent crowd patterns in the scenes and target drift caused 
by imbalanced data distribution. Most of existing global regression 
frameworks focus on the former challenge to improve the robustness of 
regression learning, but very few work concerns on mitigating the 
suffering from the latter one. This paper proposes a novel 
counting-by-regression framework to utilize the importance of training 
samples to improve the robustness against inconsistent feature-target 
relationship based on a recently-proposed learning paradigm-learning 
with privileged information. To this end, the concept of 
back-propagation is for the first time considered to select more 
informative samples contributed to robust fitting performance. Moreover,
 the direction of target drift along the continuously-changing target 
dimension is discovered by learning local classifiers under different 
situation of pedestrian density, which can thus be exploited in our 
algorithm to further boost the performance. Experimental evaluation on 
the public UCSD and shopping Mall benchmarks verifies that our approach 
significantly beats the state-of-the-art counting-by-regression 
frameworks.},
<br>  keywords={computer vision;image classification;intelligent 
transportation systems;learning (artificial 
intelligence);pedestrians;regression 
analysis;surveillance;back-propagated information;imbalanced data 
distribution;inconsistent crowd patterns;intelligent transportation 
systems;pedestrian counting;pedestrian density;public UCSD;regression 
learning robustness improvenent;robust fitting performance;shopping mall
 benchmarks;target drift remedy;visual surveillance;Cybernetics;Feature 
extraction;Image edge 
detection;Robustness;Surveillance;Training;Visualization;Back-propagated;cumulative
 attributes;pedestrian counting;regression learning;visual 
surveillance},
<br>  doi={10.1109/TSMC.2016.2618916},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>@ARTICLE{7790875,
<br>
author={Y. Li and Y. Guo and Y. Kao and R. He},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={Image Piece Learning for Weakly Supervised Semantic Segmentation}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={648-659},
<br>  abstract={The task of semantic segmentation is to infer a 
predefined category label for each pixel in the image. For most cases, 
image segmentation is established as a fully supervised task. These 
methods all built on the basis of having access to sufficient pixel-wise
 annotated samples for training. However, obtaining the satisfied ground
 truth is not only labor intensive but also time-consuming, which 
severely hinders the generality of these fully supervised methods. 
Instead of pixel-level ground truth, weakly supervised approaches learn 
their models from much less prior information, e.g., image-level 
annotation. In this paper, we propose a novel conditional random field 
(CRF) based framework for weakly supervised semantic segmentation. 
Enlightened by jigsaw puzzles, we start the approach with merging 
superpixels from an image into larger pieces by a newly designed 
strategy. Then pieces from all the training images are gathered and 
associated with appropriate semantic labels by CRF. Thus, the piece 
library is constructed, achieving remarkable universality and 
flexibility. In the case of testing, we compare the superpixels with 
image pieces in the library and assign them the labels that minimize the
 potential energy. In addition, the proposed framework is fit for domain
 adaption and obtains promising results, which is of great practical 
value. Extensive experimental results on PASCAL VOC 2007, MSRC-21, and 
VOC 2012 databases demonstrate that our framework outperforms or is 
comparable to state-of-the-art segmentation methods.},
<br>  keywords={graph theory;image annotation;image 
segmentation;inference mechanisms;learning (artificial 
intelligence);statistical analysis;CRF;category label 
inference;conditional random field;image piece learning;image-level 
annotation;weakly supervised semantic 
segmentation;Correlation;Databases;Image segmentation;Libraries;Pattern 
recognition;Semantics;Training;Conditional random field (CRF);image 
semantic segmentation;piece learning;weakly supervised},
<br>  doi={10.1109/TSMC.2016.2623683},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>@ARTICLE{7762206,
<br>
author={S. Zhang and C. Gao and F. Chen and S. Luo and N. Sang},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={Group Sparse-Based Mid-Level Representation for Action Recognition}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={660-672},
<br>  abstract={Mid-level parts are shown to be effective for human 
action recognition in videos. Typically, these semantic parts are first 
mined with some heuristic rules, then videos are represented via 
volumetric max-pooling (VMP) method. However, these methods have two 
issues: 1) the VMP strategy divides videos by static grids. In this 
case, a semantic part may occur in different localizations in different 
videos. That means the VMP strategy loses the space-time invariance. To 
solve this problem, we propose to apply a saliency-driven max-pooling 
scheme to represent a video. We extract the video semantic cues by the 
saliency map, and dynamically pool the local maximum responses. This 
scheme can be considered as a semantic content-based feature alignment 
method and 2) the parts discovered by heuristic rules may be intuitive 
but not discriminative enough for action classification because they 
neglect the relations between the detectors. For this issue, we propose 
to apply a sparse classifier model to select discriminative parts. 
Moreover, to further improve the discriminative ability of the 
representation, we propose to conduct feature selection by the 
corresponding entry magnitude of the model coefficients. We conduct 
experiments on four challenging datasets-KTH, Olympic Sports, UCF50, and
 HMDB51. The results show that the proposed method significantly 
outperforms the state-of-the-art methods.},
<br>  keywords={compressed sensing;feature extraction;feature 
selection;image classification;image motion analysis;image 
representation;video signal processing;VMP method;content-based feature 
alignment method;feature selection;group sparse-based midlevel 
representation;human action recognition;saliency map;sparse classifier 
model;video representation;video semantic cue extraction;video static 
grids;volumetric max-pooling method;Detectors;Feature 
extraction;Robustness;Semantics;Switched-mode power 
supply;Videos;Visualization;Group sparse (GS);human action 
recognition;mid-level part;saliency driven max-pooling (SMP);video 
representation},
<br>  doi={10.1109/TSMC.2016.2625840},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>@ARTICLE{7822984,
<br>
author={C. Li and X. Sun and X. Wang and L. Zhang and J. Tang},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={Grayscale-Thermal Object Tracking via Multitask Laplacian Sparse Representation}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={673-681},
<br>  abstract={This paper studies the problem of object tracking in 
challenging scenarios by leveraging multimodal visual data. We propose a
 grayscale-thermal object tracking method in Bayesian filtering 
framework based on multitask Laplacian sparse representation. Given one 
bounding box, we extract a set of overlapping local patches within it, 
and pursue the multitask joint sparse representation for grayscale and 
thermal modalities. Then, the representation coefficients of the two 
modalities are concatenated into a vector to represent the feature of 
the bounding box. Moreover, the similarity between each patch pair is 
deployed to refine their representation coefficients in the sparse 
representation, which can be formulated as the Laplacian sparse 
representation. We also incorporate the modal reliability into the 
Laplacian sparse representation to achieve an adaptive fusion of 
different source data. Experiments on two grayscale-thermal datasets 
suggest that the proposed approach outperforms both grayscale and 
grayscale-thermal tracking approaches.},
<br>  keywords={Bayes methods;feature extraction;image filtering;image 
representation;infrared imaging;object tracking;sensor 
fusion;vectors;Bayesian filtering framework;bounding box;different 
source data adaptive fusion;feature representation;grayscale-thermal 
object tracking;modal reliability;multimodal visual data;multitask 
Laplacian sparse representation;multitask joint sparse 
representation;overlapping local patch extraction;patch pair 
similarity;thermal modality;vector;Computational 
modeling;Dictionaries;Gray-scale;Laplace equations;Object 
tracking;Target tracking;Visualization;Adaptive fusion;Laplacian 
constraints;grayscale-thermal tracking;modal reliability;multitask 
learning;sparse representation},
<br>  doi={10.1109/TSMC.2016.2627052},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>@ARTICLE{7775025,
<br>
author={D. Mery and E. Svec and M. Arias and V. Riffo and J. M. Saavedra and S. Banerjee},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={Modern Computer Vision Techniques for X-Ray Testing in Baggage Inspection}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={682-692},
<br>  abstract={X-ray screening systems have been used to safeguard 
environments in which access control is of paramount importance. 
Security checkpoints have been placed at the entrances to many public 
places to detect prohibited items, such as handguns and explosives. 
Generally, human operators are in charge of these tasks as automated 
recognition in baggage inspection is still far from perfect. Research 
and development on X-ray testing is, however, exploring new approaches 
based on computer vision that can be used to aid human operators. This 
paper attempts to make a contribution to the field of object recognition
 in X-ray testing by evaluating different computer vision strategies 
that have been proposed in the last years. We tested ten approaches. 
They are based on bag of words, sparse representations, deep learning, 
and classic pattern recognition schemes among others. For each method, 
we: 1) present a brief explanation; 2) show experimental results on the 
same database; and 3) provide concluding remarks discussing pros and 
cons of each method. In order to make fair comparisons, we define a 
common experimental protocol based on training, validation, and testing 
data (selected from the public GDXray database). The effectiveness of 
each method was tested in the recognition of three different threat 
objects: 1) handguns; 2) shuriken (ninja stars); and 3) razor blades. In
 our experiments, the highest recognition rate was achieved by methods 
based on visual vocabularies and deep features with more than 95% of 
accuracy. We strongly believe that it is possible to design an automated
 aid for the human inspection task using these computer vision 
algorithms.},
<br>  keywords={X-ray imaging;authorisation;automatic optical 
inspection;bags;computer vision;object recognition;GDXray database;X-ray
 screening systems;X-ray testing;access control;automated 
recognition;bag of words;baggage inspection;computer vision;deep 
features;deep learning;human inspection task;object recognition;pattern 
recognition;security checkpoints;sparse representations;visual 
vocabularies;Computer vision;Image recognition;Inspection;Object 
recognition;Testing;Weapons;X-ray imaging;Baggage screening;X-ray 
testing;deep learning;implicit shape model (ISM);object 
categorization;object detection;object recognition;sparse 
representations;threat objects},
<br>  doi={10.1109/TSMC.2016.2628381},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>@ARTICLE{7776867,
<br>
author={B. Zhang and Z. Li and X. Cao and Q. Ye and C. Chen and L. Shen and A. Perina and R. Jill},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={Output Constraint Transfer for Kernelized Correlation Filter in Tracking}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={693-703},
<br>  abstract={The kernelized correlation filter (KCF) is one of the 
state-of-the-art object trackers. However, it does not reasonably model 
the distribution of correlation response during tracking process, which 
might cause the drifting problem, especially when targets undergo 
significant appearance changes due to occlusion, camera shaking, and/or 
deformation. In this paper, we propose an output constraint transfer 
(OCT) method that by modeling the distribution of correlation response 
in a Bayesian optimization framework is able to mitigate the drifting 
problem. OCT builds upon the reasonable assumption that the correlation 
response to the target image follows a Gaussian distribution, which we 
exploit to select training samples and reduce model uncertainty. OCT is 
rooted in a new theory which transfers data distribution to a constraint
 of the optimized variable, leading to an efficient framework to 
calculate correlation filters. Extensive experiments on a commonly used 
tracking benchmark show that the proposed method significantly improves 
KCF, and achieves better performance than other state-of-the-art 
trackers. To encourage further developments, the source code is made 
available.},
<br>  keywords={Bayes methods;Gaussian distribution;computer 
vision;learning (artificial intelligence);object 
tracking;optimisation;Bayesian optimization framework;Gaussian 
distribution;KCF;OCT method;camera shaking;correlation response 
distribution;data distribution;drifting problem mitigation;kernelized 
correlation filter;model uncertainty reduction;object deformation;object
 tracking;occlusion;output constraint transfer;Benchmark 
testing;Correlation;Gaussian distribution;Kernel;Object 
tracking;Robustness;Target tracking;Correlation filter;online 
learning;tracking},
<br>  doi={10.1109/TSMC.2016.2629509},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>@ARTICLE{7797261,
<br>
author={B. Yu and Y. Liu and Q. Sun},
<br>  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
<br> title={A Content-Adaptively Sparse Reconstruction Method for Abnormal Events Detection With Low-Rank Property}, 
<br>  year={2017},
<br>  volume={47},
<br>  number={4},
<br>  pages={704-716},
<br>  abstract={This paper presents a content-adaptively sparse 
reconstruction method for abnormal events detection by exploiting the 
low-rank property of video sequences. In dictionary learning phase, the 
bases which describe more important characteristics of the normal 
behavior patterns are assigned with lower reconstruction costs. Based on
 the low-rank property of the bases captured by the low-rank 
approximation, a weighted sparse reconstruction method is proposed to 
measure the abnormality of testing samples. Multiscale 3-D gradient 
features, which encode the spatiotemporal information, are adopted as 
the low level descriptors. The benefits of the proposed method are 
threefold: first, the low-rank property is utilized to learn the 
underlying normal dictionaries, which can represent groups of similar 
normal features effectively; second, the sparsity-based algorithm can 
adaptively determine the number of dictionary bases, which makes it a 
preferable choice for representing the dynamic scene semantics; and 
third, based on the weighted sparse reconstruction method, the proposed 
method is more efficient for detecting the abnormal events. Experimental
 results on the public datasets have shown that the proposed method 
yields competitive performance comparing with the state-of-the-art 
methods.},
<br>  keywords={approximation theory;stereo image processing;video 
coding;abnormal events detection;behavior patterns;content adaptively 
sparse reconstruction;dictionary learning;low-rank 
approximation;low-rank property;multiscale 3D gradient features;sparse 
coding;video sequences;weighted sparse reconstruction;Dictionaries;Event
 detection;Feature extraction;Hidden Markov models;Reconstruction 
algorithms;Spatiotemporal phenomena;Trajectory;Abnormal event 
detection;behavior patterns;low-rank;sparse coding;video analysis},
<br>  doi={10.1109/TSMC.2016.2638048},
<br>  ISSN={2168-2216},
<br>  month={April},}<br>

</body></html>