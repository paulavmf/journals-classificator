@ARTICLE{7035102,
author={K. Alam and T. Ray and S. G. Anavatti},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Design Optimization of an Unmanned Underwater Vehicle Using Low- and
High-Fidelity Models},
year={2017},
volume={47},
number={11},
pages={2794-2808},
abstract={Design optimization of an unmanned underwater vehicle (UUV) is a
complex and a computationally expensive exercise that requires the
identification of optimal vehicle dimensions offering the best tradeoffs
between the objectives, while satisfying the set of design constraints.
Although hull form optimization of marine vessels has long been an active area
of research, limited attempts in the past have focused on the design
optimization of UUVs and there are even fewer reports on the use of high-
fidelity analysis methods within the course of optimization. While it is
understood that the high-fidelity analysis is more accurate, they also tend to
be far more computationally expensive. Thus, it is important to identify when a
high-fidelity analysis is required as opposed to a low-fidelity estimate. The
work reported in this paper is an extension of the authors previous work of a
design optimization framework, where the design problem was solved using a low-
fidelity model based on empirical estimates of drag. In this paper, the
framework is extended to deal with high-fidelity estimates derived through
seamless integration of computer-aided design, meshing and computational fluid
dynamics analysis tools i.e., computer aided 3-D interactive application, ICEM,
and FLUENT. The effects of using low-fidelity and high-fidelity analyses are
studied in depth using a small-scale (length nominally less than 400 mm) and
light-weight (less than 450 g) toy submarine. Useful insights on possible means
to identify appropriateness of fidelity models via correlation measures are
proposed. The term optimality used in this paper refers to optimal hull form
shapes that satisfy placement of a set of prescribed internal components.},
keywords={computational fluid dynamics;optimisation;remotely operated
vehicles;ships;underwater vehicles;computational fluid dynamic
analysis;computationally expensive exercise;computer aided 3D interactive
application;high-fidelity analysis methods;high-fidelity models;low-fidelity
model;low-fidelity models;optimal hull form shapes;optimal vehicle
dimensions;unmanned underwater vehicle;Computational fluid
dynamics;Computational modeling;Geometry;Mathematical
model;Optimization;Underwater vehicles;Vehicles;Computational fluid dynamics
(CFD);multidisciplinary design;optimization;unmanned underwater vehicle (UUV)},

doi={10.1109/TSMC.2015.2390592},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7398159,
author={J. Zhou and Q. Wang and S. B. Tsai and Y. Xue and W. Dong},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={How to Evaluate the Job Satisfaction of Development Personnel},
year={2017},
volume={47},
number={11},
pages={2809-2816},
abstract={The high-technology industry is capital-, technology-, and knowledge-
intensive and considerably emphasizes the speed of innovations. Continual
updates and upgrades to technology are required for maintaining
competitiveness. Research and development (R&D) personnel are core personnel in
high-technology industries, but they are insufficient in number. Reducing R&D
personnel turnover has become a key topic in the human resource management of
innovative enterprises. Numerous studies have verified that low job
satisfaction weakens employee morale and organizational commitment, leading to
a high turnover rate. To examine the assessment criteria for the job
satisfaction of R&D personnel in high-technology industries, fuzzy theory and
decision making trial and evaluation laboratory (DEMATEL) were combined into a
novel fuzzy-DEMATEL model. First, fuzzy theory was used to examine job
satisfaction criteria and fuzzy semantic analysis. Second, the fuzzy-DEMATEL
model was used to calculate causal relationships and the degree of influence
among all of the criteria. Finally, a model for assessing the job satisfaction
of R&D personnel in high-technology industries was developed. The findings of
this paper are theoretically innovative and practically applicable to the high-
technology industry.},
keywords={decision making;fuzzy set theory;human resource management;innovation
management;organisational aspects;personnel;R&D;capital-intensive industry;core
personnel;decision making trial-and-evaluation laboratory;development
personnel;employee morale;fuzzy semantic analysis;fuzzy theory;fuzzy-DEMATEL
model;high turnover rate;high-technology industry;human resource
management;innovative enterprises;job satisfaction criteria;knowledge-intensive
industry;organizational commitment;research and development
personnel;Industries;Investment;Organizations;Personnel;Semantics;Technological
innovation;Fuzzy model;fuzzy–DEMATEL model;high-technology industry;job
satisfaction;satisfaction evaluation},
doi={10.1109/TSMC.2016.2519860},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7414487,
author={Y. Zhang and Y. Zhang and D. Chen and Z. Xiao and X. Yan},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={From Davidenko Method to Zhang Dynamics for Nonlinear Equation Systems
Solving},
year={2017},
volume={47},
number={11},
pages={2817-2830},
abstract={The solving of nonlinear equation systems (e.g., complex
transcendental dispersion equation systems in waveguide systems) is a
fundamental topic in science and engineering. Davidenko method has been used by
electromagnetism researchers to solve time-invariant nonlinear equation systems
(e.g., the aforementioned transcendental dispersion equation systems).
Meanwhile, Zhang dynamics (ZD), which is a special class of neural dynamics,
has been substantiated as an effective and accurate method for solving
nonlinear equation systems, particularly time-varying nonlinear equation
systems. In this paper, Davidenko method is compared with ZD in terms of
efficiency and accuracy in solving time-invariant and time-varying nonlinear
equation systems. Results reveal that ZD is a more competent approach than
Davidenko method. Moreover, discrete-time ZD models, corresponding block
diagrams, and circuit schematics are presented to facilitate the convenient
implementation of ZD by researchers and engineers for solving time-invariant
and time-varying nonlinear equation systems online. The theoretical analysis
and results on Davidenko method, ZD, and discrete-time ZD models are also
discussed in relation to solving time-varying nonlinear equation systems.},
keywords={nonlinear equations;time-varying systems;Zhang dynamics;block
diagrams;circuit schematics;complex transcendental dispersion equation
systems;davidenko method;discrete-time ZD models;neural dynamics;nonlinear
equation systems solving;time-invariant nonlinear equation systems;time-varying
nonlinear equation systems;waveguide
systems;Convergence;Dispersion;Electromagnetics;Mathematical model;Newton
method;Nonlinear equations;Time-varying systems;Comparison;Davidenko
method;Zhang dynamics (ZD);time-invariant nonlinear equation systems;time-
varying nonlinear equation systems},
doi={10.1109/TSMC.2016.2523917},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7419919,
author={J. Zhang and Q. Wei and G. Liu and W. Tang},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={A Supplier Switching Model With the Competitive Reactions and Economies
of Scale Effects},
year={2017},
volume={47},
number={11},
pages={2831-2843},
abstract={To minimize costs, a buying firm would seek sources which offer a
more affordable price for the required products. On the basis of a principal-
agent framework, this paper presents a buyer's supplier switching model under
asymmetric information to minimize the buying cost considering the volume-
dependent switching cost, the competitive reactions and economies of scale
effects of the incumbent supplier. The proposed model is first converted into
an optimal control problem. Then the optimal supplier switching strategy and
the corresponding transfer payment are obtained by virtue of Pontryagin's
maximum principle. It is shown that the switching cost components and
competitive reactions have significant impacts on the switching decision. Only
if the maximum price discount is greater than the fixed component of the
switching cost, there may exist a partial switching strategy for the buyer to
benefit from the competitive effects. Otherwise, the buyer should take an all-
or-nothing switching strategy or no switching strategy. Some managerial
implications for sourcing strategies with respect to the competitive reactions
and economies of scale effects are provided. Furthermore, we propose a revenue
sharing contract to highlight the advantage of the contract designed based on
the principle-agent theory. Finally, we employ numerical examples to account
for the proposed methods.},
keywords={contracts;cost reduction;decision making;economies of scale;maximum
principle;optimal control;pricing;supply chain management;Pontryagin maximum
principle;affordable price;all-or-nothing switching strategy;buyer;buying
cost;buying firm;competitive effects;competitive reactions;economies-of-scale
effects;incumbent supplier;maximum price discount;no-switching strategy;optimal
control problem;optimal supplier switching strategy;partial switching
strategy;principal-agent framework;principle-agent theory;revenue sharing
contract;supplier switching model;switching cost components;switching
decision;volume-dependent switching cost;Contracts;Convex functions;Economies
of scale;Integrated circuits;Investment;Optimal control;Switches;Asymmetric
information;Pontryagin’s maximum principle;principal-agent;revenue sharing
contract;supplier switching},
doi={10.1109/TSMC.2016.2523926},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7419884,
author={Y. Hou and M. Sallak and W. Schön},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Estimation of Imprecise Reliability of Systems Using Random Sets and
Monte Carlo Resampling Procedures},
year={2017},
volume={47},
number={11},
pages={2844-2855},
abstract={This paper is divided into three parts. First, it introduces the use
of random sets for reliability assessment of components with rare failure
events. The proposed approach is based on the use of operations defined in the
random set framework (expectations, confidence intervals, etc.) to obtain upper
and lower bounds and confidence intervals of components reliability without
assuming any prior distribution about their lifetimes. Then, instead of using
failure probabilities calculated directly from each component's observation in
order to obtain system reliability, we propose to construct pseudo-system
observations directly from components observations in order to obtain the
interval system reliability. Finally, the proposed approach is applied on the
evaluation of reliability of large-scale systems with very large fault trees
and censored reliability data by using Monte Carlo resampling procedure. A
comparison with classical probabilistic approaches is also done.},
keywords={Monte Carlo methods;failure analysis;fault trees;large-scale
systems;machine components;probability;reliability;Monte Carlo resampling
procedure;censored reliability data;classical probabilistic approach;component
observations;component reliability;confidence intervals;failure
probabilities;fault trees;imprecise reliability estimation;interval system
reliability;large-scale systems;lower bounds;prior distribution;pseudosystem
observations;random set framework;rare failure events;upper
bounds;Estimation;Measurement uncertainty;Random variables;Reliability
theory;Set theory;Uncertainty;Confidence intervals;Monte Carlo
simulations;large systems;random set theory;rare failure
events;reliability;resampling},
doi={10.1109/TSMC.2016.2523928},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7429790,
author={I. Grobelna and R. Wiśniewski and M. Grobelny and M. Wiśniewska},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Design and Verification of Real-Life Processes With Application of Petri
Nets},
year={2017},
volume={47},
number={11},
pages={2856-2869},
abstract={This paper focuses on the design and verification methods of
distributed logic controllers supervising real-life processes. Such systems
have to be designed very carefully and precisely in order to operate flawlessly
and to meet user needs. We propose to use interpreted Petri nets as modeling
formalism. A new design flow of distributed logic controllers is introduced.
The methodology covers the development process from the specification stage to
the final implementation of the controller in the distributed devices. In the
proposed solution, the system is decomposed into separate modules that form a
distributed system. Furthermore, the specification (before and after the
decomposition process) is formally verified with the application of the model
checking technique against predefined behavioral requirements. Finally, the
system is implemented in real devices. The usage of formal methods and double
model checking ensure the correct functionality of the designed distributed
logic controller. The theoretical approach is supplemented by the practical
experiments. Furthermore, the proposed idea is illustrated by an example of a
smart home system.},
keywords={Petri nets;distributed processing;formal verification;Petri
nets;decomposition process;design flow;development process;distributed
devices;distributed logic controllers;distributed system;double model
checking;formal methods;modeling formalism;real-life processes;smart home
system;specification stage;verification methods;Control systems;Field
programmable gate arrays;Hardware;Model checking;Petri nets;Process
control;Synchronization;Decomposition;Petri net;field programmable gate array
(FPGA);formal verification;microcontroller;prototyping of distributed control
system},
doi={10.1109/TSMC.2016.2531673},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7433472,
author={X. Qu and D. Zhang and G. Lu and Z. Guo},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Door Knob Hand Recognition System},
year={2017},
volume={47},
number={11},
pages={2870-2881},
abstract={Biometric applications have been used globally in everyday life.
However, conventional biometrics is created and optimized for high-security
scenarios. Being used in daily life by ordinary untrained people is a new
challenge. Facing this challenge, designing a biometric system with prior
constraints of ergonomics, we propose ergonomic biometrics design model, which
attains the physiological factors, the psychological factors, and the
conventional security characteristics. With this model, a novel hand-based
biometric system, door knob hand recognition system (DKHRS), is proposed. DKHRS
has the identical appearance of a conventional door knob, which is an optimum
solution in both physiological factors and psychological factors. In this
system, a hand image is captured by door knob imaging scheme, which is a
tailored omnivision imaging structure and is optimized for this predetermined
door knob appearance. Then features are extracted by local Gabor binary pattern
histogram sequence method and classified by projective dictionary pair
learning. In the experiment on a large data set including 12 000 images from
200 people, the proposed system achieves competitive recognition performance
comparing with conventional biometrics like face and fingerprint recognition
systems, with an equal error rate of 0.091%. This paper shows that a biometric
system could be built with a reliable recognition performance under the
ergonomic constraints.},
keywords={Gabor filters;ergonomics;feature extraction;image
classification;image sequences;palmprint recognition;biometric
system;conventional security characteristics;door knob appearance;door knob
hand recognition system;door knob imaging scheme;ergonomic biometrics design
model;feature extraction;hand image;hand-based biometric system;local Gabor
binary pattern histogram sequence method;omnivision imaging structure;ordinary
untrained people;physiological factors;projective dictionary pair
learning;psychological factors;Biological system modeling;Ergonomics;Feature
extraction;Imaging;Iris recognition;Sensors;Biometrics;ergonomics;feature
extraction;image processing;machine learning;optical imaging;pattern
recognition;user-centered design},
doi={10.1109/TSMC.2016.2531675},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7429758,
author={W. Wang and A. H. Tan and L. N. Teow},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Semantic Memory Modeling and Memory Interaction in Learning Agents},
year={2017},
volume={47},
number={11},
pages={2882-2895},
abstract={Semantic memory plays a critical role in reasoning and decision
making. It enables an agent to abstract useful knowledge learned from its past
experience. Based on an extension of fusion adaptive resonance theory network,
this paper presents a novel self-organizing memory model to represent and learn
various types of semantic knowledge in a unified manner. The proposed model,
called fusion adaptive resonance theory for multimemory learning, incorporates
a set of neural processes, through which it may transfer knowledge and
cooperate with other long-term memory systems, including episodic memory and
procedural memory. Specifically, we present a generic learning process, under
which various types of semantic knowledge can be consolidated and transferred
from the specific experience encoded in episodic memory. We also identify and
formalize two forms of memory interactions between semantic memory and
procedural memory, through which more effective decision making can be
achieved. We present experimental studies, wherein the proposed model is used
to encode various types of semantic knowledge in different domains, including a
first-person shooting game called Unreal Tournament, the Toads and Frogs
puzzle, and a strategic game known as StarCraft Broodwar. Our experiments show
that the proposed knowledge transfer process from episodic memory to semantic
memory is able to extract useful knowledge to enhance the performance of
decision making. In addition, cooperative interaction between semantic
knowledge and procedural skills can lead to a significant improvement in both
learning efficiency and performance of the learning agents.},
keywords={adaptive resonance theory;computer games;decision making;learning
(artificial intelligence);multi-agent systems;StarCraft Broodwar;effective
decision making;episodic memory;first-person shooting game;frogs puzzle;fusion
adaptive resonance theory network;generic learning process;knowledge transfer
process;learning agents;learning efficiency;long-term memory systems;memory
interaction;memory interactions;multimemory learning;procedural
memory;procedural skills;semantic knowledge;semantic memory modeling;strategic
game;toads;unreal tournament;Adaptation models;Biological system
modeling;Computational modeling;Context modeling;Decision
making;Games;Semantics;Adaptive resonance theory;learning agents;memory
interactions;semantic memory},
doi={10.1109/TSMC.2016.2531683},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7430345,
author={T. M. Choi and P. S. Chow and B. Shen and M. L. Wan},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Service Analysis of Fashion Boutique Operations: An Empirical and
Analytical Study},
year={2017},
volume={47},
number={11},
pages={2896-2907},
abstract={Service quality is a critical element for fashion boutique
operations. However, owing to limitation of resources, fashion boutiques
usually cannot afford to establish a very formal service quality management
system. Motivated by the importance of service management for fashion
boutiques, this paper examines the respective service quality issues in two
related parts: based on the revised retail service quality scale (RSQS) model,
the first part conducts a service gap analysis via an empirical survey. It is
found that the RSQS model is valid, the respective service gaps exist, and the
problem-solving dimension has the largest service gap. Focusing on this most
significant problem-solving service gap (PSSG) identified in the empirical
analysis, the second part analytically studies the optimal PSSG decision for
fashion boutiques and examines the impact of retail competition via a game
theoretic analysis. The closed-form analytical findings reveal several
insights, such as: 1) for both monopoly and homogeneous duopoly cases, the
equilibrium demands depend solely on the profit margin, and the service gap
enhancement efficiency and 2) the relative PSSG dependent demand sensitivity is
critically important because it affects the equilibrium PSSG significantly.},
keywords={customer satisfaction;customer services;game
theory;monopoly;oligopoly;quality management;retailing;service industries;RSQS
model;analytical analysis;empirical analysis;fashion boutique operations;game
theoretic analysis;homogeneous duopoly;monopoly;optimal PSSG decision;problem-
solving service gap;profit margin;relative PSSG dependent demand
sensitivity;revised retail service quality scale model;service gap enhancement
efficiency;service quality management system;Analytical models;Customer
services;Problem-solving;Standards;Supply chains;Fashion
boutiques;multimethodological approach;service analysis;service operations},
doi={10.1109/TSMC.2016.2531687},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7428960,
author={L. Zhen and W. Wang and D. Zhuge},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Optimizing Locations and Scales of Distribution Centers Under
Uncertainty},
year={2017},
volume={47},
number={11},
pages={2908-2919},
abstract={In supply chain networks of large companies, how to determine the
location for establishing distribution centers is an important strategic-level
decision problem. This paper proposes a stochastic programming model to
determine distribution centers' locations as well as their scales. The
objective of the model is to minimize the expected total transportation cost
under uncertain demands of customers. An improved particle swarm optimization
algorithm and a Lagrangean relaxation-based solution approach are proposed to
solve the model. This paper also performs a case study on applying this model
to the largest retailer in China. In addition, some numerical experiments are
conducted to validate the effectiveness of the proposed model and the
efficiency of the proposed solution methods.},
keywords={facility location;goods distribution;particle swarm
optimisation;stochastic programming;supply chain management;supply
chains;transportation;China;Lagrangean relaxation-based solution
approach;distribution centers;expected total transportation cost;improved
particle swarm optimization algorithm;location optimization;stochastic
programming model;strategic-level decision problem;supply chain
networks;Numerical models;Programming;Stochastic processes;Supply
chains;Transportation;Uncertainty;Distribution centers;Lagrangean
relaxation;facility location;particle swarm optimization (PSO);stochastic
programming},
doi={10.1109/TSMC.2016.2531696},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7436814,
author={F. Yang and N. Wu and Y. Qiao and M. Zhou},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Optimal One-Wafer Cyclic Scheduling of Time-Constrained Hybrid
Multicluster Tools via Petri Nets},
year={2017},
volume={47},
number={11},
pages={2920-2932},
abstract={Scheduling a multicluster tool with wafer residency time constraints
is highly challenging yet important in ensuring high productivity of wafer
fabrication. This paper presents a method to find an optimal one-wafer cyclic
schedule for it. A Petri net is developed to model the dynamic behavior of the
tool. By this model, a schedule of the system is analytically expressed as a
function of robots' waiting time. Based on this model, this paper presents the
necessary and sufficient conditions under which a feasible one-wafer cyclic
schedule exists. Then, it gives efficient algorithms to find such a schedule
that is optimal. These algorithms require determining the robots' waiting time
via simple calculation and thus are efficient. Examples are given to show the
application and effectiveness of the proposed method.},
keywords={Petri nets;scheduling;semiconductor technology;Petri net;hybrid
multicluster tools;optimal one-wafer cyclic scheduling;robots;time
constraints;wafer fabrication productivity;wafer residency time
constraints;Fabrication;Optimal scheduling;Petri
nets;Robots;Schedules;Semiconductor device modeling;Time factors;Discrete event
systems;Petri net (PN);multicluster tools;scheduling;semiconductor
manufacturing},
doi={10.1109/TSMC.2016.2531697},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7464340,
author={A. Dutta and P. Dasgupta},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Ensemble Learning With Weak Classifiers for Fast and Reliable Unknown
Terrain Classification Using Mobile Robots},
year={2017},
volume={47},
number={11},
pages={2933-2944},
abstract={We propose a lightweight and fast learning algorithm for classifying
the features of an unknown terrain that a robot is navigating in. Most of the
existing research on unknown terrain classification by mobile robots relies on
a single powerful classifier to correctly identify the terrain using sensor
data from a single sensor like laser or camera. In contrast, our proposed
approach uses multiple modalities of sensed data and multiple, weak but less-
complex classifiers for classifying the terrain types. The classifiers are
combined using an ensemble learning algorithm to improve the algorithm's
training rate as compared to an individual classifier. Our algorithm was tested
with data collected by navigating a four-wheeled, autonomous robot, called
Explorer, over different terrains including brick, grass, rock, sand, and
concrete. Our results show that our proposed approach performs better with up
to 63% better prediction accuracy for some terrains as compared to a support
vector machine (SVM)-based learning technique that uses sensor data from a
single sensor. Despite using multiple classifiers, our algorithm takes only a
fraction (1/65) of the time on average, as compared to the SVM technique.},
keywords={feature extraction;learning (artificial intelligence);mobile
robots;path planning;robot vision;Explorer robot;ensemble learning
algorithm;fast learning algorithm;fast terrain classification;feature
classification;four-wheeled autonomous robot;less-complex
classifiers;lightweight learning algorithm;mobile robots;multiple
classifiers;multiple modalities;reliable unknown terrain classification;robot
navigation;sensor data;terrain type classification;weak
classifiers;Bagging;Cameras;Mobile robots;Navigation;Robot sensing
systems;Support vector machines;Ensemble learning;mobile robot;terrain
classification},
doi={10.1109/TSMC.2016.2531700},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7460123,
author={Z. Bowen and Y. Ming and Y. Hidekazu and L. Hongxing},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Evaluation of Physical Protection Systems Using an Integrated Platform
for Analysis and Design},
year={2017},
volume={47},
number={11},
pages={2945-2955},
abstract={A physical protection system (PPS) is used to protect a nuclear power
plant (NPP) for preventing the nuclear material and critical infrastructure
from theft, robbery, illegal transfer, and sabotage. An integrated platform for
analysis and design (IPAD) of PPS was proposed by the authors' previous work.
By combining the functions of 3-D modeling of PPS with automatic 2-D design
drawing generation, the proposed IPAD will provide the designers with
comprehensive and visualized information of PPS in one platform which will
enable a quick and convenient design of PPS. This paper introduces the
functions of IPAD in reliability and risk analysis of PPS. A reliability
analysis of a simplified PPS at NPP is presented. With the help of IPAD, it is
very easy to identify the vulnerable paths. Furthermore, the sensitivity
analysis is conducted for identifying the key parameters to be improved for
meeting the design requirements. The reliability and risk analysis results will
provide a valuable feedback for the evaluation and revision of the design of
PPS.},
keywords={nuclear power stations;power engineering computing;power generation
protection;power generation reliability;risk analysis;sensitivity
analysis;IPAD;NPP;automatic 2-D design drawing generation;critical
infrastructure;design requirements;integrated platform;nuclear material;nuclear
power plant;physical protection system;reliability analysis;risk
analysis;sensitivity analysis;simplified
PPS;Force;Interference;Reliability;Security;Solid modeling;Tablet
computers;Visualization;Reliability analysis;risk analysis;safeguard
application},
doi={10.1109/TSMC.2016.2531995},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7466112,
author={Y. Fang and Z. Fang and F. Yuan and Y. Yang and S. Yang and N. N.
Xiong},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Optimized Multioperator Image Retargeting Based on Perceptual Similarity
Measure},
year={2017},
volume={47},
number={11},
pages={2956-2966},
abstract={With various emerging mobile devices, the visual content have be to
resized into different sizes or aspect ratios for good viewing experiences. In
this paper, we propose a new multioperator retargeting algorithm by using four
retargeting operators of seam carving, cropping, warping, and scaling
iteratively. To determine which retargeting operator should be used at each
iteration, we adopt structural similarity (SSIM) to evaluate the similarity
between the original and retargeted images. The retargeting operator sequence
is constructed based on the four types of retargeting operators by an
optimization process. Since the sizes of original and retargeted images are
different, scale-invariant feature transform flow is used for dense
correspondence between the original and retargeted images for similarity
evaluation. Additionally, visual saliency is used to weight SSIM results based
on the characteristics of the human visual system. Experimental results on a
public image retargeting database have shown the promising performance of the
proposed multioperator retargeting algorithm.},
keywords={image processing;optimisation;transforms;visual databases;SSIM;human
visual system;mobile devices;multioperator retargeting algorithm;optimization
process;optimized multioperator image retargeting;perceptual similarity
measure;public image retargeting database;retargeting operator sequence;scale-
invariant feature transform;similarity evaluation;structural similarity;visual
saliency;Algorithm design and analysis;Distortion;Measurement;Media;Mobile
handsets;Optimization;Visualization;Image retargeting;image
saliency;multioperator retargeting;structural similarity (SSIM)},
doi={10.1109/TSMC.2016.2557225},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7473879,
author={G. Ertek and X. Chi and A. N. Zhang},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={A Framework for Mining RFID Data From Schedule-Based Systems},
year={2017},
volume={47},
number={11},
pages={2967-2984},
abstract={A schedule-based system is a system that operates on or contains
within a schedule of events and breaks at particular time intervals. Entities
within the system show presence or absence in these events by entering or
exiting the locations of the events. Given radio frequency identification
(RFID) data from a schedule-based system, what can we learn about the system
(the events and entities) through data mining? Which data mining methods can be
applied so that one can obtain rich actionable insights regarding the system
and the domain? The research goal of this paper is to answer these posed
research questions, through the development of a framework that systematically
produces actionable insights for a given schedule-based system. We show that
through integrating appropriate data mining methodologies as a unified
framework, one can obtain many insights from even a very simple RFID dataset,
which contains only very few fields. The developed framework is general, and is
applicable to any schedule-based system, as long as it operates under certain
basic assumptions. The types of insights are also general, and are formulated
in this paper in the most abstract way. The applicability of the developed
framework is illustrated through a case study, where real world data from a
schedule-based system is analyzed using the introduced framework. Insights
obtained include the profiling of entities and events, the interactions between
entity and events, and the relations between events.},
keywords={data mining;decision support systems;information systems;learning
(artificial intelligence);radiofrequency identification;scheduling;RFID
data;data mining methods;decision support systems;event schedule;information
systems;radiofrequency identification;schedule-based system;Algorithm design
and analysis;Data mining;Optimal scheduling;Radiofrequency
identification;Schedules;Single machine scheduling;Data mining;decision support
systems;information systems},
doi={10.1109/TSMC.2016.2557762},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7467487,
author={L. Liu},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={New Criteria on Exponential Stability for Stochastic Delay Differential
Systems Based on Vector Lyapunov Function},
year={2017},
volume={47},
number={11},
pages={2985-2993},
abstract={This paper established the vector Razumikhin-type theorem on
exponential stability for stochastic functional differential systems by the
stochastic analysis techniques and the property of M-Matrix. Several novel
stability criteria with vector ℒ-operator differential inequalities for
stochastic delay differential systems, especially some of which includes the
cross-item, were obtained by means of the vector Razumikhin theorem. By
applying these new results, the robustness of global exponential stability of
delay recurrent neural networks to random disturbed has been analyzed.},
keywords={Lyapunov methods;asymptotic stability;delay-differential
systems;differential equations;matrix algebra;stability criteria;stochastic
systems;vectors;M-matrix property;delay recurrent neural networks;global
exponential stability;stability criteria;stochastic analysis
techniques;stochastic delay differential systems;stochastic functional
differential systems;vector ℒ-operator differential inequalities;vector
Razumikhin theorem;vector lyapunov function;Control theory;Delays;Lyapunov
methods;Stability criteria;Stochastic processes;Exponential stability;vector L-
operator differential inequality;vector Razumikhin theorem},
doi={10.1109/TSMC.2016.2558047},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7467492,
author={B. Chen and G. Hu and D. W. C. Ho and W. A. Zhang and L. Yu},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Distributed Robust Fusion Estimation With Application to State
Monitoring Systems},
year={2017},
volume={47},
number={11},
pages={2994-3005},
abstract={This paper studies the distributed robust fusion estimation problem
with stochastic and deterministic parameter uncertainties, where the covariance
of the Gaussian white noise is unknown, and the covariances of the random
variables in the stochastic uncertainties are in a bounded set. By using the
discrete-time stochastic bounded real lemma and the matrix analysis approach,
each local robust estimator is derived to guarantee an optimal estimation
performance for admissible uncertainties, and then necessary and sufficient
condition for the distributed robust fusion estimator is presented to obtain an
optimal weighting fusion criterion. Note that the local robust estimation
problem and the distributed robust fusion estimation problem are both converted
into convex optimization problems, which can be easily solved by standard
software packages. The advantage and effectiveness of the proposed methods are
demonstrated through state monitoring for target tracking system and stirred
rank reactor system.},
keywords={Gaussian noise;covariance matrices;discrete time systems;estimation
theory;optimisation;sensor fusion;stochastic processes;stochastic
systems;target tracking;uncertain systems;white noise;Gaussian white
noise;convex optimization problems;covariance;deterministic parameter
uncertainties;discrete-time stochastic bounded real lemma;distributed robust
fusion estimation problem;local robust estimation problem;local robust
estimator;matrix analysis approach;optimal estimation performance;optimal
weighting fusion criterion;standard software packages;state monitoring
systems;stirred rank reactor system;stochastic parameter
uncertainties;stochastic uncertainties;target tracking
system;Estimation;Monitoring;Robustness;Stochastic processes;Uncertain
systems;Uncertainty;White noise;Convex optimization;distributed fusion
estimation;inaccurate covariances;sensor fusion;state monitoring
systems;stochastic and deterministic uncertainties},
doi={10.1109/TSMC.2016.2558103},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7466820,
author={K. Ding and P. Jiang and P. Sun and C. Wang},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={RFID-Enabled Physical Object Tracking in Process Flow Based on an
Enhanced Graphical Deduction Modeling Method},
year={2017},
volume={47},
number={11},
pages={3006-3018},
abstract={The purpose of this paper is to develop an enhanced radio frequency
identification (RFID)-enabled graphical deduction model (rfid-GDM) for tracking
the time-sensitive state, position, and other attributes of RFID-tagged objects
in process flow. Concepts and definitions related to processes and RFID
applications are first clarified, and enhanced state blocks are proposed to
depict four kinds of RFID application scenarios. The implementation framework
of rfid-GDM and its five steps are further addressed. Both mathematical
formalization and graphical description of each step are involved. Finally, a
case is studied to verify the feasibility of rfid-GDM. It is expected that
rfid-GDM will provide instructions for modeling and tracking RFID-enabled
process flows in diverse fields.},
keywords={graph theory;object tracking;radiofrequency identification;GDM;RFID-
enabled physical object tracking;RFID-tagged objects;diverse fields;enhanced
graphical deduction modeling method;enhanced radio frequency
identification;enhanced state blocks;graphical deduction model;graphical
description;mathematical formalization;process flow;time-sensitive
state;Adaptation models;Logistics;Manufacturing;Modeling;Monitoring;RFID
tags;Configuration;graphical deduction modeling;process flow;radio frequency
identification (RFID);tracking},
doi={10.1109/TSMC.2016.2558104},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7486137,
author={Y. Feng and K. Xing and Z. Gao and Y. Wu},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Transition Cover-Based Robust Petri Net Controllers for Automated
Manufacturing Systems With a Type of Unreliable Resources},
year={2017},
volume={47},
number={11},
pages={3019-3029},
abstract={So far, the majority of deadlock control policies for automated
manufacturing systems (AMSs) are based on the assumption that no resource
fails; while for AMSs with unreliable resources, the main concerns are deadlock
avoidance problems. This paper focuses on the robust deadlock prevention
problem for AMSs with a type of unreliable resources, and assumes that at most
one of unreliable resources fails at a time. Petri net is introduced to model
the considered AMS. Deadlock can be characterized in terms of maximal perfect
resource transition-circuits (MPRT-circuits). To develop robust Petri net
deadlock controllers with small structures for the system, a new concept of
strong transition covers is presented, which is a special kind of transition
covers. By designing a control place with a proper control variable to each
MPRT-circuit in the strong transition cover, a 1-robust Petri net controller is
obtained, whereas the control variables can be determined by an integer linear
programming. Such a 1-robust controller ensures that the system can process all
types of parts infinitely even if one of unreliable resources fails. Since the
number of MPRT-circuits in a strong transition cover is much less than that of
all MPRT-circuits, our Petri net controller is of small structural size. Each
AMS with a type of unreliable resources has at least a transition cover. An
algorithm is presented for checking the strongness of transition covers, and
transforming weak transition covers into strong ones. Finally, some examples
are given to illustrate the effectiveness of the proposed method.},
keywords={Petri nets;concurrency control;integer programming;linear
programming;manufacturing systems;robust control;automated manufacturing
systems;deadlock avoidance problems;deadlock control policies;integer linear
programming;maximal perfect resource transition circuit;robust Petri net
controllers;Control systems;Cybernetics;Integrated circuit
modeling;Manufacturing systems;Petri nets;Robustness;System recovery;Automated
manufacturing system (AMS);Petri net;deadlock prevention;resource
failure;robust controller;transition cover},
doi={10.1109/TSMC.2016.2558106},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7466813,
author={R. Bapat and C. W. Wu},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Control Localization in Networks of Dynamical Systems Connected via a
Weighted Tree},
year={2017},
volume={47},
number={11},
pages={3030-3036},
abstract={For a network of dynamical systems coupled via an undirected weighted
tree, we consider the problem of which system to apply control, in the case
when only a single system receives control. We abstract this problem into a
study of eigenvalues of a perturbed Laplacian matrix. We show that this
eigenvalue problem has a complete solution for arbitrarily large control by
showing that the best and the worst places to apply control have well-known
characterization in graph theory, thus linking the computational eigenvalue
problem with graph-theoretical concepts. Some partial results are proved in the
case when the control effort is bounded. In particular, we show that a local
maximum in localizing the best place for control is also a global maximum. We
conjecture in the bounded control case that the best place to apply control
must also necessarily be a characteristic vertex and present evidence from
numerical experiments to support this conjecture.},
keywords={eigenvalues and eigenfunctions;matrix algebra;nonlinear control
systems;perturbation techniques;trees (mathematics);bounded control
case;computational eigenvalue problem;control localization;dynamical
systems;global maximum;graph theory;local maximum;perturbed Laplacian
matrix;single system;undirected weighted tree;Eigenvalues and
eigenfunctions;Graph theory;Laplace equations;Matrix decomposition;Symmetric
matrices;Trajectory;Control systems;graph theory;nonlinear dynamical systems},
doi={10.1109/TSMC.2016.2558107},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7469781,
author={Y. Wang and X. Li and R. Ruiz},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={An Exact Algorithm for the Shortest Path Problem With Position-Based
Learning Effects},
year={2017},
volume={47},
number={11},
pages={3037-3049},
abstract={The shortest path problems (SPPs) with learning effects (SPLEs) have
many potential and interesting applications. However, at the same time they are
very complex and have not been studied much in the literature. In this paper,
we show that learning effects make SPLEs completely different from SPPs. An
adapted A* (AA*) is proposed for the SPLE problem under study. Though global
optimality implies local optimality in SPPs, it is not the case for SPLEs. As
all subpaths of potential shortest solution paths need to be stored during the
search process, a search graph is adopted by AA* rather than a search tree used
by A*. Admissibility of AA* is proven. Monotonicity and consistency of the
heuristic functions of AA* are redefined and the corresponding properties are
analyzed. Consistency/monotonicity relationships between the heuristic
functions of AA* and those of A* are explored. Their impacts on efficiency of
searching procedures are theoretically analyzed and experimentally evaluated.},

keywords={learning (artificial intelligence);optimisation;search problems;tree
searching;A* (AA*) adaptation;SPLE problem;SPPs;consistency-monotonicity
relationships;exact algorithm;heuristic functions;position-based learning
effects;search graph;search process;search tree;shortest path
problem;Cybernetics;Heuristic algorithms;Indexes;Logistics;Maintenance
engineering;Robots;Shortest path problem;A* search;admissibility;learning
effect;shortest path},
doi={10.1109/TSMC.2016.2560418},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7473862,
author={R. Qin and C. H. Dagli and N. Amaeshi},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={A Contract Negotiation Model for Constituent Systems in the Acquisition
of Acknowledged System of Systems},
year={2017},
volume={47},
number={11},
pages={3050-3062},
abstract={In the acquisition of acknowledged system of systems (SoSs), the SoS
manager attempts to reach a coalition agreement with each of selected
constituent systems. When the systems have self-interests that are incompatible
with the SoS's goal, contract negotiation is a way of resolving conflicts.
Complexity of the acknowledged SoS acquisition makes the negotiation a hard
decision process for constituent systems. To the SoS manager, the negotiation
with consistent systems is difficult too, considering that the behavior and
strategies of constituent systems in negotiation impact the ability and
timeliness of a group to agree on an SoS architecture, and so the overall
mission effectiveness of the SoS. Motivated by these, this paper develops a
contract negotiation model for constituent systems in the acknowledged SoS
acquisition. The model consists of: 1) a protocol for negotiating multiple
interdependent issues of multiple items over multiple stages and 2) a decision
framework of constituent systems. The model is not only a decision support for
constituent systems but also a tool with which the SoS manager can better
understand constituent systems.},
keywords={decision making;multi-agent systems;systems engineering;SoS
architecture;SoS manager;acknowledged SoS acquisition;acknowledged system of
systems;consistent systems;constituent systems;contract negotiation
model;decision support;hard decision
process;Collaboration;Contracts;Games;Indexes;Modeling;Optimization;Protocols;Complex
systems;contract negotiation;decision modeling;system of systems (SoSs)},
doi={10.1109/TSMC.2016.2560520},
ISSN={2168-2216},
month={Nov},}
@ARTICLE{7476880,
author={Z. Zhang and C. Guo and L. Martínez},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
title={Managing Multigranular Linguistic Distribution Assessments in Large-
Scale Multiattribute Group Decision Making},
year={2017},
volume={47},
number={11},
pages={3063-3076},
abstract={Linguistic large-scale group decision making (LGDM) problems are more
and more common nowadays. In such problems a large group of decision makers are
involved in the decision process and elicit linguistic information that are
usually assessed in different linguistic scales with diverse granularity
because of decision makers' distinct knowledge and background. To keep maximum
information in initial stages of the linguistic LGDM problems, the use of
multigranular linguistic distribution assessments seems a suitable choice,
however, to manage such multigranular linguistic distribution assessments, it
is necessary the development of a new linguistic computational approach. In
this paper, it is proposed a novel computational model based on the use of
extended linguistic hierarchies, which not only can be used to operate with
multigranular linguistic distribution assessments but also can provide
interpretable linguistic results to decision makers. Based on this new
linguistic computational model, an approach to linguistic large-scale
multiattribute group decision making is proposed and applied to a talent
selection process in universities.},
keywords={computational linguistics;decision making;decision theory;decision
makers;decision process;elicit linguistic information;extended linguistic
hierarchies;large-scale multiattribute group decision making;linguistic LGDM
problems;linguistic computational model;linguistic scales;multigranular
linguistic distribution assessments;talent selection
process;universities;Computational modeling;Cybernetics;Decision
making;Indexes;Numerical models;Pragmatics;Group decision making (GDM);large-
scale GDM (LGDM);linguistic distribution assessment;multigranular linguistic
information},
doi={10.1109/TSMC.2016.2560521},
ISSN={2168-2216},
month={Nov},}
